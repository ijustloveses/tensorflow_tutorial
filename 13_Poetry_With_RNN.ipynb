{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poetry with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节参考 [斗大的熊猫](http://blog.topspeedsnail.com/archives/10542) ，使用 RNN 在唐诗数据集上进行 RNN 学习和生成\n",
    "\n",
    "代码参考 [char-rnn](https://github.com/karpathy/char-rnn)\n",
    "\n",
    "使用的数据集：[全唐诗(43030首)](https://pan.baidu.com/s/1o7QlUhO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import codecs\n",
    "\n",
    "class Poetry(object):\n",
    "    def __init__(self, poetry_file='poetry.txt'):\n",
    "        self.poetry_file = poetry_file\n",
    "        print \"parsing poetry file ...\"\n",
    "        self.poetrys = self.parse_datafile()\n",
    "        print \"creating lexicons ...\"\n",
    "        self.create_lexicon()\n",
    "\n",
    "    def parse_datafile(self):\n",
    "        poetrys = []\n",
    "        with codecs.open(self.poetry_file, 'r', 'utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    title, content = line.strip().split(':')\n",
    "                    content = content.replace(' ', '')\n",
    "                    if '_' in content or '(' in content or u'（' in content or u'《' in content or '[' in content:\n",
    "                        continue\n",
    "                    if len(content) < 5 or len(content) > 79:\n",
    "                        continue\n",
    "                    content = '[' + content + ']'\n",
    "                    poetrys.append(content)\n",
    "                except Exception, e:\n",
    "                    print content\n",
    "        sorted(poetrys, key=lambda l: len(l))\n",
    "        print(\"Total poetrys number: {}\".format(len(poetrys)))\n",
    "        return poetrys\n",
    "\n",
    "    def create_lexicon(self):\n",
    "        all_words = []\n",
    "        for p in self.poetrys:\n",
    "            # 这里以字符为单位进行拆分，包括了前后缀 '[' 和 ']'\n",
    "            all_words += [w for w in p]\n",
    "        counter = Counter(all_words)\n",
    "        # 按字符计数，按计数倒序\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        # 去按计数倒序排列的 words，得到 lexicon\n",
    "        words, _ = zip(*count_pairs)\n",
    "\n",
    "        # words 加上一个 ' ' 空格字符，形如 ['[', 'a', 'f', 'g', 'h', 'i', ']', ' ']\n",
    "        self.words = words + (' ',)\n",
    "        # 字符 ==> 索引 id，形如 {'[':0, 'a':1, 'f':2, 'g':3, 'h':4, 'i':5, ']':6, ' ':7}\n",
    "        self.word_num_map = dict(zip(self.words, range(len(self.words))))\n",
    "        # 把 poetrys 转为为索引形式: poetrys 形如 ['[afg]', '[aghi]']\n",
    "        # 得到的结果 poetrys_vector 形如：[[0, 1, 2, 3, 6], [0, 1, 3, 4, 5, 6]]; 不在 map 中的词设为最后一位，对应 ' '\n",
    "        self.poetrys_vector = [[self.word_num_map.get(w, len(self.words)) for w in p] for p in self.poetrys]\n",
    "\n",
    "    def split_batch_data(self, batch_size=64):\n",
    "        # total batch number = total peotrys number // batch size，截断，而不是 4 舍 5 入\n",
    "        n_chunk = len(self.poetrys_vector) // batch_size\n",
    "        x_batches = []\n",
    "        y_batches = []\n",
    "        # 前面的截断，保证了这里每个 batch 都是 batch_size 个 poetrys\n",
    "        for i in range(n_chunk):\n",
    "            start_index = i * batch_size\n",
    "            end_index = start_index + batch_size\n",
    "            batch = self.poetrys_vector[start_index: end_index]\n",
    "            # batch 中最长的一个 poetry 的长度，注意这个不是所有 poetrys 的最长长度，而是 batch 的；故此不同 batch 此值会不同\n",
    "            length = max(map(len, batch))\n",
    "            # 长度不足的句子，使用空格对应的 index 来填补\n",
    "            xdata = np.full((batch_size, length), self.word_num_map[' '], np.int32)\n",
    "            for i in range(batch_size):\n",
    "                xdata[i, :len(batch[i])] = batch[i]\n",
    "            \"\"\"\n",
    "            xdata             ydata\n",
    "            [6,2,4,6,9]       [2,4,6,9,9]\n",
    "            [1,4,2,8,5]       [4,2,8,5,5]\n",
    "            ydata 就是 xdata 中的每个句子向后错一位，也就是说通过 xdata 的句子的任何一个字预测其后面一个字\n",
    "            当然，到了最后一个字，就只能预测它自己了\n",
    "            \"\"\"\n",
    "            ydata = np.copy(xdata)\n",
    "            ydata[:, :-1] = xdata[:, 1:]\n",
    "            x_batches.append(xdata)\n",
    "            y_batches.append(ydata)\n",
    "\n",
    "        return x_batches, y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part I. 探索数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing poetry file ...\n",
      "[渐老风光不著人，花溪柳陌早逢春。近来行到门前少，趁暖闲眠似病人。]\n",
      "Total poetrys number: 34646\n",
      "creating lexicons ...\n",
      "541 64\n",
      "541 64\n",
      "541\n"
     ]
    }
   ],
   "source": [
    "poetry = Poetry('data/poetry.txt')\n",
    "batch_size = 64\n",
    "x_batches, y_batches = poetry.split_batch_data(batch_size=batch_size)\n",
    "n_chunk = len(x_batches)\n",
    "print len(x_batches), len(x_batches[0])\n",
    "print len(y_batches), len(y_batches[0])\n",
    "print n_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看到我们把文件中的有效唐诗分成 541 个 batch；每个 batch 中为 64 个唐诗文本，以及通过这些唐诗文本错位得到的下一个字的结果集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\uff0c', u'\\u3002', u']', u'[', u'\\u4e0d', u'\\u4eba', u'\\u5c71', u'\\u98ce', u'\\u65e5', u'\\u65e0')\n",
      "6110 6110 34646\n"
     ]
    }
   ],
   "source": [
    "total_words = len(poetry.words)\n",
    "unique_words = len(poetry.word_num_map)\n",
    "print poetry.words[0: 10]\n",
    "print total_words, unique_words, len(poetry.poetrys_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看到，最多出现的字符为 (u'，', u'。', u']', u'[', u'不', u'人', u'山', u'风', u'日', u'无')\n",
    "\n",
    "共计 6110 个字，共计 34646 首唐诗。  541 * 64 = 34624，说明由于截断的作用，最后有 22 首唐诗没有被收录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Part II. Layer definations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "output_targets = tf.placeholder(tf.int32, [batch_size, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neural_network(model='lstm', rnn_size=128, num_layers=2):\n",
    "    if model == 'rnn':\n",
    "        cell_fun = tf.nn.rnn_cell.BasicRNNCell\n",
    "    elif model == 'gru':\n",
    "        cell_fun = tf.nn.rnn_cell.GRUCell\n",
    "    elif model == 'lstm':\n",
    "        cell_fun = tf.nn.rnn_cell.BasicLSTMCell\n",
    "\n",
    "    # num_units = rnn_size，是 rnn 输入的 length，后面有详解\n",
    "    # state_is_tuple\n",
    "    #    - If True, accepted and returned states are 2-tuples of the c_state and m_state. \n",
    "    #    - If False, they are concatenated along the column axis.\n",
    "    # cell = cell_fun(rnn_size, state_is_tuple=True)    # tf version 0.9 will raise exception - LSTMStateTuple invalid type <type 'tuple'>, must be a string or Tensor.\n",
    "    cell = cell_fun(rnn_size, state_is_tuple=False)\n",
    "    # 默认为双层的 RNN (stacked RNN)\n",
    "    # cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=False)\n",
    "    # 注意第一个参数为 batch_size，就是说为 batch 中的每一条数据都准备一个 state; 这个其实就相当于 RNN 中的隐藏变量\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    scopename = 'rnnlm1'\n",
    "    with tf.variable_scope(scopename):\n",
    "        \"\"\"\n",
    "        定义一个 softmax 层，核心是根据一个字 (rnn_size 维) 去预测下一个字； rnn_size 下面介绍\n",
    "        故此 W 为 rnn_size * len(poetry.words) 维度\n",
    "        \"\"\"\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, len(poetry.words)+1])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [len(poetry.words)+1])\n",
    "        \"\"\"\n",
    "        定义一个 embedding 层，前面我们知道词典共计 len(poetry.words) 个词，故此每个词可以表示为 len(poetry.words) 维的 one-hot 矢量\n",
    "        但是我们想得到的是一个 rnn_size 维度的矢量，故此需要一个 look_up 矩阵，为 len(poetry.words) x rnn_size 维度\n",
    "        \n",
    "        然后，input_data 为 batch_size  x  len(max_length_in_batch) 维度的数据，也就是说共计 batch_size 条数据，每个数据都等长，长度为 batch 中最长的句子\n",
    "        注意 tf.nn.embedding_lookup 要求 input_data 中的数据都是等长的，否则报错\n",
    "        而且，input_data 中每条数据中的样子类似 [6,2,4,6,9] ，其中每个数字代表数据中的字对应到词典中的 index\n",
    "        \n",
    "        这样， tf.nn.embedding_lookup 就根据 index 去词典中 lookup，得到对应字的 embedded_vector\n",
    "        故此，最后得到的结果为 batch_size x len(max_length_in_batch) x rnn_size 维度\n",
    "        就是说共计 batch_size 条数据，每个数据中有 len(max_length_in_batch) 个字，每个字对应的嵌入矢量为 rnn_size 维\n",
    "        \n",
    "        最后，这里保守的取了 len(poetry.words) + 1，其实最后也用不到，没关系\n",
    "        \"\"\"\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [len(poetry.words)+1, rnn_size])\n",
    "            inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "        \n",
    "    \"\"\"\n",
    "    embedding 层得到 input 接到这里，也就是说接在 RNN 层上\n",
    "    注意 dynamic_rnn 会动态展开输入数据，提升效率，见 http://jialin114.wang/2016/09/02/dynamic-rnn/\n",
    "    \"\"\"\n",
    "    outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, scope=scopename)\n",
    "    # 把输出转为 rnn_size 的矢量\n",
    "    output = tf.reshape(outputs, [-1, rnn_size])\n",
    "    # 这时才接入 softmax 层\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    return logits, last_state, probs, cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part III. Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f95a6de0f10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 0, train_loss 8.74556541443\n",
      "epoch 0, batch 1, train_loss 8.74578857422\n",
      "epoch 0, batch 2, train_loss 7.99591207504\n",
      "epoch 0, batch 3, train_loss 7.80856180191\n",
      "epoch 0, batch 4, train_loss 6.79696178436\n",
      "epoch 0, batch 5, train_loss 6.39533138275\n",
      "epoch 0, batch 6, train_loss 7.56495714188\n",
      "epoch 0, batch 7, train_loss 5.9849395752\n",
      "epoch 0, batch 8, train_loss 5.21713018417\n",
      "epoch 0, batch 9, train_loss 6.9343495369\n",
      "epoch 0, batch 10, train_loss 5.70776367188\n",
      "epoch 0, batch 11, train_loss 6.16476678848\n",
      "epoch 0, batch 12, train_loss 5.43673849106\n",
      "epoch 0, batch 13, train_loss 6.06676721573\n",
      "epoch 0, batch 14, train_loss 4.97399902344\n",
      "epoch 0, batch 15, train_loss 5.77471065521\n",
      "epoch 0, batch 16, train_loss 5.15108203888\n",
      "epoch 0, batch 17, train_loss 4.95245409012\n",
      "epoch 0, batch 18, train_loss 4.48413991928\n",
      "epoch 0, batch 19, train_loss 3.8879468441\n",
      "epoch 0, batch 20, train_loss 4.79983282089\n",
      "epoch 0, batch 21, train_loss 4.51987314224\n",
      "epoch 0, batch 22, train_loss 5.1531496048\n",
      "epoch 0, batch 23, train_loss 4.60852384567\n",
      "epoch 0, batch 24, train_loss 4.51663351059\n",
      "epoch 0, batch 25, train_loss 5.14199399948\n",
      "epoch 0, batch 26, train_loss 4.91644906998\n",
      "epoch 0, batch 27, train_loss 4.85973072052\n",
      "epoch 0, batch 28, train_loss 5.25195455551\n",
      "epoch 0, batch 29, train_loss 5.08284711838\n",
      "epoch 0, batch 30, train_loss 5.19286632538\n",
      "epoch 0, batch 31, train_loss 4.450091362\n",
      "epoch 0, batch 32, train_loss 4.06396150589\n",
      "epoch 0, batch 33, train_loss 6.66986083984\n",
      "epoch 0, batch 34, train_loss 6.71485424042\n",
      "epoch 0, batch 35, train_loss 4.80967187881\n",
      "epoch 0, batch 36, train_loss 4.71823883057\n",
      "epoch 0, batch 37, train_loss 4.70627880096\n",
      "epoch 0, batch 38, train_loss 4.7109079361\n",
      "epoch 0, batch 39, train_loss 5.37218809128\n",
      "epoch 0, batch 40, train_loss 4.2746963501\n",
      "epoch 0, batch 41, train_loss 4.57638549805\n",
      "epoch 0, batch 42, train_loss 4.66584300995\n",
      "epoch 0, batch 43, train_loss 4.40157938004\n",
      "epoch 0, batch 44, train_loss 4.91971492767\n",
      "epoch 0, batch 45, train_loss 4.67425680161\n",
      "epoch 0, batch 46, train_loss 4.5726647377\n",
      "epoch 0, batch 47, train_loss 4.70574998856\n",
      "epoch 0, batch 48, train_loss 4.44095849991\n",
      "epoch 0, batch 49, train_loss 4.77681398392\n",
      "epoch 0, batch 50, train_loss 4.53425359726\n",
      "epoch 0, batch 51, train_loss 4.08010339737\n",
      "epoch 0, batch 52, train_loss 4.64959621429\n",
      "epoch 0, batch 53, train_loss 4.6064248085\n",
      "epoch 0, batch 54, train_loss 4.15316343307\n",
      "epoch 0, batch 55, train_loss 4.23143005371\n",
      "epoch 0, batch 56, train_loss 4.08717632294\n",
      "epoch 0, batch 57, train_loss 4.60366344452\n",
      "epoch 0, batch 58, train_loss 4.4158949852\n",
      "epoch 0, batch 59, train_loss 4.55083179474\n",
      "epoch 0, batch 60, train_loss 4.68648481369\n",
      "epoch 0, batch 61, train_loss 2.80886483192\n",
      "epoch 0, batch 62, train_loss 4.10072517395\n",
      "epoch 0, batch 63, train_loss 4.35506343842\n",
      "epoch 0, batch 64, train_loss 4.50282669067\n",
      "epoch 0, batch 65, train_loss 5.00799465179\n",
      "epoch 0, batch 66, train_loss 4.57475423813\n",
      "epoch 0, batch 67, train_loss 3.05988454819\n",
      "epoch 0, batch 68, train_loss 4.26618814468\n",
      "epoch 0, batch 69, train_loss 3.55493998528\n",
      "epoch 0, batch 70, train_loss 6.12757825851\n",
      "epoch 0, batch 71, train_loss 6.12444448471\n",
      "epoch 0, batch 72, train_loss 4.15065383911\n",
      "epoch 0, batch 73, train_loss 4.7217912674\n",
      "epoch 0, batch 74, train_loss 5.66983556747\n",
      "epoch 0, batch 75, train_loss 4.72285079956\n",
      "epoch 0, batch 76, train_loss 4.92531204224\n",
      "epoch 0, batch 77, train_loss 6.21054124832\n",
      "epoch 0, batch 78, train_loss 4.47970962524\n",
      "epoch 0, batch 79, train_loss 4.34179019928\n",
      "epoch 0, batch 80, train_loss 4.01239395142\n",
      "epoch 0, batch 81, train_loss 4.30126094818\n",
      "epoch 0, batch 82, train_loss 4.13523483276\n",
      "epoch 0, batch 83, train_loss 4.29444551468\n",
      "epoch 0, batch 84, train_loss 3.97936010361\n",
      "epoch 0, batch 85, train_loss 4.07335042953\n",
      "epoch 0, batch 86, train_loss 4.59152555466\n",
      "epoch 0, batch 87, train_loss 3.64367890358\n",
      "epoch 0, batch 88, train_loss 3.98238062859\n",
      "epoch 0, batch 89, train_loss 4.29453754425\n",
      "epoch 0, batch 90, train_loss 3.83921694756\n",
      "epoch 0, batch 91, train_loss 4.13295793533\n",
      "epoch 0, batch 92, train_loss 4.62315893173\n",
      "epoch 0, batch 93, train_loss 5.90904092789\n",
      "epoch 0, batch 94, train_loss 4.16588354111\n",
      "epoch 0, batch 95, train_loss 4.32853126526\n",
      "epoch 0, batch 96, train_loss 3.40810132027\n",
      "epoch 0, batch 97, train_loss 4.18286371231\n",
      "epoch 0, batch 98, train_loss 4.21346950531\n",
      "epoch 0, batch 99, train_loss 4.63944339752\n",
      "epoch 0, batch 100, train_loss 4.21486854553\n",
      "epoch 0, batch 101, train_loss 4.3503575325\n",
      "epoch 0, batch 102, train_loss 4.23415756226\n",
      "epoch 0, batch 103, train_loss 5.23091030121\n",
      "epoch 0, batch 104, train_loss 4.71583414078\n",
      "epoch 0, batch 105, train_loss 4.42645597458\n",
      "epoch 0, batch 106, train_loss 4.9903216362\n",
      "epoch 0, batch 107, train_loss 4.7625002861\n",
      "epoch 0, batch 108, train_loss 4.29942512512\n",
      "epoch 0, batch 109, train_loss 4.46889400482\n",
      "epoch 0, batch 110, train_loss 4.33966064453\n",
      "epoch 0, batch 111, train_loss 4.75728416443\n",
      "epoch 0, batch 112, train_loss 4.72546625137\n",
      "epoch 0, batch 113, train_loss 4.09027004242\n",
      "epoch 0, batch 114, train_loss 4.65346956253\n",
      "epoch 0, batch 115, train_loss 5.97394657135\n",
      "epoch 0, batch 116, train_loss 5.91051387787\n",
      "epoch 0, batch 117, train_loss 5.81106567383\n",
      "epoch 0, batch 118, train_loss 5.13133573532\n",
      "epoch 0, batch 119, train_loss 3.25995755196\n",
      "epoch 0, batch 120, train_loss 4.37866926193\n",
      "epoch 0, batch 121, train_loss 4.39395141602\n",
      "epoch 0, batch 122, train_loss 4.64674568176\n",
      "epoch 0, batch 123, train_loss 4.19336271286\n",
      "epoch 0, batch 124, train_loss 3.99235081673\n",
      "epoch 0, batch 125, train_loss 3.82837915421\n",
      "epoch 0, batch 126, train_loss 3.79717946053\n",
      "epoch 0, batch 127, train_loss 3.96102976799\n",
      "epoch 0, batch 128, train_loss 3.73325324059\n",
      "epoch 0, batch 129, train_loss 4.12557220459\n",
      "epoch 0, batch 130, train_loss 4.04455089569\n",
      "epoch 0, batch 131, train_loss 4.53175210953\n",
      "epoch 0, batch 132, train_loss 3.96861696243\n",
      "epoch 0, batch 133, train_loss 3.05857515335\n",
      "epoch 0, batch 134, train_loss 3.26989889145\n",
      "epoch 0, batch 135, train_loss 5.86355447769\n",
      "epoch 0, batch 136, train_loss 4.78443288803\n",
      "epoch 0, batch 137, train_loss 3.77935957909\n",
      "epoch 0, batch 138, train_loss 3.58754134178\n",
      "epoch 0, batch 139, train_loss 4.13349199295\n",
      "epoch 0, batch 140, train_loss 3.54660248756\n",
      "epoch 0, batch 141, train_loss 3.99428367615\n",
      "epoch 0, batch 142, train_loss 5.81745052338\n",
      "epoch 0, batch 143, train_loss 3.59034276009\n",
      "epoch 0, batch 144, train_loss 2.94648146629\n",
      "epoch 0, batch 145, train_loss 4.16317081451\n",
      "epoch 0, batch 146, train_loss 4.36644029617\n",
      "epoch 0, batch 147, train_loss 4.06654977798\n",
      "epoch 0, batch 148, train_loss 3.90424489975\n",
      "epoch 0, batch 149, train_loss 4.55087280273\n",
      "epoch 0, batch 150, train_loss 4.5354385376\n",
      "epoch 0, batch 151, train_loss 2.86030840874\n",
      "epoch 0, batch 152, train_loss 3.20869755745\n",
      "epoch 0, batch 153, train_loss 5.6768579483\n",
      "epoch 0, batch 154, train_loss 4.37765407562\n",
      "epoch 0, batch 155, train_loss 3.76825118065\n",
      "epoch 0, batch 156, train_loss 4.01808977127\n",
      "epoch 0, batch 157, train_loss 3.42810440063\n",
      "epoch 0, batch 158, train_loss 3.53280568123\n",
      "epoch 0, batch 159, train_loss 3.76090788841\n",
      "epoch 0, batch 160, train_loss 3.98255467415\n",
      "epoch 0, batch 161, train_loss 4.36421108246\n",
      "epoch 0, batch 162, train_loss 4.28790903091\n",
      "epoch 0, batch 163, train_loss 6.25936079025\n",
      "epoch 0, batch 164, train_loss 2.98947310448\n",
      "epoch 0, batch 165, train_loss 2.95945215225\n",
      "epoch 0, batch 166, train_loss 3.47767519951\n",
      "epoch 0, batch 167, train_loss 3.63512730598\n",
      "epoch 0, batch 168, train_loss 3.84910988808\n",
      "epoch 0, batch 169, train_loss 3.5910487175\n",
      "epoch 0, batch 170, train_loss 4.20826292038\n",
      "epoch 0, batch 171, train_loss 3.54826068878\n",
      "epoch 0, batch 172, train_loss 4.39202785492\n",
      "epoch 0, batch 173, train_loss 2.98787498474\n",
      "epoch 0, batch 174, train_loss 4.91192722321\n",
      "epoch 0, batch 175, train_loss 4.13906764984\n",
      "epoch 0, batch 176, train_loss 3.99685120583\n",
      "epoch 0, batch 177, train_loss 3.75815033913\n",
      "epoch 0, batch 178, train_loss 3.60798621178\n",
      "epoch 0, batch 179, train_loss 4.02031087875\n",
      "epoch 0, batch 180, train_loss 3.9013812542\n",
      "epoch 0, batch 181, train_loss 4.748691082\n",
      "epoch 0, batch 182, train_loss 4.05197143555\n",
      "epoch 0, batch 183, train_loss 4.55382013321\n",
      "epoch 0, batch 184, train_loss 3.06937670708\n",
      "epoch 0, batch 185, train_loss 3.55541944504\n",
      "epoch 0, batch 186, train_loss 4.44772434235\n",
      "epoch 0, batch 187, train_loss 3.35688257217\n",
      "epoch 0, batch 188, train_loss 3.65535378456\n",
      "epoch 0, batch 189, train_loss 3.66670179367\n",
      "epoch 0, batch 190, train_loss 4.44516611099\n",
      "epoch 0, batch 191, train_loss 4.34366559982\n",
      "epoch 0, batch 192, train_loss 5.73296165466\n",
      "epoch 0, batch 193, train_loss 5.40662193298\n",
      "epoch 0, batch 194, train_loss 6.23339653015\n",
      "epoch 0, batch 195, train_loss 6.26281499863\n",
      "epoch 0, batch 196, train_loss 4.15892457962\n",
      "epoch 0, batch 197, train_loss 3.41736292839\n",
      "epoch 0, batch 198, train_loss 3.09029483795\n",
      "epoch 0, batch 199, train_loss 4.41568899155\n",
      "epoch 0, batch 200, train_loss 3.66261506081\n",
      "epoch 0, batch 201, train_loss 3.95538568497\n",
      "epoch 0, batch 202, train_loss 4.33785104752\n",
      "epoch 0, batch 203, train_loss 4.46919107437\n",
      "epoch 0, batch 204, train_loss 4.45097303391\n",
      "epoch 0, batch 205, train_loss 4.03197145462\n",
      "epoch 0, batch 206, train_loss 5.62009143829\n",
      "epoch 0, batch 207, train_loss 4.64622974396\n",
      "epoch 0, batch 208, train_loss 5.62527322769\n",
      "epoch 0, batch 209, train_loss 5.43072938919\n",
      "epoch 0, batch 210, train_loss 2.86235952377\n",
      "epoch 0, batch 211, train_loss 3.59192156792\n",
      "epoch 0, batch 212, train_loss 4.68155050278\n",
      "epoch 0, batch 213, train_loss 3.89391350746\n",
      "epoch 0, batch 214, train_loss 3.90247416496\n",
      "epoch 0, batch 215, train_loss 2.97755026817\n",
      "epoch 0, batch 216, train_loss 4.49536895752\n",
      "epoch 0, batch 217, train_loss 3.70617437363\n",
      "epoch 0, batch 218, train_loss 4.25206279755\n",
      "epoch 0, batch 219, train_loss 4.32223510742\n",
      "epoch 0, batch 220, train_loss 4.31408929825\n",
      "epoch 0, batch 221, train_loss 4.5586400032\n",
      "epoch 0, batch 222, train_loss 3.07857966423\n",
      "epoch 0, batch 223, train_loss 3.11779665947\n",
      "epoch 0, batch 224, train_loss 3.46836328506\n",
      "epoch 0, batch 225, train_loss 3.6691775322\n",
      "epoch 0, batch 226, train_loss 4.37956666946\n",
      "epoch 0, batch 227, train_loss 4.04743909836\n",
      "epoch 0, batch 228, train_loss 4.25717020035\n",
      "epoch 0, batch 229, train_loss 4.01529121399\n",
      "epoch 0, batch 230, train_loss 3.82061886787\n",
      "epoch 0, batch 231, train_loss 3.45738792419\n",
      "epoch 0, batch 232, train_loss 4.29177618027\n",
      "epoch 0, batch 233, train_loss 4.4893488884\n",
      "epoch 0, batch 234, train_loss 4.00428581238\n",
      "epoch 0, batch 235, train_loss 4.80354881287\n",
      "epoch 0, batch 236, train_loss 4.13170289993\n",
      "epoch 0, batch 237, train_loss 4.03780460358\n",
      "epoch 0, batch 238, train_loss 4.07118558884\n",
      "epoch 0, batch 239, train_loss 3.77612066269\n",
      "epoch 0, batch 240, train_loss 4.36179637909\n",
      "epoch 0, batch 241, train_loss 4.23047685623\n",
      "epoch 0, batch 242, train_loss 4.07616949081\n",
      "epoch 0, batch 243, train_loss 4.30242681503\n",
      "epoch 0, batch 244, train_loss 4.45010852814\n",
      "epoch 0, batch 245, train_loss 4.1401257515\n",
      "epoch 0, batch 246, train_loss 4.05783510208\n",
      "epoch 0, batch 247, train_loss 4.25779438019\n",
      "epoch 0, batch 248, train_loss 3.98847746849\n",
      "epoch 0, batch 249, train_loss 4.77642679214\n",
      "epoch 0, batch 250, train_loss 4.63564157486\n",
      "epoch 0, batch 251, train_loss 5.25745344162\n",
      "epoch 0, batch 252, train_loss 3.49850416183\n",
      "epoch 0, batch 253, train_loss 2.85009026527\n",
      "epoch 0, batch 254, train_loss 3.73794150352\n",
      "epoch 0, batch 255, train_loss 2.88413119316\n",
      "epoch 0, batch 256, train_loss 3.36722397804\n",
      "epoch 0, batch 257, train_loss 4.01509141922\n",
      "epoch 0, batch 258, train_loss 3.20820999146\n",
      "epoch 0, batch 259, train_loss 3.18793416023\n",
      "epoch 0, batch 260, train_loss 2.73374080658\n",
      "epoch 0, batch 261, train_loss 4.55152845383\n",
      "epoch 0, batch 262, train_loss 5.28297948837\n",
      "epoch 0, batch 263, train_loss 3.94804692268\n",
      "epoch 0, batch 264, train_loss 3.6419479847\n",
      "epoch 0, batch 265, train_loss 4.55876588821\n",
      "epoch 0, batch 266, train_loss 4.63870096207\n",
      "epoch 0, batch 267, train_loss 3.59546279907\n",
      "epoch 0, batch 268, train_loss 2.98827648163\n",
      "epoch 0, batch 269, train_loss 3.1903898716\n",
      "epoch 0, batch 270, train_loss 3.57147789001\n",
      "epoch 0, batch 271, train_loss 4.07308530807\n",
      "epoch 0, batch 272, train_loss 4.03185749054\n",
      "epoch 0, batch 273, train_loss 4.19482135773\n",
      "epoch 0, batch 274, train_loss 3.81604957581\n",
      "epoch 0, batch 275, train_loss 3.9862909317\n",
      "epoch 0, batch 276, train_loss 3.61546874046\n",
      "epoch 0, batch 277, train_loss 4.7093963623\n",
      "epoch 0, batch 278, train_loss 3.69914293289\n",
      "epoch 0, batch 279, train_loss 4.32193374634\n",
      "epoch 0, batch 280, train_loss 4.34962081909\n",
      "epoch 0, batch 281, train_loss 4.07547092438\n",
      "epoch 0, batch 282, train_loss 3.56150770187\n",
      "epoch 0, batch 283, train_loss 5.73053121567\n",
      "epoch 0, batch 284, train_loss 4.29524612427\n",
      "epoch 0, batch 285, train_loss 2.64885640144\n",
      "epoch 0, batch 286, train_loss 2.9267616272\n",
      "epoch 0, batch 287, train_loss 3.06051278114\n",
      "epoch 0, batch 288, train_loss 3.85727620125\n",
      "epoch 0, batch 289, train_loss 4.00075483322\n",
      "epoch 0, batch 290, train_loss 3.86221647263\n",
      "epoch 0, batch 291, train_loss 3.62543582916\n",
      "epoch 0, batch 292, train_loss 3.01609420776\n",
      "epoch 0, batch 293, train_loss 4.35849475861\n",
      "epoch 0, batch 294, train_loss 3.79858231544\n",
      "epoch 0, batch 295, train_loss 3.42250275612\n",
      "epoch 0, batch 296, train_loss 3.75173950195\n",
      "epoch 0, batch 297, train_loss 3.44691061974\n",
      "epoch 0, batch 298, train_loss 3.85274577141\n",
      "epoch 0, batch 299, train_loss 4.62927722931\n",
      "epoch 0, batch 300, train_loss 4.20410346985\n",
      "epoch 0, batch 301, train_loss 5.56083011627\n",
      "epoch 0, batch 302, train_loss 5.53588151932\n",
      "epoch 0, batch 303, train_loss 5.14999628067\n",
      "epoch 0, batch 304, train_loss 6.04363059998\n",
      "epoch 0, batch 305, train_loss 5.98729515076\n",
      "epoch 0, batch 306, train_loss 4.22729206085\n",
      "epoch 0, batch 307, train_loss 3.32937312126\n",
      "epoch 0, batch 308, train_loss 3.86200404167\n",
      "epoch 0, batch 309, train_loss 4.03581953049\n",
      "epoch 0, batch 310, train_loss 3.79059839249\n",
      "epoch 0, batch 311, train_loss 3.97059583664\n",
      "epoch 0, batch 312, train_loss 4.14836454391\n",
      "epoch 0, batch 313, train_loss 3.7034740448\n",
      "epoch 0, batch 314, train_loss 4.26631355286\n",
      "epoch 0, batch 315, train_loss 3.55287361145\n",
      "epoch 0, batch 316, train_loss 3.70565962791\n",
      "epoch 0, batch 317, train_loss 4.36252307892\n",
      "epoch 0, batch 318, train_loss 3.25689530373\n",
      "epoch 0, batch 319, train_loss 5.62656259537\n",
      "epoch 0, batch 320, train_loss 4.21637392044\n",
      "epoch 0, batch 321, train_loss 5.87630319595\n",
      "epoch 0, batch 322, train_loss 3.23754763603\n",
      "epoch 0, batch 323, train_loss 3.06231117249\n",
      "epoch 0, batch 324, train_loss 4.46458959579\n",
      "epoch 0, batch 325, train_loss 3.97020959854\n",
      "epoch 0, batch 326, train_loss 4.01058292389\n",
      "epoch 0, batch 327, train_loss 4.00006389618\n",
      "epoch 0, batch 328, train_loss 3.83052921295\n",
      "epoch 0, batch 329, train_loss 3.34696578979\n",
      "epoch 0, batch 330, train_loss 5.04600572586\n",
      "epoch 0, batch 331, train_loss 4.56381988525\n",
      "epoch 0, batch 332, train_loss 3.16483783722\n",
      "epoch 0, batch 333, train_loss 4.60241603851\n",
      "epoch 0, batch 334, train_loss 4.18669128418\n",
      "epoch 0, batch 335, train_loss 3.99901938438\n",
      "epoch 0, batch 336, train_loss 4.84636449814\n",
      "epoch 0, batch 337, train_loss 2.93619251251\n",
      "epoch 0, batch 338, train_loss 3.74530363083\n",
      "epoch 0, batch 339, train_loss 3.69859099388\n",
      "epoch 0, batch 340, train_loss 3.88472986221\n",
      "epoch 0, batch 341, train_loss 3.76908040047\n",
      "epoch 0, batch 342, train_loss 3.84669780731\n",
      "epoch 0, batch 343, train_loss 3.76121282578\n",
      "epoch 0, batch 344, train_loss 5.0171046257\n",
      "epoch 0, batch 345, train_loss 4.54631376266\n",
      "epoch 0, batch 346, train_loss 4.09905910492\n",
      "epoch 0, batch 347, train_loss 3.76767420769\n",
      "epoch 0, batch 348, train_loss 3.81727218628\n",
      "epoch 0, batch 349, train_loss 5.6616563797\n",
      "epoch 0, batch 350, train_loss 5.53168725967\n",
      "epoch 0, batch 351, train_loss 5.08137655258\n",
      "epoch 0, batch 352, train_loss 5.25301218033\n",
      "epoch 0, batch 353, train_loss 4.23588943481\n",
      "epoch 0, batch 354, train_loss 3.34244275093\n",
      "epoch 0, batch 355, train_loss 3.65806627274\n",
      "epoch 0, batch 356, train_loss 3.8701133728\n",
      "epoch 0, batch 357, train_loss 3.72138595581\n",
      "epoch 0, batch 358, train_loss 3.1609621048\n",
      "epoch 0, batch 359, train_loss 4.28317403793\n",
      "epoch 0, batch 360, train_loss 3.79744338989\n",
      "epoch 0, batch 361, train_loss 3.31725740433\n",
      "epoch 0, batch 362, train_loss 5.52104377747\n",
      "epoch 0, batch 363, train_loss 3.98848986626\n",
      "epoch 0, batch 364, train_loss 3.8113296032\n",
      "epoch 0, batch 365, train_loss 4.7512421608\n",
      "epoch 0, batch 366, train_loss 5.2196149826\n",
      "epoch 0, batch 367, train_loss 6.09133005142\n",
      "epoch 0, batch 368, train_loss 3.07438707352\n",
      "epoch 0, batch 369, train_loss 4.16105461121\n",
      "epoch 0, batch 370, train_loss 4.75353574753\n",
      "epoch 0, batch 371, train_loss 6.07709932327\n",
      "epoch 0, batch 372, train_loss 2.91735768318\n",
      "epoch 0, batch 373, train_loss 3.1723074913\n",
      "epoch 0, batch 374, train_loss 3.43794894218\n",
      "epoch 0, batch 375, train_loss 4.03593730927\n",
      "epoch 0, batch 376, train_loss 3.11885309219\n",
      "epoch 0, batch 377, train_loss 3.6949775219\n",
      "epoch 0, batch 378, train_loss 4.36592054367\n",
      "epoch 0, batch 379, train_loss 3.55461168289\n",
      "epoch 0, batch 380, train_loss 3.67083334923\n",
      "epoch 0, batch 381, train_loss 3.82017230988\n",
      "epoch 0, batch 382, train_loss 4.96751403809\n",
      "epoch 0, batch 383, train_loss 4.21177387238\n",
      "epoch 0, batch 384, train_loss 3.82437372208\n",
      "epoch 0, batch 385, train_loss 5.69433736801\n",
      "epoch 0, batch 386, train_loss 3.5868241787\n",
      "epoch 0, batch 387, train_loss 6.13491630554\n",
      "epoch 0, batch 388, train_loss 3.51601433754\n",
      "epoch 0, batch 389, train_loss 5.57183218002\n",
      "epoch 0, batch 390, train_loss 5.4924249649\n",
      "epoch 0, batch 391, train_loss 5.50259780884\n",
      "epoch 0, batch 392, train_loss 5.15442943573\n",
      "epoch 0, batch 393, train_loss 4.33720064163\n",
      "epoch 0, batch 394, train_loss 5.16271257401\n",
      "epoch 0, batch 395, train_loss 3.83248400688\n",
      "epoch 0, batch 396, train_loss 5.45563983917\n",
      "epoch 0, batch 397, train_loss 5.68706035614\n",
      "epoch 0, batch 398, train_loss 4.12458992004\n",
      "epoch 0, batch 399, train_loss 4.45837640762\n",
      "epoch 0, batch 400, train_loss 5.76188421249\n",
      "epoch 0, batch 401, train_loss 4.83317804337\n",
      "epoch 0, batch 402, train_loss 4.1568775177\n",
      "epoch 0, batch 403, train_loss 4.19066810608\n",
      "epoch 0, batch 404, train_loss 5.15099382401\n",
      "epoch 0, batch 405, train_loss 4.46157360077\n",
      "epoch 0, batch 406, train_loss 4.29431819916\n",
      "epoch 0, batch 407, train_loss 4.05408763885\n",
      "epoch 0, batch 408, train_loss 3.85923433304\n",
      "epoch 0, batch 409, train_loss 3.23937296867\n",
      "epoch 0, batch 410, train_loss 5.32819461823\n",
      "epoch 0, batch 411, train_loss 4.07547998428\n",
      "epoch 0, batch 412, train_loss 3.64405703545\n",
      "epoch 0, batch 413, train_loss 4.14623022079\n",
      "epoch 0, batch 414, train_loss 4.55551767349\n",
      "epoch 0, batch 415, train_loss 4.68043041229\n",
      "epoch 0, batch 416, train_loss 4.42817306519\n",
      "epoch 0, batch 417, train_loss 3.75704503059\n",
      "epoch 0, batch 418, train_loss 4.35113191605\n",
      "epoch 0, batch 419, train_loss 3.88233375549\n",
      "epoch 0, batch 420, train_loss 3.12348341942\n",
      "epoch 0, batch 421, train_loss 5.55298471451\n",
      "epoch 0, batch 422, train_loss 3.89285945892\n",
      "epoch 0, batch 423, train_loss 3.97937655449\n",
      "epoch 0, batch 424, train_loss 5.41088628769\n",
      "epoch 0, batch 425, train_loss 4.50553035736\n",
      "epoch 0, batch 426, train_loss 5.98298692703\n",
      "epoch 0, batch 427, train_loss 4.90008544922\n",
      "epoch 0, batch 428, train_loss 3.33180141449\n",
      "epoch 0, batch 429, train_loss 4.81607866287\n",
      "epoch 0, batch 430, train_loss 4.85602998734\n",
      "epoch 0, batch 431, train_loss 4.42977905273\n",
      "epoch 0, batch 432, train_loss 4.63988828659\n",
      "epoch 0, batch 433, train_loss 3.78562808037\n",
      "epoch 0, batch 434, train_loss 4.28161144257\n",
      "epoch 0, batch 435, train_loss 4.07950735092\n",
      "epoch 0, batch 436, train_loss 4.47283506393\n",
      "epoch 0, batch 437, train_loss 5.1016998291\n",
      "epoch 0, batch 438, train_loss 4.60801315308\n",
      "epoch 0, batch 439, train_loss 4.29301834106\n",
      "epoch 0, batch 440, train_loss 6.04248714447\n",
      "epoch 0, batch 441, train_loss 6.180164814\n",
      "epoch 0, batch 442, train_loss 5.05666351318\n",
      "epoch 0, batch 443, train_loss 3.12716269493\n",
      "epoch 0, batch 444, train_loss 3.73380112648\n",
      "epoch 0, batch 445, train_loss 5.62937879562\n",
      "epoch 0, batch 446, train_loss 4.17040634155\n",
      "epoch 0, batch 447, train_loss 3.73420333862\n",
      "epoch 0, batch 448, train_loss 5.48980998993\n",
      "epoch 0, batch 449, train_loss 4.63969945908\n",
      "epoch 0, batch 450, train_loss 3.97720098495\n",
      "epoch 0, batch 451, train_loss 3.86694550514\n",
      "epoch 0, batch 452, train_loss 3.3824570179\n",
      "epoch 0, batch 453, train_loss 6.31932449341\n",
      "epoch 0, batch 454, train_loss 6.35387992859\n",
      "epoch 0, batch 455, train_loss 3.19943499565\n",
      "epoch 0, batch 456, train_loss 4.093501091\n",
      "epoch 0, batch 457, train_loss 4.49935865402\n",
      "epoch 0, batch 458, train_loss 3.93061137199\n",
      "epoch 0, batch 459, train_loss 4.3922791481\n",
      "epoch 0, batch 460, train_loss 5.14423179626\n",
      "epoch 0, batch 461, train_loss 4.21469306946\n",
      "epoch 0, batch 462, train_loss 3.98866319656\n",
      "epoch 0, batch 463, train_loss 4.21250200272\n",
      "epoch 0, batch 464, train_loss 4.53426551819\n",
      "epoch 0, batch 465, train_loss 3.76310443878\n",
      "epoch 0, batch 466, train_loss 4.75271511078\n",
      "epoch 0, batch 467, train_loss 4.51064157486\n",
      "epoch 0, batch 468, train_loss 4.66274118423\n",
      "epoch 0, batch 469, train_loss 4.07630872726\n",
      "epoch 0, batch 470, train_loss 3.92494392395\n",
      "epoch 0, batch 471, train_loss 4.00270843506\n",
      "epoch 0, batch 472, train_loss 3.79612159729\n",
      "epoch 0, batch 473, train_loss 4.73327493668\n",
      "epoch 0, batch 474, train_loss 5.53087043762\n",
      "epoch 0, batch 475, train_loss 3.80305767059\n",
      "epoch 0, batch 476, train_loss 3.22691845894\n",
      "epoch 0, batch 477, train_loss 3.34272217751\n",
      "epoch 0, batch 478, train_loss 3.14559841156\n",
      "epoch 0, batch 479, train_loss 3.84191608429\n",
      "epoch 0, batch 480, train_loss 4.00402164459\n",
      "epoch 0, batch 481, train_loss 5.96455669403\n",
      "epoch 0, batch 482, train_loss 3.67119908333\n",
      "epoch 0, batch 483, train_loss 4.4673409462\n",
      "epoch 0, batch 484, train_loss 4.00815153122\n",
      "epoch 0, batch 485, train_loss 3.03610754013\n",
      "epoch 0, batch 486, train_loss 3.05356526375\n",
      "epoch 0, batch 487, train_loss 3.2453918457\n",
      "epoch 0, batch 488, train_loss 3.7751019001\n",
      "epoch 0, batch 489, train_loss 3.78127002716\n",
      "epoch 0, batch 490, train_loss 3.87323403358\n",
      "epoch 0, batch 491, train_loss 3.76390528679\n",
      "epoch 0, batch 492, train_loss 4.00458574295\n",
      "epoch 0, batch 493, train_loss 3.52814936638\n",
      "epoch 0, batch 494, train_loss 3.62371587753\n",
      "epoch 0, batch 495, train_loss 3.75691342354\n",
      "epoch 0, batch 496, train_loss 3.29300165176\n",
      "epoch 0, batch 497, train_loss 3.53201818466\n",
      "epoch 0, batch 498, train_loss 3.6610891819\n",
      "epoch 0, batch 499, train_loss 3.66332054138\n",
      "epoch 0, batch 500, train_loss 4.1465139389\n",
      "epoch 0, batch 501, train_loss 3.80199742317\n",
      "epoch 0, batch 502, train_loss 5.45810174942\n",
      "epoch 0, batch 503, train_loss 5.44186162949\n",
      "epoch 0, batch 504, train_loss 4.69726848602\n",
      "epoch 0, batch 505, train_loss 4.48047876358\n",
      "epoch 0, batch 506, train_loss 4.06622743607\n",
      "epoch 0, batch 507, train_loss 5.2724070549\n",
      "epoch 0, batch 508, train_loss 3.7097094059\n",
      "epoch 0, batch 509, train_loss 5.43467998505\n",
      "epoch 0, batch 510, train_loss 5.49204397202\n",
      "epoch 0, batch 511, train_loss 5.4632062912\n",
      "epoch 0, batch 512, train_loss 4.21502637863\n",
      "epoch 0, batch 513, train_loss 5.98580121994\n",
      "epoch 0, batch 514, train_loss 5.96509552002\n",
      "epoch 0, batch 515, train_loss 5.87947797775\n",
      "epoch 0, batch 516, train_loss 3.5612514019\n",
      "epoch 0, batch 517, train_loss 3.78850436211\n",
      "epoch 0, batch 518, train_loss 3.69101953506\n",
      "epoch 0, batch 519, train_loss 3.25132679939\n",
      "epoch 0, batch 520, train_loss 4.60056781769\n",
      "epoch 0, batch 521, train_loss 3.7255115509\n",
      "epoch 0, batch 522, train_loss 3.15077710152\n",
      "epoch 0, batch 523, train_loss 3.1581223011\n",
      "epoch 0, batch 524, train_loss 3.02779483795\n",
      "epoch 0, batch 525, train_loss 2.84500980377\n",
      "epoch 0, batch 526, train_loss 2.92171740532\n",
      "epoch 0, batch 527, train_loss 2.83749747276\n",
      "epoch 0, batch 528, train_loss 3.22462797165\n",
      "epoch 0, batch 529, train_loss 3.05812478065\n",
      "epoch 0, batch 530, train_loss 2.40257549286\n",
      "epoch 0, batch 531, train_loss 2.02310419083\n",
      "epoch 0, batch 532, train_loss 3.12765979767\n",
      "epoch 0, batch 533, train_loss 1.97478425503\n",
      "epoch 0, batch 534, train_loss 1.99118924141\n",
      "epoch 0, batch 535, train_loss 3.91252017021\n",
      "epoch 0, batch 536, train_loss 4.25888204575\n",
      "epoch 0, batch 537, train_loss 4.10919380188\n",
      "epoch 0, batch 538, train_loss 4.37776327133\n",
      "epoch 0, batch 539, train_loss 4.74763250351\n",
      "epoch 0, batch 540, train_loss 5.10568237305\n",
      "epoch 1, batch 0, train_loss 4.03569793701\n",
      "epoch 1, batch 1, train_loss 4.55283403397\n",
      "epoch 1, batch 2, train_loss 3.79455804825\n",
      "epoch 1, batch 3, train_loss 4.1748714447\n",
      "epoch 1, batch 4, train_loss 3.90277886391\n",
      "epoch 1, batch 5, train_loss 3.96080899239\n",
      "epoch 1, batch 6, train_loss 5.35044670105\n",
      "epoch 1, batch 7, train_loss 3.97538518906\n",
      "epoch 1, batch 8, train_loss 3.4800889492\n",
      "epoch 1, batch 9, train_loss 4.77449703217\n",
      "epoch 1, batch 10, train_loss 3.71443009377\n",
      "epoch 1, batch 11, train_loss 3.94190621376\n",
      "epoch 1, batch 12, train_loss 3.53872442245\n",
      "epoch 1, batch 13, train_loss 4.15898609161\n",
      "epoch 1, batch 14, train_loss 3.3819463253\n",
      "epoch 1, batch 15, train_loss 4.20164632797\n",
      "epoch 1, batch 16, train_loss 3.69441223145\n",
      "epoch 1, batch 17, train_loss 3.5658185482\n",
      "epoch 1, batch 18, train_loss 3.18181467056\n",
      "epoch 1, batch 19, train_loss 2.64629673958\n",
      "epoch 1, batch 20, train_loss 3.65681791306\n",
      "epoch 1, batch 21, train_loss 3.47241544724\n",
      "epoch 1, batch 22, train_loss 4.09844207764\n",
      "epoch 1, batch 23, train_loss 3.5584218502\n",
      "epoch 1, batch 24, train_loss 3.52310919762\n",
      "epoch 1, batch 25, train_loss 4.10055923462\n",
      "epoch 1, batch 26, train_loss 3.99904823303\n",
      "epoch 1, batch 27, train_loss 3.93981933594\n",
      "epoch 1, batch 28, train_loss 4.34332942963\n",
      "epoch 1, batch 29, train_loss 4.25706768036\n",
      "epoch 1, batch 30, train_loss 4.34524059296\n",
      "epoch 1, batch 31, train_loss 3.70642709732\n",
      "epoch 1, batch 32, train_loss 3.35651779175\n",
      "epoch 1, batch 33, train_loss 5.72210693359\n",
      "epoch 1, batch 34, train_loss 5.79455518723\n",
      "epoch 1, batch 35, train_loss 4.08294916153\n",
      "epoch 1, batch 36, train_loss 4.04407930374\n",
      "epoch 1, batch 37, train_loss 4.07940006256\n",
      "epoch 1, batch 38, train_loss 4.05167150497\n",
      "epoch 1, batch 39, train_loss 4.6798324585\n",
      "epoch 1, batch 40, train_loss 3.65026283264\n",
      "epoch 1, batch 41, train_loss 3.90429282188\n",
      "epoch 1, batch 42, train_loss 4.02873373032\n",
      "epoch 1, batch 43, train_loss 3.76268577576\n",
      "epoch 1, batch 44, train_loss 4.30625104904\n",
      "epoch 1, batch 45, train_loss 4.02575349808\n",
      "epoch 1, batch 46, train_loss 3.92907381058\n",
      "epoch 1, batch 47, train_loss 4.12792301178\n",
      "epoch 1, batch 48, train_loss 3.89656853676\n",
      "epoch 1, batch 49, train_loss 4.14073228836\n",
      "epoch 1, batch 50, train_loss 3.91934704781\n",
      "epoch 1, batch 51, train_loss 3.57054495811\n",
      "epoch 1, batch 52, train_loss 4.07730007172\n",
      "epoch 1, batch 53, train_loss 4.00863027573\n",
      "epoch 1, batch 54, train_loss 3.62445640564\n",
      "epoch 1, batch 55, train_loss 3.68337154388\n",
      "epoch 1, batch 56, train_loss 3.55852937698\n",
      "epoch 1, batch 57, train_loss 4.12835693359\n",
      "epoch 1, batch 58, train_loss 3.91729974747\n",
      "epoch 1, batch 59, train_loss 3.93139147758\n",
      "epoch 1, batch 60, train_loss 4.14896869659\n",
      "epoch 1, batch 61, train_loss 2.47311925888\n",
      "epoch 1, batch 62, train_loss 3.6193459034\n",
      "epoch 1, batch 63, train_loss 3.81723570824\n",
      "epoch 1, batch 64, train_loss 3.97815513611\n",
      "epoch 1, batch 65, train_loss 4.3849606514\n",
      "epoch 1, batch 66, train_loss 4.06291484833\n",
      "epoch 1, batch 67, train_loss 2.74400949478\n",
      "epoch 1, batch 68, train_loss 3.78999781609\n",
      "epoch 1, batch 69, train_loss 3.08283877373\n",
      "epoch 1, batch 70, train_loss 5.27210092545\n",
      "epoch 1, batch 71, train_loss 5.2657790184\n",
      "epoch 1, batch 72, train_loss 3.64908337593\n",
      "epoch 1, batch 73, train_loss 4.18790912628\n",
      "epoch 1, batch 74, train_loss 5.11829948425\n",
      "epoch 1, batch 75, train_loss 4.2195854187\n",
      "epoch 1, batch 76, train_loss 4.38686084747\n",
      "epoch 1, batch 77, train_loss 5.39202404022\n",
      "epoch 1, batch 78, train_loss 3.90495967865\n",
      "epoch 1, batch 79, train_loss 3.88149785995\n",
      "epoch 1, batch 80, train_loss 3.62126350403\n",
      "epoch 1, batch 81, train_loss 3.7996571064\n",
      "epoch 1, batch 82, train_loss 3.65281486511\n",
      "epoch 1, batch 83, train_loss 3.78800582886\n",
      "epoch 1, batch 84, train_loss 3.55545949936\n",
      "epoch 1, batch 85, train_loss 3.61827468872\n",
      "epoch 1, batch 86, train_loss 4.12354898453\n",
      "epoch 1, batch 87, train_loss 3.22995996475\n",
      "epoch 1, batch 88, train_loss 3.55770301819\n",
      "epoch 1, batch 89, train_loss 3.86910319328\n",
      "epoch 1, batch 90, train_loss 3.46887373924\n",
      "epoch 1, batch 91, train_loss 3.71066474915\n",
      "epoch 1, batch 92, train_loss 4.223716259\n",
      "epoch 1, batch 93, train_loss 5.24920415878\n",
      "epoch 1, batch 94, train_loss 3.71132111549\n",
      "epoch 1, batch 95, train_loss 3.86502814293\n",
      "epoch 1, batch 96, train_loss 3.06340098381\n",
      "epoch 1, batch 97, train_loss 3.79259419441\n",
      "epoch 1, batch 98, train_loss 3.75406217575\n",
      "epoch 1, batch 99, train_loss 4.16297483444\n",
      "epoch 1, batch 100, train_loss 3.79493117332\n",
      "epoch 1, batch 101, train_loss 3.99298095703\n",
      "epoch 1, batch 102, train_loss 3.80840444565\n",
      "epoch 1, batch 103, train_loss 4.94452142715\n",
      "epoch 1, batch 104, train_loss 4.28900194168\n",
      "epoch 1, batch 105, train_loss 4.02767133713\n",
      "epoch 1, batch 106, train_loss 4.53741645813\n",
      "epoch 1, batch 107, train_loss 4.34032058716\n",
      "epoch 1, batch 108, train_loss 3.9093735218\n",
      "epoch 1, batch 109, train_loss 4.09081125259\n",
      "epoch 1, batch 110, train_loss 3.92434287071\n",
      "epoch 1, batch 111, train_loss 4.36916542053\n",
      "epoch 1, batch 112, train_loss 4.33197164536\n",
      "epoch 1, batch 113, train_loss 3.72836375237\n",
      "epoch 1, batch 114, train_loss 4.33453416824\n",
      "epoch 1, batch 115, train_loss 5.3988904953\n",
      "epoch 1, batch 116, train_loss 5.31007099152\n",
      "epoch 1, batch 117, train_loss 5.31292247772\n",
      "epoch 1, batch 118, train_loss 4.71122169495\n",
      "epoch 1, batch 119, train_loss 3.08059644699\n",
      "epoch 1, batch 120, train_loss 4.02363204956\n",
      "epoch 1, batch 121, train_loss 3.96158576012\n",
      "epoch 1, batch 122, train_loss 4.24344587326\n",
      "epoch 1, batch 123, train_loss 3.84969472885\n",
      "epoch 1, batch 124, train_loss 3.63944792747\n",
      "epoch 1, batch 125, train_loss 3.51284503937\n",
      "epoch 1, batch 126, train_loss 3.46637248993\n",
      "epoch 1, batch 127, train_loss 3.61944508553\n",
      "epoch 1, batch 128, train_loss 3.4420363903\n",
      "epoch 1, batch 129, train_loss 3.79691410065\n",
      "epoch 1, batch 130, train_loss 3.73045182228\n",
      "epoch 1, batch 131, train_loss 4.15658998489\n",
      "epoch 1, batch 132, train_loss 3.75572037697\n",
      "epoch 1, batch 133, train_loss 2.85215687752\n",
      "epoch 1, batch 134, train_loss 3.03316283226\n",
      "epoch 1, batch 135, train_loss 5.28458070755\n",
      "epoch 1, batch 136, train_loss 4.4214091301\n",
      "epoch 1, batch 137, train_loss 3.47975826263\n",
      "epoch 1, batch 138, train_loss 3.29363942146\n",
      "epoch 1, batch 139, train_loss 3.82345867157\n",
      "epoch 1, batch 140, train_loss 3.27171564102\n",
      "epoch 1, batch 141, train_loss 3.6983757019\n",
      "epoch 1, batch 142, train_loss 5.29715204239\n",
      "epoch 1, batch 143, train_loss 3.33513045311\n",
      "epoch 1, batch 144, train_loss 2.72682142258\n",
      "epoch 1, batch 145, train_loss 3.86191701889\n",
      "epoch 1, batch 146, train_loss 4.08110761642\n",
      "epoch 1, batch 147, train_loss 3.77802968025\n",
      "epoch 1, batch 148, train_loss 3.62908434868\n",
      "epoch 1, batch 149, train_loss 4.20814657211\n",
      "epoch 1, batch 150, train_loss 4.23430585861\n",
      "epoch 1, batch 151, train_loss 2.65506291389\n",
      "epoch 1, batch 152, train_loss 3.00840592384\n",
      "epoch 1, batch 153, train_loss 5.19764852524\n",
      "epoch 1, batch 154, train_loss 4.02876138687\n",
      "epoch 1, batch 155, train_loss 3.5098657608\n",
      "epoch 1, batch 156, train_loss 3.7391076088\n",
      "epoch 1, batch 157, train_loss 3.18196678162\n",
      "epoch 1, batch 158, train_loss 3.29227972031\n",
      "epoch 1, batch 159, train_loss 3.50201892853\n",
      "epoch 1, batch 160, train_loss 3.67280125618\n",
      "epoch 1, batch 161, train_loss 4.18217229843\n",
      "epoch 1, batch 162, train_loss 4.0320930481\n",
      "epoch 1, batch 163, train_loss 5.91420221329\n",
      "epoch 1, batch 164, train_loss 2.85397791862\n",
      "epoch 1, batch 165, train_loss 2.82868313789\n",
      "epoch 1, batch 166, train_loss 3.3008556366\n",
      "epoch 1, batch 167, train_loss 3.35719227791\n",
      "epoch 1, batch 168, train_loss 3.58634543419\n",
      "epoch 1, batch 169, train_loss 3.31739854813\n",
      "epoch 1, batch 170, train_loss 3.97673892975\n",
      "epoch 1, batch 171, train_loss 3.31140518188\n",
      "epoch 1, batch 172, train_loss 4.08845043182\n",
      "epoch 1, batch 173, train_loss 2.75019598007\n",
      "epoch 1, batch 174, train_loss 4.58635997772\n",
      "epoch 1, batch 175, train_loss 3.89822053909\n",
      "epoch 1, batch 176, train_loss 3.78456020355\n",
      "epoch 1, batch 177, train_loss 3.54078507423\n",
      "epoch 1, batch 178, train_loss 3.44659757614\n",
      "epoch 1, batch 179, train_loss 3.76939630508\n",
      "epoch 1, batch 180, train_loss 3.64974069595\n",
      "epoch 1, batch 181, train_loss 4.48072576523\n",
      "epoch 1, batch 182, train_loss 3.79775166512\n",
      "epoch 1, batch 183, train_loss 4.37708616257\n",
      "epoch 1, batch 184, train_loss 2.90615034103\n",
      "epoch 1, batch 185, train_loss 3.34343528748\n",
      "epoch 1, batch 186, train_loss 4.20519208908\n",
      "epoch 1, batch 187, train_loss 3.16430282593\n",
      "epoch 1, batch 188, train_loss 3.45424365997\n",
      "epoch 1, batch 189, train_loss 3.47490596771\n",
      "epoch 1, batch 190, train_loss 4.29508638382\n",
      "epoch 1, batch 191, train_loss 4.14949941635\n",
      "epoch 1, batch 192, train_loss 5.38326787949\n",
      "epoch 1, batch 193, train_loss 5.07991981506\n",
      "epoch 1, batch 194, train_loss 5.8564658165\n",
      "epoch 1, batch 195, train_loss 5.90642118454\n",
      "epoch 1, batch 196, train_loss 3.94013524055\n",
      "epoch 1, batch 197, train_loss 3.21935153008\n",
      "epoch 1, batch 198, train_loss 2.90653276443\n",
      "epoch 1, batch 199, train_loss 4.11266136169\n",
      "epoch 1, batch 200, train_loss 3.41488814354\n",
      "epoch 1, batch 201, train_loss 3.76165390015\n",
      "epoch 1, batch 202, train_loss 4.18171691895\n",
      "epoch 1, batch 203, train_loss 4.31428575516\n",
      "epoch 1, batch 204, train_loss 4.29363918304\n",
      "epoch 1, batch 205, train_loss 3.85331678391\n",
      "epoch 1, batch 206, train_loss 5.21964740753\n",
      "epoch 1, batch 207, train_loss 4.32671403885\n",
      "epoch 1, batch 208, train_loss 5.21872329712\n",
      "epoch 1, batch 209, train_loss 5.04037284851\n",
      "epoch 1, batch 210, train_loss 2.68640065193\n",
      "epoch 1, batch 211, train_loss 3.48628282547\n",
      "epoch 1, batch 212, train_loss 4.53898191452\n",
      "epoch 1, batch 213, train_loss 3.77712845802\n",
      "epoch 1, batch 214, train_loss 3.67103004456\n",
      "epoch 1, batch 215, train_loss 2.7972638607\n",
      "epoch 1, batch 216, train_loss 4.24319791794\n",
      "epoch 1, batch 217, train_loss 3.51474738121\n",
      "epoch 1, batch 218, train_loss 4.06429243088\n",
      "epoch 1, batch 219, train_loss 4.1229891777\n",
      "epoch 1, batch 220, train_loss 4.12505626678\n",
      "epoch 1, batch 221, train_loss 4.38200330734\n",
      "epoch 1, batch 222, train_loss 2.91301870346\n",
      "epoch 1, batch 223, train_loss 2.94302201271\n",
      "epoch 1, batch 224, train_loss 3.29918313026\n",
      "epoch 1, batch 225, train_loss 3.48147177696\n",
      "epoch 1, batch 226, train_loss 4.13272237778\n",
      "epoch 1, batch 227, train_loss 3.81764793396\n",
      "epoch 1, batch 228, train_loss 4.01987934113\n",
      "epoch 1, batch 229, train_loss 3.80859184265\n",
      "epoch 1, batch 230, train_loss 3.62954044342\n",
      "epoch 1, batch 231, train_loss 3.27410888672\n",
      "epoch 1, batch 232, train_loss 4.06665182114\n",
      "epoch 1, batch 233, train_loss 4.33785867691\n",
      "epoch 1, batch 234, train_loss 3.79260754585\n",
      "epoch 1, batch 235, train_loss 4.55439996719\n",
      "epoch 1, batch 236, train_loss 3.90991139412\n",
      "epoch 1, batch 237, train_loss 3.82069778442\n",
      "epoch 1, batch 238, train_loss 3.8500418663\n",
      "epoch 1, batch 239, train_loss 3.58900952339\n",
      "epoch 1, batch 240, train_loss 4.13457775116\n",
      "epoch 1, batch 241, train_loss 4.0730047226\n",
      "epoch 1, batch 242, train_loss 3.86980581284\n",
      "epoch 1, batch 243, train_loss 4.09014892578\n",
      "epoch 1, batch 244, train_loss 4.21728038788\n",
      "epoch 1, batch 245, train_loss 3.93517374992\n",
      "epoch 1, batch 246, train_loss 3.8572936058\n",
      "epoch 1, batch 247, train_loss 4.06367349625\n",
      "epoch 1, batch 248, train_loss 3.77414631844\n",
      "epoch 1, batch 249, train_loss 4.54442310333\n",
      "epoch 1, batch 250, train_loss 4.37689447403\n",
      "epoch 1, batch 251, train_loss 4.92996168137\n",
      "epoch 1, batch 252, train_loss 3.30143976212\n",
      "epoch 1, batch 253, train_loss 2.70674800873\n",
      "epoch 1, batch 254, train_loss 3.5632109642\n",
      "epoch 1, batch 255, train_loss 2.74019551277\n",
      "epoch 1, batch 256, train_loss 3.18723511696\n",
      "epoch 1, batch 257, train_loss 3.81911087036\n",
      "epoch 1, batch 258, train_loss 3.03331565857\n",
      "epoch 1, batch 259, train_loss 2.98805952072\n",
      "epoch 1, batch 260, train_loss 2.58315515518\n",
      "epoch 1, batch 261, train_loss 4.32598209381\n",
      "epoch 1, batch 262, train_loss 5.036277771\n",
      "epoch 1, batch 263, train_loss 3.80343151093\n",
      "epoch 1, batch 264, train_loss 3.47379660606\n",
      "epoch 1, batch 265, train_loss 4.33007049561\n",
      "epoch 1, batch 266, train_loss 4.39475345612\n",
      "epoch 1, batch 267, train_loss 3.43410372734\n",
      "epoch 1, batch 268, train_loss 2.89198541641\n",
      "epoch 1, batch 269, train_loss 3.01717805862\n",
      "epoch 1, batch 270, train_loss 3.38054561615\n",
      "epoch 1, batch 271, train_loss 3.81617617607\n",
      "epoch 1, batch 272, train_loss 3.79788994789\n",
      "epoch 1, batch 273, train_loss 3.95390439034\n",
      "epoch 1, batch 274, train_loss 3.62061858177\n",
      "epoch 1, batch 275, train_loss 3.74283003807\n",
      "epoch 1, batch 276, train_loss 3.4115922451\n",
      "epoch 1, batch 277, train_loss 4.43693208694\n",
      "epoch 1, batch 278, train_loss 3.47934865952\n",
      "epoch 1, batch 279, train_loss 4.09314060211\n",
      "epoch 1, batch 280, train_loss 4.10511636734\n",
      "epoch 1, batch 281, train_loss 3.83540081978\n",
      "epoch 1, batch 282, train_loss 3.36676931381\n",
      "epoch 1, batch 283, train_loss 5.37849473953\n",
      "epoch 1, batch 284, train_loss 4.04569292068\n",
      "epoch 1, batch 285, train_loss 2.50094604492\n",
      "epoch 1, batch 286, train_loss 2.7601313591\n",
      "epoch 1, batch 287, train_loss 2.89170598984\n",
      "epoch 1, batch 288, train_loss 3.63530993462\n",
      "epoch 1, batch 289, train_loss 3.75670099258\n",
      "epoch 1, batch 290, train_loss 3.63119673729\n",
      "epoch 1, batch 291, train_loss 3.42101287842\n",
      "epoch 1, batch 292, train_loss 2.81447815895\n",
      "epoch 1, batch 293, train_loss 4.14745616913\n",
      "epoch 1, batch 294, train_loss 3.58438396454\n",
      "epoch 1, batch 295, train_loss 3.23365569115\n",
      "epoch 1, batch 296, train_loss 3.56140470505\n",
      "epoch 1, batch 297, train_loss 3.23526334763\n",
      "epoch 1, batch 298, train_loss 3.62548708916\n",
      "epoch 1, batch 299, train_loss 4.35330581665\n",
      "epoch 1, batch 300, train_loss 3.95183086395\n",
      "epoch 1, batch 301, train_loss 5.21255254745\n",
      "epoch 1, batch 302, train_loss 5.1935968399\n",
      "epoch 1, batch 303, train_loss 4.82500982285\n",
      "epoch 1, batch 304, train_loss 5.69191217422\n",
      "epoch 1, batch 305, train_loss 5.62805461884\n",
      "epoch 1, batch 306, train_loss 3.97046804428\n",
      "epoch 1, batch 307, train_loss 3.11670446396\n",
      "epoch 1, batch 308, train_loss 3.62144875526\n",
      "epoch 1, batch 309, train_loss 3.79426431656\n",
      "epoch 1, batch 310, train_loss 3.55617785454\n",
      "epoch 1, batch 311, train_loss 3.72338628769\n",
      "epoch 1, batch 312, train_loss 3.89863824844\n",
      "epoch 1, batch 313, train_loss 3.50735640526\n",
      "epoch 1, batch 314, train_loss 3.99875330925\n",
      "epoch 1, batch 315, train_loss 3.35405039787\n",
      "epoch 1, batch 316, train_loss 3.50094485283\n",
      "epoch 1, batch 317, train_loss 4.1005449295\n",
      "epoch 1, batch 318, train_loss 3.0784034729\n",
      "epoch 1, batch 319, train_loss 5.29284334183\n",
      "epoch 1, batch 320, train_loss 3.94057154655\n",
      "epoch 1, batch 321, train_loss 5.50692939758\n",
      "epoch 1, batch 322, train_loss 3.02230882645\n",
      "epoch 1, batch 323, train_loss 2.8467605114\n",
      "epoch 1, batch 324, train_loss 4.20136594772\n",
      "epoch 1, batch 325, train_loss 3.72765612602\n",
      "epoch 1, batch 326, train_loss 3.78734970093\n",
      "epoch 1, batch 327, train_loss 3.7814438343\n",
      "epoch 1, batch 328, train_loss 3.60888624191\n",
      "epoch 1, batch 329, train_loss 3.14881968498\n",
      "epoch 1, batch 330, train_loss 4.75542736053\n",
      "epoch 1, batch 331, train_loss 4.30896759033\n",
      "epoch 1, batch 332, train_loss 2.96099448204\n",
      "epoch 1, batch 333, train_loss 4.32382392883\n",
      "epoch 1, batch 334, train_loss 3.93669652939\n",
      "epoch 1, batch 335, train_loss 3.82879304886\n",
      "epoch 1, batch 336, train_loss 4.54165649414\n",
      "epoch 1, batch 337, train_loss 2.77461981773\n",
      "epoch 1, batch 338, train_loss 3.6675901413\n",
      "epoch 1, batch 339, train_loss 3.47039556503\n",
      "epoch 1, batch 340, train_loss 3.65830206871\n",
      "epoch 1, batch 341, train_loss 3.54571032524\n",
      "epoch 1, batch 342, train_loss 3.63983225822\n",
      "epoch 1, batch 343, train_loss 3.56585931778\n",
      "epoch 1, batch 344, train_loss 4.7892947197\n",
      "epoch 1, batch 345, train_loss 4.21977567673\n",
      "epoch 1, batch 346, train_loss 3.83424305916\n",
      "epoch 1, batch 347, train_loss 3.53004288673\n",
      "epoch 1, batch 348, train_loss 3.74225091934\n",
      "epoch 1, batch 349, train_loss 5.33054876328\n",
      "epoch 1, batch 350, train_loss 5.19372463226\n",
      "epoch 1, batch 351, train_loss 4.73919391632\n",
      "epoch 1, batch 352, train_loss 4.85934448242\n",
      "epoch 1, batch 353, train_loss 3.95577764511\n",
      "epoch 1, batch 354, train_loss 3.18574380875\n",
      "epoch 1, batch 355, train_loss 3.54428768158\n",
      "epoch 1, batch 356, train_loss 3.61842346191\n",
      "epoch 1, batch 357, train_loss 3.52577710152\n",
      "epoch 1, batch 358, train_loss 2.94535255432\n",
      "epoch 1, batch 359, train_loss 4.14018964767\n",
      "epoch 1, batch 360, train_loss 3.54367923737\n",
      "epoch 1, batch 361, train_loss 3.10051035881\n",
      "epoch 1, batch 362, train_loss 5.14138841629\n",
      "epoch 1, batch 363, train_loss 3.76378273964\n",
      "epoch 1, batch 364, train_loss 3.63370251656\n",
      "epoch 1, batch 365, train_loss 4.62852525711\n",
      "epoch 1, batch 366, train_loss 4.88956642151\n",
      "epoch 1, batch 367, train_loss 5.72040700912\n",
      "epoch 1, batch 368, train_loss 2.95253682137\n",
      "epoch 1, batch 369, train_loss 4.04864263535\n",
      "epoch 1, batch 370, train_loss 4.50777435303\n",
      "epoch 1, batch 371, train_loss 5.69684553146\n",
      "epoch 1, batch 372, train_loss 2.76757860184\n",
      "epoch 1, batch 373, train_loss 2.9737610817\n",
      "epoch 1, batch 374, train_loss 3.30034685135\n",
      "epoch 1, batch 375, train_loss 3.81391620636\n",
      "epoch 1, batch 376, train_loss 2.92627692223\n",
      "epoch 1, batch 377, train_loss 3.44724011421\n",
      "epoch 1, batch 378, train_loss 4.26755809784\n",
      "epoch 1, batch 379, train_loss 3.31540942192\n",
      "epoch 1, batch 380, train_loss 3.41090893745\n",
      "epoch 1, batch 381, train_loss 3.54914808273\n",
      "epoch 1, batch 382, train_loss 4.63589382172\n",
      "epoch 1, batch 383, train_loss 3.99954771996\n",
      "epoch 1, batch 384, train_loss 3.62358880043\n",
      "epoch 1, batch 385, train_loss 5.46208906174\n",
      "epoch 1, batch 386, train_loss 3.31381750107\n",
      "epoch 1, batch 387, train_loss 5.67273712158\n",
      "epoch 1, batch 388, train_loss 3.30672764778\n",
      "epoch 1, batch 389, train_loss 5.2106127739\n",
      "epoch 1, batch 390, train_loss 5.18789958954\n",
      "epoch 1, batch 391, train_loss 5.19880771637\n",
      "epoch 1, batch 392, train_loss 4.82191944122\n",
      "epoch 1, batch 393, train_loss 4.01957702637\n",
      "epoch 1, batch 394, train_loss 4.84840250015\n",
      "epoch 1, batch 395, train_loss 3.56458997726\n",
      "epoch 1, batch 396, train_loss 5.07773399353\n",
      "epoch 1, batch 397, train_loss 5.28926229477\n",
      "epoch 1, batch 398, train_loss 3.86989045143\n",
      "epoch 1, batch 399, train_loss 4.17762327194\n",
      "epoch 1, batch 400, train_loss 5.39581537247\n",
      "epoch 1, batch 401, train_loss 4.52422142029\n",
      "epoch 1, batch 402, train_loss 3.89346170425\n",
      "epoch 1, batch 403, train_loss 3.89245700836\n",
      "epoch 1, batch 404, train_loss 4.82269287109\n",
      "epoch 1, batch 405, train_loss 4.19741344452\n",
      "epoch 1, batch 406, train_loss 3.99413728714\n",
      "epoch 1, batch 407, train_loss 3.81516194344\n",
      "epoch 1, batch 408, train_loss 3.6269094944\n",
      "epoch 1, batch 409, train_loss 3.01429653168\n",
      "epoch 1, batch 410, train_loss 4.96845054626\n",
      "epoch 1, batch 411, train_loss 3.82477641106\n",
      "epoch 1, batch 412, train_loss 3.37867355347\n",
      "epoch 1, batch 413, train_loss 3.84799146652\n",
      "epoch 1, batch 414, train_loss 4.27728700638\n",
      "epoch 1, batch 415, train_loss 4.40049409866\n",
      "epoch 1, batch 416, train_loss 4.14712095261\n",
      "epoch 1, batch 417, train_loss 3.5501229763\n",
      "epoch 1, batch 418, train_loss 4.04329299927\n",
      "epoch 1, batch 419, train_loss 3.59503102303\n",
      "epoch 1, batch 420, train_loss 2.89992976189\n",
      "epoch 1, batch 421, train_loss 5.14031982422\n",
      "epoch 1, batch 422, train_loss 3.62416934967\n",
      "epoch 1, batch 423, train_loss 3.69475626945\n",
      "epoch 1, batch 424, train_loss 5.02240180969\n",
      "epoch 1, batch 425, train_loss 4.1759352684\n",
      "epoch 1, batch 426, train_loss 5.5438580513\n",
      "epoch 1, batch 427, train_loss 4.53470849991\n",
      "epoch 1, batch 428, train_loss 3.12366247177\n",
      "epoch 1, batch 429, train_loss 4.4391708374\n",
      "epoch 1, batch 430, train_loss 4.4731798172\n",
      "epoch 1, batch 431, train_loss 4.03938674927\n",
      "epoch 1, batch 432, train_loss 4.24440717697\n",
      "epoch 1, batch 433, train_loss 3.51870059967\n",
      "epoch 1, batch 434, train_loss 4.00087308884\n",
      "epoch 1, batch 435, train_loss 3.78361296654\n",
      "epoch 1, batch 436, train_loss 4.18653392792\n",
      "epoch 1, batch 437, train_loss 4.75445795059\n",
      "epoch 1, batch 438, train_loss 4.27951860428\n",
      "epoch 1, batch 439, train_loss 4.00508546829\n",
      "epoch 1, batch 440, train_loss 5.59955120087\n",
      "epoch 1, batch 441, train_loss 5.75861883163\n",
      "epoch 1, batch 442, train_loss 4.69975662231\n",
      "epoch 1, batch 443, train_loss 2.92173552513\n",
      "epoch 1, batch 444, train_loss 3.47433280945\n",
      "epoch 1, batch 445, train_loss 5.27875137329\n",
      "epoch 1, batch 446, train_loss 3.89609050751\n",
      "epoch 1, batch 447, train_loss 3.56706666946\n",
      "epoch 1, batch 448, train_loss 5.14972496033\n",
      "epoch 1, batch 449, train_loss 4.3580904007\n",
      "epoch 1, batch 450, train_loss 3.71599268913\n",
      "epoch 1, batch 451, train_loss 3.5831990242\n",
      "epoch 1, batch 452, train_loss 3.1601626873\n",
      "epoch 1, batch 453, train_loss 5.91709280014\n",
      "epoch 1, batch 454, train_loss 5.93056821823\n",
      "epoch 1, batch 455, train_loss 2.97818040848\n",
      "epoch 1, batch 456, train_loss 3.83528590202\n",
      "epoch 1, batch 457, train_loss 4.19644498825\n",
      "epoch 1, batch 458, train_loss 3.65357303619\n",
      "epoch 1, batch 459, train_loss 4.06887674332\n",
      "epoch 1, batch 460, train_loss 4.73245668411\n",
      "epoch 1, batch 461, train_loss 3.9813182354\n",
      "epoch 1, batch 462, train_loss 3.70208072662\n",
      "epoch 1, batch 463, train_loss 3.90080547333\n",
      "epoch 1, batch 464, train_loss 4.20526790619\n",
      "epoch 1, batch 465, train_loss 3.48622918129\n",
      "epoch 1, batch 466, train_loss 4.38276100159\n",
      "epoch 1, batch 467, train_loss 4.15752840042\n",
      "epoch 1, batch 468, train_loss 4.33437728882\n",
      "epoch 1, batch 469, train_loss 3.78698396683\n",
      "epoch 1, batch 470, train_loss 3.66745495796\n",
      "epoch 1, batch 471, train_loss 3.71812796593\n",
      "epoch 1, batch 472, train_loss 3.54693603516\n",
      "epoch 1, batch 473, train_loss 4.40161275864\n",
      "epoch 1, batch 474, train_loss 5.10950469971\n",
      "epoch 1, batch 475, train_loss 3.51155138016\n",
      "epoch 1, batch 476, train_loss 2.99554657936\n",
      "epoch 1, batch 477, train_loss 3.12463665009\n",
      "epoch 1, batch 478, train_loss 2.95943832397\n",
      "epoch 1, batch 479, train_loss 3.5622484684\n",
      "epoch 1, batch 480, train_loss 3.76749181747\n",
      "epoch 1, batch 481, train_loss 5.59192323685\n",
      "epoch 1, batch 482, train_loss 3.42126083374\n",
      "epoch 1, batch 483, train_loss 4.22029876709\n",
      "epoch 1, batch 484, train_loss 3.75308966637\n",
      "epoch 1, batch 485, train_loss 2.88133621216\n",
      "epoch 1, batch 486, train_loss 2.88179159164\n",
      "epoch 1, batch 487, train_loss 3.01007676125\n",
      "epoch 1, batch 488, train_loss 3.49351453781\n",
      "epoch 1, batch 489, train_loss 3.59572601318\n",
      "epoch 1, batch 490, train_loss 3.58488559723\n",
      "epoch 1, batch 491, train_loss 3.46882176399\n",
      "epoch 1, batch 492, train_loss 3.74871611595\n",
      "epoch 1, batch 493, train_loss 3.29291296005\n",
      "epoch 1, batch 494, train_loss 3.42172718048\n",
      "epoch 1, batch 495, train_loss 3.53147602081\n",
      "epoch 1, batch 496, train_loss 3.07888007164\n",
      "epoch 1, batch 497, train_loss 3.31573843956\n",
      "epoch 1, batch 498, train_loss 3.46650004387\n",
      "epoch 1, batch 499, train_loss 3.42481541634\n",
      "epoch 1, batch 500, train_loss 4.03072452545\n",
      "epoch 1, batch 501, train_loss 3.54186272621\n",
      "epoch 1, batch 502, train_loss 5.09125614166\n",
      "epoch 1, batch 503, train_loss 5.06158924103\n",
      "epoch 1, batch 504, train_loss 4.37680625916\n",
      "epoch 1, batch 505, train_loss 4.15734672546\n",
      "epoch 1, batch 506, train_loss 3.79092216492\n",
      "epoch 1, batch 507, train_loss 4.9078707695\n",
      "epoch 1, batch 508, train_loss 3.46333169937\n",
      "epoch 1, batch 509, train_loss 5.08041620255\n",
      "epoch 1, batch 510, train_loss 5.10354566574\n",
      "epoch 1, batch 511, train_loss 5.0862903595\n",
      "epoch 1, batch 512, train_loss 3.92293286324\n",
      "epoch 1, batch 513, train_loss 5.5478720665\n",
      "epoch 1, batch 514, train_loss 5.51556301117\n",
      "epoch 1, batch 515, train_loss 5.41993951797\n",
      "epoch 1, batch 516, train_loss 3.37790703773\n",
      "epoch 1, batch 517, train_loss 3.51925206184\n",
      "epoch 1, batch 518, train_loss 3.44370245934\n",
      "epoch 1, batch 519, train_loss 3.09378051758\n",
      "epoch 1, batch 520, train_loss 4.46009588242\n",
      "epoch 1, batch 521, train_loss 3.48042821884\n",
      "epoch 1, batch 522, train_loss 2.97973275185\n",
      "epoch 1, batch 523, train_loss 3.01373958588\n",
      "epoch 1, batch 524, train_loss 2.86023640633\n",
      "epoch 1, batch 525, train_loss 2.69700789452\n",
      "epoch 1, batch 526, train_loss 2.76485085487\n",
      "epoch 1, batch 527, train_loss 2.73170495033\n",
      "epoch 1, batch 528, train_loss 3.03196573257\n",
      "epoch 1, batch 529, train_loss 2.91876506805\n",
      "epoch 1, batch 530, train_loss 2.33408927917\n",
      "epoch 1, batch 531, train_loss 1.97714662552\n",
      "epoch 1, batch 532, train_loss 3.08158016205\n",
      "epoch 1, batch 533, train_loss 1.92758274078\n",
      "epoch 1, batch 534, train_loss 1.94557213783\n",
      "epoch 1, batch 535, train_loss 3.71517896652\n",
      "epoch 1, batch 536, train_loss 3.97167611122\n",
      "epoch 1, batch 537, train_loss 3.83400034904\n",
      "epoch 1, batch 538, train_loss 4.10599470139\n",
      "epoch 1, batch 539, train_loss 4.54627132416\n",
      "epoch 1, batch 540, train_loss 4.86734628677\n",
      "epoch 2, batch 0, train_loss 3.83026194572\n",
      "epoch 2, batch 1, train_loss 4.32919406891\n",
      "epoch 2, batch 2, train_loss 3.69775462151\n",
      "epoch 2, batch 3, train_loss 3.91736459732\n",
      "epoch 2, batch 4, train_loss 3.84665465355\n",
      "epoch 2, batch 5, train_loss 3.92735433578\n",
      "epoch 2, batch 6, train_loss 5.285364151\n",
      "epoch 2, batch 7, train_loss 3.87811517715\n",
      "epoch 2, batch 8, train_loss 3.41753649712\n",
      "epoch 2, batch 9, train_loss 4.66616868973\n",
      "epoch 2, batch 10, train_loss 3.61313915253\n",
      "epoch 2, batch 11, train_loss 3.74324846268\n",
      "epoch 2, batch 12, train_loss 3.37239193916\n",
      "epoch 2, batch 13, train_loss 3.97628211975\n",
      "epoch 2, batch 14, train_loss 3.24170136452\n",
      "epoch 2, batch 15, train_loss 4.07879209518\n",
      "epoch 2, batch 16, train_loss 3.55831575394\n",
      "epoch 2, batch 17, train_loss 3.4301776886\n",
      "epoch 2, batch 18, train_loss 2.99125766754\n",
      "epoch 2, batch 19, train_loss 2.46789216995\n",
      "epoch 2, batch 20, train_loss 3.47777938843\n",
      "epoch 2, batch 21, train_loss 3.39058089256\n",
      "epoch 2, batch 22, train_loss 3.93014645576\n",
      "epoch 2, batch 23, train_loss 3.36378288269\n",
      "epoch 2, batch 24, train_loss 3.34352755547\n",
      "epoch 2, batch 25, train_loss 3.8933403492\n",
      "epoch 2, batch 26, train_loss 3.77381849289\n",
      "epoch 2, batch 27, train_loss 3.70769262314\n",
      "epoch 2, batch 28, train_loss 4.08060979843\n",
      "epoch 2, batch 29, train_loss 4.03211307526\n",
      "epoch 2, batch 30, train_loss 4.07533311844\n",
      "epoch 2, batch 31, train_loss 3.4723212719\n",
      "epoch 2, batch 32, train_loss 3.13243174553\n",
      "epoch 2, batch 33, train_loss 5.33851480484\n",
      "epoch 2, batch 34, train_loss 5.42991828918\n",
      "epoch 2, batch 35, train_loss 3.81576299667\n",
      "epoch 2, batch 36, train_loss 3.8145942688\n",
      "epoch 2, batch 37, train_loss 3.84819555283\n",
      "epoch 2, batch 38, train_loss 3.8596060276\n",
      "epoch 2, batch 39, train_loss 4.40288925171\n",
      "epoch 2, batch 40, train_loss 3.43028450012\n",
      "epoch 2, batch 41, train_loss 3.67370676994\n",
      "epoch 2, batch 42, train_loss 3.80063438416\n",
      "epoch 2, batch 43, train_loss 3.5607688427\n",
      "epoch 2, batch 44, train_loss 4.15878391266\n",
      "epoch 2, batch 45, train_loss 3.76967191696\n",
      "epoch 2, batch 46, train_loss 3.68506264687\n",
      "epoch 2, batch 47, train_loss 3.87039852142\n",
      "epoch 2, batch 48, train_loss 3.70425081253\n",
      "epoch 2, batch 49, train_loss 3.8620736599\n",
      "epoch 2, batch 50, train_loss 3.68037486076\n",
      "epoch 2, batch 51, train_loss 3.34380316734\n",
      "epoch 2, batch 52, train_loss 3.84977507591\n",
      "epoch 2, batch 53, train_loss 3.77526688576\n",
      "epoch 2, batch 54, train_loss 3.41985249519\n",
      "epoch 2, batch 55, train_loss 3.45215201378\n",
      "epoch 2, batch 56, train_loss 3.34925174713\n",
      "epoch 2, batch 57, train_loss 3.92981791496\n",
      "epoch 2, batch 58, train_loss 3.72380948067\n",
      "epoch 2, batch 59, train_loss 3.66332793236\n",
      "epoch 2, batch 60, train_loss 3.87655067444\n",
      "epoch 2, batch 61, train_loss 2.3045463562\n",
      "epoch 2, batch 62, train_loss 3.40257453918\n",
      "epoch 2, batch 63, train_loss 3.58811831474\n",
      "epoch 2, batch 64, train_loss 3.77118444443\n",
      "epoch 2, batch 65, train_loss 4.15102577209\n",
      "epoch 2, batch 66, train_loss 3.90614533424\n",
      "epoch 2, batch 67, train_loss 2.54692745209\n",
      "epoch 2, batch 68, train_loss 3.60048747063\n",
      "epoch 2, batch 69, train_loss 2.86277389526\n",
      "epoch 2, batch 70, train_loss 4.83927440643\n",
      "epoch 2, batch 71, train_loss 4.85028362274\n",
      "epoch 2, batch 72, train_loss 3.40477108955\n",
      "epoch 2, batch 73, train_loss 3.89662742615\n",
      "epoch 2, batch 74, train_loss 4.66409349442\n",
      "epoch 2, batch 75, train_loss 3.94009637833\n",
      "epoch 2, batch 76, train_loss 4.17515325546\n",
      "epoch 2, batch 77, train_loss 5.03368186951\n",
      "epoch 2, batch 78, train_loss 3.64561867714\n",
      "epoch 2, batch 79, train_loss 3.69230794907\n",
      "epoch 2, batch 80, train_loss 3.47698068619\n",
      "epoch 2, batch 81, train_loss 3.61478757858\n",
      "epoch 2, batch 82, train_loss 3.4815955162\n",
      "epoch 2, batch 83, train_loss 3.62239146233\n",
      "epoch 2, batch 84, train_loss 3.40856814384\n",
      "epoch 2, batch 85, train_loss 3.42236852646\n",
      "epoch 2, batch 86, train_loss 3.94075083733\n",
      "epoch 2, batch 87, train_loss 3.06196689606\n",
      "epoch 2, batch 88, train_loss 3.38061237335\n",
      "epoch 2, batch 89, train_loss 3.71576428413\n",
      "epoch 2, batch 90, train_loss 3.34528565407\n",
      "epoch 2, batch 91, train_loss 3.50490045547\n",
      "epoch 2, batch 92, train_loss 4.06634378433\n",
      "epoch 2, batch 93, train_loss 4.93376111984\n",
      "epoch 2, batch 94, train_loss 3.49266839027\n",
      "epoch 2, batch 95, train_loss 3.6081366539\n",
      "epoch 2, batch 96, train_loss 2.86268973351\n",
      "epoch 2, batch 97, train_loss 3.58229637146\n",
      "epoch 2, batch 98, train_loss 3.50821208954\n",
      "epoch 2, batch 99, train_loss 3.86271595955\n",
      "epoch 2, batch 100, train_loss 3.54045391083\n",
      "epoch 2, batch 101, train_loss 3.76194572449\n",
      "epoch 2, batch 102, train_loss 3.53715586662\n",
      "epoch 2, batch 103, train_loss 4.82008266449\n",
      "epoch 2, batch 104, train_loss 4.03198623657\n",
      "epoch 2, batch 105, train_loss 3.7953441143\n",
      "epoch 2, batch 106, train_loss 4.2511715889\n",
      "epoch 2, batch 107, train_loss 4.05511808395\n",
      "epoch 2, batch 108, train_loss 3.65608000755\n",
      "epoch 2, batch 109, train_loss 3.8633658886\n",
      "epoch 2, batch 110, train_loss 3.68765211105\n",
      "epoch 2, batch 111, train_loss 4.11417627335\n",
      "epoch 2, batch 112, train_loss 4.08205509186\n",
      "epoch 2, batch 113, train_loss 3.48418283463\n",
      "epoch 2, batch 114, train_loss 4.12846040726\n",
      "epoch 2, batch 115, train_loss 5.03944969177\n",
      "epoch 2, batch 116, train_loss 4.92609882355\n",
      "epoch 2, batch 117, train_loss 4.98247241974\n",
      "epoch 2, batch 118, train_loss 4.37845611572\n",
      "epoch 2, batch 119, train_loss 2.9290895462\n",
      "epoch 2, batch 120, train_loss 3.80387759209\n",
      "epoch 2, batch 121, train_loss 3.66767334938\n",
      "epoch 2, batch 122, train_loss 3.92229652405\n",
      "epoch 2, batch 123, train_loss 3.60876965523\n",
      "epoch 2, batch 124, train_loss 3.36203026772\n",
      "epoch 2, batch 125, train_loss 3.2533531189\n",
      "epoch 2, batch 126, train_loss 3.23736095428\n",
      "epoch 2, batch 127, train_loss 3.35755491257\n",
      "epoch 2, batch 128, train_loss 3.23119592667\n",
      "epoch 2, batch 129, train_loss 3.58562660217\n",
      "epoch 2, batch 130, train_loss 3.50072097778\n",
      "epoch 2, batch 131, train_loss 3.87479019165\n",
      "epoch 2, batch 132, train_loss 3.58570146561\n",
      "epoch 2, batch 133, train_loss 2.69110131264\n",
      "epoch 2, batch 134, train_loss 2.80739784241\n",
      "epoch 2, batch 135, train_loss 4.89434242249\n",
      "epoch 2, batch 136, train_loss 4.1192650795\n",
      "epoch 2, batch 137, train_loss 3.23429059982\n",
      "epoch 2, batch 138, train_loss 3.06001853943\n",
      "epoch 2, batch 139, train_loss 3.59266543388\n",
      "epoch 2, batch 140, train_loss 3.04255509377\n",
      "epoch 2, batch 141, train_loss 3.45570778847\n",
      "epoch 2, batch 142, train_loss 4.91345119476\n",
      "epoch 2, batch 143, train_loss 3.08660507202\n",
      "epoch 2, batch 144, train_loss 2.49725174904\n",
      "epoch 2, batch 145, train_loss 3.62732338905\n",
      "epoch 2, batch 146, train_loss 3.82424092293\n",
      "epoch 2, batch 147, train_loss 3.52154779434\n",
      "epoch 2, batch 148, train_loss 3.38758325577\n",
      "epoch 2, batch 149, train_loss 3.92179179192\n",
      "epoch 2, batch 150, train_loss 3.99541378021\n",
      "epoch 2, batch 151, train_loss 2.46533298492\n",
      "epoch 2, batch 152, train_loss 2.78634667397\n",
      "epoch 2, batch 153, train_loss 4.80157470703\n",
      "epoch 2, batch 154, train_loss 3.75172042847\n",
      "epoch 2, batch 155, train_loss 3.26062679291\n",
      "epoch 2, batch 156, train_loss 3.47882866859\n",
      "epoch 2, batch 157, train_loss 2.96184277534\n",
      "epoch 2, batch 158, train_loss 3.04159903526\n",
      "epoch 2, batch 159, train_loss 3.25266098976\n",
      "epoch 2, batch 160, train_loss 3.39832520485\n",
      "epoch 2, batch 161, train_loss 3.98703670502\n",
      "epoch 2, batch 162, train_loss 3.83475065231\n",
      "epoch 2, batch 163, train_loss 5.56454181671\n",
      "epoch 2, batch 164, train_loss 2.69785094261\n",
      "epoch 2, batch 165, train_loss 2.64498782158\n",
      "epoch 2, batch 166, train_loss 3.11040878296\n",
      "epoch 2, batch 167, train_loss 3.12428307533\n",
      "epoch 2, batch 168, train_loss 3.38532853127\n",
      "epoch 2, batch 169, train_loss 3.09182667732\n",
      "epoch 2, batch 170, train_loss 3.74681472778\n",
      "epoch 2, batch 171, train_loss 3.09800934792\n",
      "epoch 2, batch 172, train_loss 3.80736804008\n",
      "epoch 2, batch 173, train_loss 2.53961253166\n",
      "epoch 2, batch 174, train_loss 4.30926609039\n",
      "epoch 2, batch 175, train_loss 3.6947350502\n",
      "epoch 2, batch 176, train_loss 3.55765199661\n",
      "epoch 2, batch 177, train_loss 3.33550715446\n",
      "epoch 2, batch 178, train_loss 3.26150107384\n",
      "epoch 2, batch 179, train_loss 3.51784539223\n",
      "epoch 2, batch 180, train_loss 3.39808440208\n",
      "epoch 2, batch 181, train_loss 4.17397260666\n",
      "epoch 2, batch 182, train_loss 3.53884768486\n",
      "epoch 2, batch 183, train_loss 4.22881269455\n",
      "epoch 2, batch 184, train_loss 2.73116755486\n",
      "epoch 2, batch 185, train_loss 3.13660287857\n",
      "epoch 2, batch 186, train_loss 3.95508313179\n",
      "epoch 2, batch 187, train_loss 2.95189237595\n",
      "epoch 2, batch 188, train_loss 3.23914980888\n",
      "epoch 2, batch 189, train_loss 3.24289155006\n",
      "epoch 2, batch 190, train_loss 4.11291503906\n",
      "epoch 2, batch 191, train_loss 3.96694684029\n",
      "epoch 2, batch 192, train_loss 5.09561824799\n",
      "epoch 2, batch 193, train_loss 4.75558614731\n",
      "epoch 2, batch 194, train_loss 5.47383737564\n",
      "epoch 2, batch 195, train_loss 5.53000974655\n",
      "epoch 2, batch 196, train_loss 3.70630216599\n",
      "epoch 2, batch 197, train_loss 2.98960900307\n",
      "epoch 2, batch 198, train_loss 2.72614979744\n",
      "epoch 2, batch 199, train_loss 3.89212083817\n",
      "epoch 2, batch 200, train_loss 3.23557591438\n",
      "epoch 2, batch 201, train_loss 3.62448358536\n",
      "epoch 2, batch 202, train_loss 4.09753274918\n",
      "epoch 2, batch 203, train_loss 4.22733640671\n",
      "epoch 2, batch 204, train_loss 4.1757440567\n",
      "epoch 2, batch 205, train_loss 3.7167224884\n",
      "epoch 2, batch 206, train_loss 4.89636278152\n",
      "epoch 2, batch 207, train_loss 4.05971002579\n",
      "epoch 2, batch 208, train_loss 4.88066148758\n",
      "epoch 2, batch 209, train_loss 4.6633477211\n",
      "epoch 2, batch 210, train_loss 2.51922440529\n",
      "epoch 2, batch 211, train_loss 3.36165595055\n",
      "epoch 2, batch 212, train_loss 4.36952400208\n",
      "epoch 2, batch 213, train_loss 3.62332606316\n",
      "epoch 2, batch 214, train_loss 3.47051882744\n",
      "epoch 2, batch 215, train_loss 2.60700726509\n",
      "epoch 2, batch 216, train_loss 3.95221590996\n",
      "epoch 2, batch 217, train_loss 3.26895475388\n",
      "epoch 2, batch 218, train_loss 3.83461284637\n",
      "epoch 2, batch 219, train_loss 3.93921732903\n",
      "epoch 2, batch 220, train_loss 3.96797561646\n",
      "epoch 2, batch 221, train_loss 4.19976949692\n",
      "epoch 2, batch 222, train_loss 2.72010087967\n",
      "epoch 2, batch 223, train_loss 2.74497532845\n",
      "epoch 2, batch 224, train_loss 3.10508966446\n",
      "epoch 2, batch 225, train_loss 3.25791835785\n",
      "epoch 2, batch 226, train_loss 3.87736272812\n",
      "epoch 2, batch 227, train_loss 3.56991696358\n",
      "epoch 2, batch 228, train_loss 3.77077102661\n",
      "epoch 2, batch 229, train_loss 3.57683205605\n",
      "epoch 2, batch 230, train_loss 3.41299104691\n",
      "epoch 2, batch 231, train_loss 3.07910847664\n",
      "epoch 2, batch 232, train_loss 3.80580353737\n",
      "epoch 2, batch 233, train_loss 4.10215473175\n",
      "epoch 2, batch 234, train_loss 3.57216262817\n",
      "epoch 2, batch 235, train_loss 4.26965999603\n",
      "epoch 2, batch 236, train_loss 3.66907858849\n",
      "epoch 2, batch 237, train_loss 3.59436774254\n",
      "epoch 2, batch 238, train_loss 3.59947323799\n",
      "epoch 2, batch 239, train_loss 3.370241642\n",
      "epoch 2, batch 240, train_loss 3.88268971443\n",
      "epoch 2, batch 241, train_loss 3.84860038757\n",
      "epoch 2, batch 242, train_loss 3.64138555527\n",
      "epoch 2, batch 243, train_loss 3.84507703781\n",
      "epoch 2, batch 244, train_loss 3.95974040031\n",
      "epoch 2, batch 245, train_loss 3.69692993164\n",
      "epoch 2, batch 246, train_loss 3.62226080894\n",
      "epoch 2, batch 247, train_loss 3.82174611092\n",
      "epoch 2, batch 248, train_loss 3.54538464546\n",
      "epoch 2, batch 249, train_loss 4.33532762527\n",
      "epoch 2, batch 250, train_loss 4.12775325775\n",
      "epoch 2, batch 251, train_loss 4.58067464828\n",
      "epoch 2, batch 252, train_loss 3.10163712502\n",
      "epoch 2, batch 253, train_loss 2.5647752285\n",
      "epoch 2, batch 254, train_loss 3.3650739193\n",
      "epoch 2, batch 255, train_loss 2.5791695118\n",
      "epoch 2, batch 256, train_loss 3.00217747688\n",
      "epoch 2, batch 257, train_loss 3.56885766983\n",
      "epoch 2, batch 258, train_loss 2.84599232674\n",
      "epoch 2, batch 259, train_loss 2.77602553368\n",
      "epoch 2, batch 260, train_loss 2.43011522293\n",
      "epoch 2, batch 261, train_loss 4.05574512482\n",
      "epoch 2, batch 262, train_loss 4.7344789505\n",
      "epoch 2, batch 263, train_loss 3.62950396538\n",
      "epoch 2, batch 264, train_loss 3.28522443771\n",
      "epoch 2, batch 265, train_loss 4.06259298325\n",
      "epoch 2, batch 266, train_loss 4.14168071747\n",
      "epoch 2, batch 267, train_loss 3.25614356995\n",
      "epoch 2, batch 268, train_loss 2.76116204262\n",
      "epoch 2, batch 269, train_loss 2.82074689865\n",
      "epoch 2, batch 270, train_loss 3.166472435\n",
      "epoch 2, batch 271, train_loss 3.57638883591\n",
      "epoch 2, batch 272, train_loss 3.57213950157\n",
      "epoch 2, batch 273, train_loss 3.72905254364\n",
      "epoch 2, batch 274, train_loss 3.44328522682\n",
      "epoch 2, batch 275, train_loss 3.50746273994\n",
      "epoch 2, batch 276, train_loss 3.20712757111\n",
      "epoch 2, batch 277, train_loss 4.17826938629\n",
      "epoch 2, batch 278, train_loss 3.26089024544\n",
      "epoch 2, batch 279, train_loss 3.84479117393\n",
      "epoch 2, batch 280, train_loss 3.85481476784\n",
      "epoch 2, batch 281, train_loss 3.60735607147\n",
      "epoch 2, batch 282, train_loss 3.18535494804\n",
      "epoch 2, batch 283, train_loss 5.05897331238\n",
      "epoch 2, batch 284, train_loss 3.80065464973\n",
      "epoch 2, batch 285, train_loss 2.34255552292\n",
      "epoch 2, batch 286, train_loss 2.59740829468\n",
      "epoch 2, batch 287, train_loss 2.71733117104\n",
      "epoch 2, batch 288, train_loss 3.417324543\n",
      "epoch 2, batch 289, train_loss 3.51038002968\n",
      "epoch 2, batch 290, train_loss 3.40496325493\n",
      "epoch 2, batch 291, train_loss 3.20194482803\n",
      "epoch 2, batch 292, train_loss 2.60936737061\n",
      "epoch 2, batch 293, train_loss 3.92314600945\n",
      "epoch 2, batch 294, train_loss 3.3798353672\n",
      "epoch 2, batch 295, train_loss 3.05068969727\n",
      "epoch 2, batch 296, train_loss 3.36884498596\n",
      "epoch 2, batch 297, train_loss 3.01664018631\n",
      "epoch 2, batch 298, train_loss 3.38264727592\n",
      "epoch 2, batch 299, train_loss 4.0727481842\n",
      "epoch 2, batch 300, train_loss 3.70339822769\n",
      "epoch 2, batch 301, train_loss 4.89444494247\n",
      "epoch 2, batch 302, train_loss 4.89594364166\n",
      "epoch 2, batch 303, train_loss 4.48848724365\n",
      "epoch 2, batch 304, train_loss 5.28894138336\n",
      "epoch 2, batch 305, train_loss 5.23500919342\n",
      "epoch 2, batch 306, train_loss 3.68216466904\n",
      "epoch 2, batch 307, train_loss 2.91728401184\n",
      "epoch 2, batch 308, train_loss 3.40840244293\n",
      "epoch 2, batch 309, train_loss 3.57567071915\n",
      "epoch 2, batch 310, train_loss 3.34319734573\n",
      "epoch 2, batch 311, train_loss 3.49778413773\n",
      "epoch 2, batch 312, train_loss 3.68680214882\n",
      "epoch 2, batch 313, train_loss 3.3431084156\n",
      "epoch 2, batch 314, train_loss 3.78618550301\n",
      "epoch 2, batch 315, train_loss 3.17913007736\n",
      "epoch 2, batch 316, train_loss 3.33348226547\n",
      "epoch 2, batch 317, train_loss 3.89158439636\n",
      "epoch 2, batch 318, train_loss 2.90464520454\n",
      "epoch 2, batch 319, train_loss 4.96562957764\n",
      "epoch 2, batch 320, train_loss 3.69776797295\n",
      "epoch 2, batch 321, train_loss 5.13036441803\n",
      "epoch 2, batch 322, train_loss 2.82540512085\n",
      "epoch 2, batch 323, train_loss 2.6746366024\n",
      "epoch 2, batch 324, train_loss 3.97941708565\n",
      "epoch 2, batch 325, train_loss 3.52560424805\n",
      "epoch 2, batch 326, train_loss 3.56236791611\n",
      "epoch 2, batch 327, train_loss 3.5663626194\n",
      "epoch 2, batch 328, train_loss 3.40336108208\n",
      "epoch 2, batch 329, train_loss 2.98249030113\n",
      "epoch 2, batch 330, train_loss 4.48409414291\n",
      "epoch 2, batch 331, train_loss 4.07721757889\n",
      "epoch 2, batch 332, train_loss 2.78767037392\n",
      "epoch 2, batch 333, train_loss 4.06967735291\n",
      "epoch 2, batch 334, train_loss 3.72038650513\n",
      "epoch 2, batch 335, train_loss 3.63843154907\n",
      "epoch 2, batch 336, train_loss 4.28512239456\n",
      "epoch 2, batch 337, train_loss 2.63376092911\n",
      "epoch 2, batch 338, train_loss 3.55181837082\n",
      "epoch 2, batch 339, train_loss 3.29578447342\n",
      "epoch 2, batch 340, train_loss 3.50157856941\n",
      "epoch 2, batch 341, train_loss 3.39160251617\n",
      "epoch 2, batch 342, train_loss 3.4660012722\n",
      "epoch 2, batch 343, train_loss 3.38342547417\n",
      "epoch 2, batch 344, train_loss 4.55012369156\n",
      "epoch 2, batch 345, train_loss 3.94498872757\n",
      "epoch 2, batch 346, train_loss 3.62614893913\n",
      "epoch 2, batch 347, train_loss 3.33802223206\n",
      "epoch 2, batch 348, train_loss 3.58242344856\n",
      "epoch 2, batch 349, train_loss 4.96866130829\n",
      "epoch 2, batch 350, train_loss 4.87949848175\n",
      "epoch 2, batch 351, train_loss 4.49807167053\n",
      "epoch 2, batch 352, train_loss 4.59325933456\n",
      "epoch 2, batch 353, train_loss 3.74159240723\n",
      "epoch 2, batch 354, train_loss 3.03262495995\n",
      "epoch 2, batch 355, train_loss 3.38048148155\n",
      "epoch 2, batch 356, train_loss 3.41417098045\n",
      "epoch 2, batch 357, train_loss 3.34216880798\n",
      "epoch 2, batch 358, train_loss 2.77520871162\n",
      "epoch 2, batch 359, train_loss 3.93139004707\n",
      "epoch 2, batch 360, train_loss 3.35374903679\n",
      "epoch 2, batch 361, train_loss 2.9396314621\n",
      "epoch 2, batch 362, train_loss 4.89663124084\n",
      "epoch 2, batch 363, train_loss 3.58428287506\n",
      "epoch 2, batch 364, train_loss 3.47407913208\n",
      "epoch 2, batch 365, train_loss 4.4445734024\n",
      "epoch 2, batch 366, train_loss 4.64965343475\n",
      "epoch 2, batch 367, train_loss 5.45164203644\n",
      "epoch 2, batch 368, train_loss 2.82770299911\n",
      "epoch 2, batch 369, train_loss 3.87797021866\n",
      "epoch 2, batch 370, train_loss 4.29745340347\n",
      "epoch 2, batch 371, train_loss 5.39895677567\n",
      "epoch 2, batch 372, train_loss 2.64157629013\n",
      "epoch 2, batch 373, train_loss 2.8324842453\n",
      "epoch 2, batch 374, train_loss 3.17409396172\n",
      "epoch 2, batch 375, train_loss 3.61752653122\n",
      "epoch 2, batch 376, train_loss 2.78575253487\n",
      "epoch 2, batch 377, train_loss 3.2648859024\n",
      "epoch 2, batch 378, train_loss 4.06222105026\n",
      "epoch 2, batch 379, train_loss 3.15679168701\n",
      "epoch 2, batch 380, train_loss 3.20294165611\n",
      "epoch 2, batch 381, train_loss 3.32152104378\n",
      "epoch 2, batch 382, train_loss 4.37637233734\n",
      "epoch 2, batch 383, train_loss 3.81331682205\n",
      "epoch 2, batch 384, train_loss 3.43678021431\n",
      "epoch 2, batch 385, train_loss 5.02981328964\n",
      "epoch 2, batch 386, train_loss 3.13124966621\n",
      "epoch 2, batch 387, train_loss 5.33258485794\n",
      "epoch 2, batch 388, train_loss 3.10213899612\n",
      "epoch 2, batch 389, train_loss 4.93574810028\n",
      "epoch 2, batch 390, train_loss 4.85335016251\n",
      "epoch 2, batch 391, train_loss 4.89274406433\n",
      "epoch 2, batch 392, train_loss 4.55945110321\n",
      "epoch 2, batch 393, train_loss 3.77575302124\n",
      "epoch 2, batch 394, train_loss 4.51691007614\n",
      "epoch 2, batch 395, train_loss 3.34305477142\n",
      "epoch 2, batch 396, train_loss 4.81787014008\n",
      "epoch 2, batch 397, train_loss 4.99228811264\n",
      "epoch 2, batch 398, train_loss 3.67324256897\n",
      "epoch 2, batch 399, train_loss 3.94556307793\n",
      "epoch 2, batch 400, train_loss 5.08120822906\n",
      "epoch 2, batch 401, train_loss 4.25226926804\n",
      "epoch 2, batch 402, train_loss 3.69772553444\n",
      "epoch 2, batch 403, train_loss 3.68823337555\n",
      "epoch 2, batch 404, train_loss 4.59694194794\n",
      "epoch 2, batch 405, train_loss 3.99237513542\n",
      "epoch 2, batch 406, train_loss 3.80258917809\n",
      "epoch 2, batch 407, train_loss 3.61475229263\n",
      "epoch 2, batch 408, train_loss 3.43364691734\n",
      "epoch 2, batch 409, train_loss 2.86918735504\n",
      "epoch 2, batch 410, train_loss 4.74220323563\n",
      "epoch 2, batch 411, train_loss 3.66202187538\n",
      "epoch 2, batch 412, train_loss 3.18955302238\n",
      "epoch 2, batch 413, train_loss 3.62889266014\n",
      "epoch 2, batch 414, train_loss 4.10265254974\n",
      "epoch 2, batch 415, train_loss 4.18837690353\n",
      "epoch 2, batch 416, train_loss 3.95722746849\n",
      "epoch 2, batch 417, train_loss 3.40364480019\n",
      "epoch 2, batch 418, train_loss 3.86131739616\n",
      "epoch 2, batch 419, train_loss 3.44500112534\n",
      "epoch 2, batch 420, train_loss 2.75965118408\n",
      "epoch 2, batch 421, train_loss 4.88258314133\n",
      "epoch 2, batch 422, train_loss 3.43555927277\n",
      "epoch 2, batch 423, train_loss 3.53692412376\n",
      "epoch 2, batch 424, train_loss 4.7944393158\n",
      "epoch 2, batch 425, train_loss 3.98143458366\n",
      "epoch 2, batch 426, train_loss 5.30704832077\n",
      "epoch 2, batch 427, train_loss 4.32058095932\n",
      "epoch 2, batch 428, train_loss 3.00107884407\n",
      "epoch 2, batch 429, train_loss 4.20121908188\n",
      "epoch 2, batch 430, train_loss 4.26463460922\n",
      "epoch 2, batch 431, train_loss 3.83208489418\n",
      "epoch 2, batch 432, train_loss 4.02850341797\n",
      "epoch 2, batch 433, train_loss 3.34085440636\n",
      "epoch 2, batch 434, train_loss 3.83460068703\n",
      "epoch 2, batch 435, train_loss 3.6177380085\n",
      "epoch 2, batch 436, train_loss 4.02898168564\n",
      "epoch 2, batch 437, train_loss 4.55869817734\n",
      "epoch 2, batch 438, train_loss 4.10690450668\n",
      "epoch 2, batch 439, train_loss 3.84743309021\n",
      "epoch 2, batch 440, train_loss 5.34700775146\n",
      "epoch 2, batch 441, train_loss 5.52367210388\n",
      "epoch 2, batch 442, train_loss 4.50487756729\n",
      "epoch 2, batch 443, train_loss 2.79706740379\n",
      "epoch 2, batch 444, train_loss 3.34237861633\n",
      "epoch 2, batch 445, train_loss 5.08972406387\n",
      "epoch 2, batch 446, train_loss 3.73621916771\n",
      "epoch 2, batch 447, train_loss 3.4111468792\n",
      "epoch 2, batch 448, train_loss 4.9635720253\n",
      "epoch 2, batch 449, train_loss 4.21266460419\n",
      "epoch 2, batch 450, train_loss 3.57521104813\n",
      "epoch 2, batch 451, train_loss 3.38859462738\n",
      "epoch 2, batch 452, train_loss 3.03024482727\n",
      "epoch 2, batch 453, train_loss 5.70107412338\n",
      "epoch 2, batch 454, train_loss 5.6897854805\n",
      "epoch 2, batch 455, train_loss 2.85793972015\n",
      "epoch 2, batch 456, train_loss 3.686429739\n",
      "epoch 2, batch 457, train_loss 4.02438831329\n",
      "epoch 2, batch 458, train_loss 3.50220298767\n",
      "epoch 2, batch 459, train_loss 3.89646029472\n",
      "epoch 2, batch 460, train_loss 4.52262449265\n",
      "epoch 2, batch 461, train_loss 3.85846734047\n",
      "epoch 2, batch 462, train_loss 3.53815150261\n",
      "epoch 2, batch 463, train_loss 3.71468710899\n",
      "epoch 2, batch 464, train_loss 4.03540182114\n",
      "epoch 2, batch 465, train_loss 3.33019781113\n",
      "epoch 2, batch 466, train_loss 4.19883298874\n",
      "epoch 2, batch 467, train_loss 3.97171068192\n",
      "epoch 2, batch 468, train_loss 4.13402318954\n",
      "epoch 2, batch 469, train_loss 3.62082481384\n",
      "epoch 2, batch 470, train_loss 3.49754738808\n",
      "epoch 2, batch 471, train_loss 3.56388163567\n",
      "epoch 2, batch 472, train_loss 3.40961813927\n",
      "epoch 2, batch 473, train_loss 4.22845172882\n",
      "epoch 2, batch 474, train_loss 4.89488554001\n",
      "epoch 2, batch 475, train_loss 3.36024951935\n",
      "epoch 2, batch 476, train_loss 2.86384963989\n",
      "epoch 2, batch 477, train_loss 3.00209927559\n",
      "epoch 2, batch 478, train_loss 2.83989214897\n",
      "epoch 2, batch 479, train_loss 3.41541934013\n",
      "epoch 2, batch 480, train_loss 3.61833453178\n",
      "epoch 2, batch 481, train_loss 5.39834594727\n",
      "epoch 2, batch 482, train_loss 3.28634738922\n",
      "epoch 2, batch 483, train_loss 4.03560161591\n",
      "epoch 2, batch 484, train_loss 3.61738872528\n",
      "epoch 2, batch 485, train_loss 2.76966524124\n",
      "epoch 2, batch 486, train_loss 2.76073384285\n",
      "epoch 2, batch 487, train_loss 2.89106726646\n",
      "epoch 2, batch 488, train_loss 3.35427761078\n",
      "epoch 2, batch 489, train_loss 3.4573636055\n",
      "epoch 2, batch 490, train_loss 3.42797613144\n",
      "epoch 2, batch 491, train_loss 3.3228096962\n",
      "epoch 2, batch 492, train_loss 3.6168358326\n",
      "epoch 2, batch 493, train_loss 3.16797709465\n",
      "epoch 2, batch 494, train_loss 3.30098009109\n",
      "epoch 2, batch 495, train_loss 3.3845076561\n",
      "epoch 2, batch 496, train_loss 2.9656689167\n",
      "epoch 2, batch 497, train_loss 3.17543649673\n",
      "epoch 2, batch 498, train_loss 3.33716392517\n",
      "epoch 2, batch 499, train_loss 3.29190206528\n",
      "epoch 2, batch 500, train_loss 3.87404322624\n",
      "epoch 2, batch 501, train_loss 3.40417075157\n",
      "epoch 2, batch 502, train_loss 4.89101696014\n",
      "epoch 2, batch 503, train_loss 4.85385894775\n",
      "epoch 2, batch 504, train_loss 4.20337820053\n",
      "epoch 2, batch 505, train_loss 3.98844003677\n",
      "epoch 2, batch 506, train_loss 3.63335084915\n",
      "epoch 2, batch 507, train_loss 4.7264251709\n",
      "epoch 2, batch 508, train_loss 3.34878897667\n",
      "epoch 2, batch 509, train_loss 4.89159679413\n",
      "epoch 2, batch 510, train_loss 4.90068864822\n",
      "epoch 2, batch 511, train_loss 4.89828443527\n",
      "epoch 2, batch 512, train_loss 3.77149343491\n",
      "epoch 2, batch 513, train_loss 5.33582830429\n",
      "epoch 2, batch 514, train_loss 5.29840993881\n",
      "epoch 2, batch 515, train_loss 5.20463037491\n",
      "epoch 2, batch 516, train_loss 3.25302433968\n",
      "epoch 2, batch 517, train_loss 3.39774084091\n",
      "epoch 2, batch 518, train_loss 3.3163022995\n",
      "epoch 2, batch 519, train_loss 2.99310231209\n",
      "epoch 2, batch 520, train_loss 4.30502462387\n",
      "epoch 2, batch 521, train_loss 3.35321164131\n",
      "epoch 2, batch 522, train_loss 2.87117099762\n",
      "epoch 2, batch 523, train_loss 2.91251802444\n",
      "epoch 2, batch 524, train_loss 2.74704527855\n",
      "epoch 2, batch 525, train_loss 2.58904981613\n",
      "epoch 2, batch 526, train_loss 2.65529608727\n",
      "epoch 2, batch 527, train_loss 2.65637731552\n",
      "epoch 2, batch 528, train_loss 2.93466877937\n",
      "epoch 2, batch 529, train_loss 2.82344222069\n",
      "epoch 2, batch 530, train_loss 2.25326681137\n",
      "epoch 2, batch 531, train_loss 1.90636193752\n",
      "epoch 2, batch 532, train_loss 2.95071721077\n",
      "epoch 2, batch 533, train_loss 1.85098230839\n",
      "epoch 2, batch 534, train_loss 1.89379811287\n",
      "epoch 2, batch 535, train_loss 3.58202648163\n",
      "epoch 2, batch 536, train_loss 3.84401059151\n",
      "epoch 2, batch 537, train_loss 3.70386600494\n",
      "epoch 2, batch 538, train_loss 3.94218206406\n",
      "epoch 2, batch 539, train_loss 4.33041954041\n",
      "epoch 2, batch 540, train_loss 4.65043258667\n",
      "epoch 3, batch 0, train_loss 3.69199562073\n",
      "epoch 3, batch 1, train_loss 4.19704580307\n",
      "epoch 3, batch 2, train_loss 3.59999322891\n",
      "epoch 3, batch 3, train_loss 3.78767347336\n",
      "epoch 3, batch 4, train_loss 3.72999835014\n",
      "epoch 3, batch 5, train_loss 3.80136919022\n",
      "epoch 3, batch 6, train_loss 5.09110832214\n",
      "epoch 3, batch 7, train_loss 3.7386174202\n",
      "epoch 3, batch 8, train_loss 3.29439115524\n",
      "epoch 3, batch 9, train_loss 4.49280309677\n",
      "epoch 3, batch 10, train_loss 3.48373985291\n",
      "epoch 3, batch 11, train_loss 3.59954595566\n",
      "epoch 3, batch 12, train_loss 3.25921869278\n",
      "epoch 3, batch 13, train_loss 3.83679914474\n",
      "epoch 3, batch 14, train_loss 3.13386774063\n",
      "epoch 3, batch 15, train_loss 3.94511413574\n",
      "epoch 3, batch 16, train_loss 3.41492462158\n",
      "epoch 3, batch 17, train_loss 3.29047012329\n",
      "epoch 3, batch 18, train_loss 2.87029647827\n",
      "epoch 3, batch 19, train_loss 2.36901974678\n",
      "epoch 3, batch 20, train_loss 3.34530830383\n",
      "epoch 3, batch 21, train_loss 3.28789281845\n",
      "epoch 3, batch 22, train_loss 3.7890765667\n",
      "epoch 3, batch 23, train_loss 3.25449800491\n",
      "epoch 3, batch 24, train_loss 3.23358225822\n",
      "epoch 3, batch 25, train_loss 3.77084302902\n",
      "epoch 3, batch 26, train_loss 3.65677285194\n",
      "epoch 3, batch 27, train_loss 3.58085846901\n",
      "epoch 3, batch 28, train_loss 3.93406844139\n",
      "epoch 3, batch 29, train_loss 3.87384939194\n",
      "epoch 3, batch 30, train_loss 3.93531847\n",
      "epoch 3, batch 31, train_loss 3.33438777924\n",
      "epoch 3, batch 32, train_loss 3.0277466774\n",
      "epoch 3, batch 33, train_loss 5.14189147949\n",
      "epoch 3, batch 34, train_loss 5.22103738785\n",
      "epoch 3, batch 35, train_loss 3.67325615883\n",
      "epoch 3, batch 36, train_loss 3.68389940262\n",
      "epoch 3, batch 37, train_loss 3.69611215591\n",
      "epoch 3, batch 38, train_loss 3.74791169167\n",
      "epoch 3, batch 39, train_loss 4.26256132126\n",
      "epoch 3, batch 40, train_loss 3.31341791153\n",
      "epoch 3, batch 41, train_loss 3.56634736061\n",
      "epoch 3, batch 42, train_loss 3.65678620338\n",
      "epoch 3, batch 43, train_loss 3.42367935181\n",
      "epoch 3, batch 44, train_loss 4.01640033722\n",
      "epoch 3, batch 45, train_loss 3.64605093002\n",
      "epoch 3, batch 46, train_loss 3.57483148575\n",
      "epoch 3, batch 47, train_loss 3.73997378349\n",
      "epoch 3, batch 48, train_loss 3.56350874901\n",
      "epoch 3, batch 49, train_loss 3.72421264648\n",
      "epoch 3, batch 50, train_loss 3.55968236923\n",
      "epoch 3, batch 51, train_loss 3.23005414009\n",
      "epoch 3, batch 52, train_loss 3.72562766075\n",
      "epoch 3, batch 53, train_loss 3.62056016922\n",
      "epoch 3, batch 54, train_loss 3.29376196861\n",
      "epoch 3, batch 55, train_loss 3.33311963081\n",
      "epoch 3, batch 56, train_loss 3.22888803482\n",
      "epoch 3, batch 57, train_loss 3.78327298164\n",
      "epoch 3, batch 58, train_loss 3.57950234413\n",
      "epoch 3, batch 59, train_loss 3.5239982605\n",
      "epoch 3, batch 60, train_loss 3.72938156128\n",
      "epoch 3, batch 61, train_loss 2.2094836235\n",
      "epoch 3, batch 62, train_loss 3.27826094627\n",
      "epoch 3, batch 63, train_loss 3.44231581688\n",
      "epoch 3, batch 64, train_loss 3.63722443581\n",
      "epoch 3, batch 65, train_loss 4.00220155716\n",
      "epoch 3, batch 66, train_loss 3.76250934601\n",
      "epoch 3, batch 67, train_loss 2.43943810463\n",
      "epoch 3, batch 68, train_loss 3.46953034401\n",
      "epoch 3, batch 69, train_loss 2.74730825424\n",
      "epoch 3, batch 70, train_loss 4.61908864975\n",
      "epoch 3, batch 71, train_loss 4.62697792053\n",
      "epoch 3, batch 72, train_loss 3.25937390327\n",
      "epoch 3, batch 73, train_loss 3.74424219131\n",
      "epoch 3, batch 74, train_loss 4.45360946655\n",
      "epoch 3, batch 75, train_loss 3.78965234756\n",
      "epoch 3, batch 76, train_loss 4.03298473358\n",
      "epoch 3, batch 77, train_loss 4.85453271866\n",
      "epoch 3, batch 78, train_loss 3.52578902245\n",
      "epoch 3, batch 79, train_loss 3.55880975723\n",
      "epoch 3, batch 80, train_loss 3.35960459709\n",
      "epoch 3, batch 81, train_loss 3.48765516281\n",
      "epoch 3, batch 82, train_loss 3.37710881233\n",
      "epoch 3, batch 83, train_loss 3.491335392\n",
      "epoch 3, batch 84, train_loss 3.27450680733\n",
      "epoch 3, batch 85, train_loss 3.30313682556\n",
      "epoch 3, batch 86, train_loss 3.80140686035\n",
      "epoch 3, batch 87, train_loss 2.96076273918\n",
      "epoch 3, batch 88, train_loss 3.27436804771\n",
      "epoch 3, batch 89, train_loss 3.59618473053\n",
      "epoch 3, batch 90, train_loss 3.2472114563\n",
      "epoch 3, batch 91, train_loss 3.37595200539\n",
      "epoch 3, batch 92, train_loss 3.92006826401\n",
      "epoch 3, batch 93, train_loss 4.74760723114\n",
      "epoch 3, batch 94, train_loss 3.37149262428\n",
      "epoch 3, batch 95, train_loss 3.46962809563\n",
      "epoch 3, batch 96, train_loss 2.75518059731\n",
      "epoch 3, batch 97, train_loss 3.45464682579\n",
      "epoch 3, batch 98, train_loss 3.36706280708\n",
      "epoch 3, batch 99, train_loss 3.69455337524\n",
      "epoch 3, batch 100, train_loss 3.40202760696\n",
      "epoch 3, batch 101, train_loss 3.61537599564\n",
      "epoch 3, batch 102, train_loss 3.39587926865\n",
      "epoch 3, batch 103, train_loss 4.70034456253\n",
      "epoch 3, batch 104, train_loss 3.89539074898\n",
      "epoch 3, batch 105, train_loss 3.67619943619\n",
      "epoch 3, batch 106, train_loss 4.1243596077\n",
      "epoch 3, batch 107, train_loss 3.92045092583\n",
      "epoch 3, batch 108, train_loss 3.52712273598\n",
      "epoch 3, batch 109, train_loss 3.75464940071\n",
      "epoch 3, batch 110, train_loss 3.57197904587\n",
      "epoch 3, batch 111, train_loss 3.99388170242\n",
      "epoch 3, batch 112, train_loss 3.96597695351\n",
      "epoch 3, batch 113, train_loss 3.36510539055\n",
      "epoch 3, batch 114, train_loss 3.96258616447\n",
      "epoch 3, batch 115, train_loss 4.8601436615\n",
      "epoch 3, batch 116, train_loss 4.74843740463\n",
      "epoch 3, batch 117, train_loss 4.82123947144\n",
      "epoch 3, batch 118, train_loss 4.21387720108\n",
      "epoch 3, batch 119, train_loss 2.79345488548\n",
      "epoch 3, batch 120, train_loss 3.6696999073\n",
      "epoch 3, batch 121, train_loss 3.5427069664\n",
      "epoch 3, batch 122, train_loss 3.77596545219\n",
      "epoch 3, batch 123, train_loss 3.48847270012\n",
      "epoch 3, batch 124, train_loss 3.21693611145\n",
      "epoch 3, batch 125, train_loss 3.13306045532\n",
      "epoch 3, batch 126, train_loss 3.1163918972\n",
      "epoch 3, batch 127, train_loss 3.21812367439\n",
      "epoch 3, batch 128, train_loss 3.09478020668\n",
      "epoch 3, batch 129, train_loss 3.46236658096\n",
      "epoch 3, batch 130, train_loss 3.3834939003\n",
      "epoch 3, batch 131, train_loss 3.74301719666\n",
      "epoch 3, batch 132, train_loss 3.46439647675\n",
      "epoch 3, batch 133, train_loss 2.60573601723\n",
      "epoch 3, batch 134, train_loss 2.69392633438\n",
      "epoch 3, batch 135, train_loss 4.71678924561\n",
      "epoch 3, batch 136, train_loss 3.9783308506\n",
      "epoch 3, batch 137, train_loss 3.10880422592\n",
      "epoch 3, batch 138, train_loss 2.94468712807\n",
      "epoch 3, batch 139, train_loss 3.4739408493\n",
      "epoch 3, batch 140, train_loss 2.92228865623\n",
      "epoch 3, batch 141, train_loss 3.31987500191\n",
      "epoch 3, batch 142, train_loss 4.72137737274\n",
      "epoch 3, batch 143, train_loss 2.97022867203\n",
      "epoch 3, batch 144, train_loss 2.39562487602\n",
      "epoch 3, batch 145, train_loss 3.50322008133\n",
      "epoch 3, batch 146, train_loss 3.67946076393\n",
      "epoch 3, batch 147, train_loss 3.39848566055\n",
      "epoch 3, batch 148, train_loss 3.26698875427\n",
      "epoch 3, batch 149, train_loss 3.78372859955\n",
      "epoch 3, batch 150, train_loss 3.8642616272\n",
      "epoch 3, batch 151, train_loss 2.37581729889\n",
      "epoch 3, batch 152, train_loss 2.67782473564\n",
      "epoch 3, batch 153, train_loss 4.60594320297\n",
      "epoch 3, batch 154, train_loss 3.60866260529\n",
      "epoch 3, batch 155, train_loss 3.14132547379\n",
      "epoch 3, batch 156, train_loss 3.35259389877\n",
      "epoch 3, batch 157, train_loss 2.8642206192\n",
      "epoch 3, batch 158, train_loss 2.93090867996\n",
      "epoch 3, batch 159, train_loss 3.13394331932\n",
      "epoch 3, batch 160, train_loss 3.27953767776\n",
      "epoch 3, batch 161, train_loss 3.8488471508\n",
      "epoch 3, batch 162, train_loss 3.7333407402\n",
      "epoch 3, batch 163, train_loss 5.3910574913\n",
      "epoch 3, batch 164, train_loss 2.6059896946\n",
      "epoch 3, batch 165, train_loss 2.56028532982\n",
      "epoch 3, batch 166, train_loss 2.99774336815\n",
      "epoch 3, batch 167, train_loss 3.00271344185\n",
      "epoch 3, batch 168, train_loss 3.25748038292\n",
      "epoch 3, batch 169, train_loss 2.98930430412\n",
      "epoch 3, batch 170, train_loss 3.6056303978\n",
      "epoch 3, batch 171, train_loss 2.98020482063\n",
      "epoch 3, batch 172, train_loss 3.6637160778\n",
      "epoch 3, batch 173, train_loss 2.43549585342\n",
      "epoch 3, batch 174, train_loss 4.15430021286\n",
      "epoch 3, batch 175, train_loss 3.56927251816\n",
      "epoch 3, batch 176, train_loss 3.42678546906\n",
      "epoch 3, batch 177, train_loss 3.21850657463\n",
      "epoch 3, batch 178, train_loss 3.13764405251\n",
      "epoch 3, batch 179, train_loss 3.39476013184\n",
      "epoch 3, batch 180, train_loss 3.28026294708\n",
      "epoch 3, batch 181, train_loss 4.02251291275\n",
      "epoch 3, batch 182, train_loss 3.40924668312\n",
      "epoch 3, batch 183, train_loss 4.09722614288\n",
      "epoch 3, batch 184, train_loss 2.65585255623\n",
      "epoch 3, batch 185, train_loss 3.04137992859\n",
      "epoch 3, batch 186, train_loss 3.82013177872\n",
      "epoch 3, batch 187, train_loss 2.8325278759\n",
      "epoch 3, batch 188, train_loss 3.13083362579\n",
      "epoch 3, batch 189, train_loss 3.12605333328\n",
      "epoch 3, batch 190, train_loss 3.97407317162\n",
      "epoch 3, batch 191, train_loss 3.83707022667\n",
      "epoch 3, batch 192, train_loss 4.91980981827\n",
      "epoch 3, batch 193, train_loss 4.59579992294\n",
      "epoch 3, batch 194, train_loss 5.28212022781\n",
      "epoch 3, batch 195, train_loss 5.34978675842\n",
      "epoch 3, batch 196, train_loss 3.58152842522\n",
      "epoch 3, batch 197, train_loss 2.87928390503\n",
      "epoch 3, batch 198, train_loss 2.62704181671\n",
      "epoch 3, batch 199, train_loss 3.76563334465\n",
      "epoch 3, batch 200, train_loss 3.13580846786\n",
      "epoch 3, batch 201, train_loss 3.50809383392\n",
      "epoch 3, batch 202, train_loss 3.96190571785\n",
      "epoch 3, batch 203, train_loss 4.10372591019\n",
      "epoch 3, batch 204, train_loss 4.04981517792\n",
      "epoch 3, batch 205, train_loss 3.58872628212\n",
      "epoch 3, batch 206, train_loss 4.76397943497\n",
      "epoch 3, batch 207, train_loss 3.92982816696\n",
      "epoch 3, batch 208, train_loss 4.71515130997\n",
      "epoch 3, batch 209, train_loss 4.50084018707\n",
      "epoch 3, batch 210, train_loss 2.43120884895\n",
      "epoch 3, batch 211, train_loss 3.27429986\n",
      "epoch 3, batch 212, train_loss 4.26394414902\n",
      "epoch 3, batch 213, train_loss 3.53802204132\n",
      "epoch 3, batch 214, train_loss 3.36895632744\n",
      "epoch 3, batch 215, train_loss 2.51178765297\n",
      "epoch 3, batch 216, train_loss 3.81339001656\n",
      "epoch 3, batch 217, train_loss 3.16507363319\n",
      "epoch 3, batch 218, train_loss 3.70058059692\n",
      "epoch 3, batch 219, train_loss 3.8098487854\n",
      "epoch 3, batch 220, train_loss 3.85357427597\n",
      "epoch 3, batch 221, train_loss 4.06143426895\n",
      "epoch 3, batch 222, train_loss 2.63575863838\n",
      "epoch 3, batch 223, train_loss 2.65382766724\n",
      "epoch 3, batch 224, train_loss 3.0081114769\n",
      "epoch 3, batch 225, train_loss 3.15659070015\n",
      "epoch 3, batch 226, train_loss 3.76452803612\n",
      "epoch 3, batch 227, train_loss 3.46417069435\n",
      "epoch 3, batch 228, train_loss 3.65596294403\n",
      "epoch 3, batch 229, train_loss 3.46248078346\n",
      "epoch 3, batch 230, train_loss 3.29868483543\n",
      "epoch 3, batch 231, train_loss 2.98433160782\n",
      "epoch 3, batch 232, train_loss 3.68281602859\n",
      "epoch 3, batch 233, train_loss 3.9568707943\n",
      "epoch 3, batch 234, train_loss 3.46519851685\n",
      "epoch 3, batch 235, train_loss 4.13081264496\n",
      "epoch 3, batch 236, train_loss 3.55834126472\n",
      "epoch 3, batch 237, train_loss 3.48869657516\n",
      "epoch 3, batch 238, train_loss 3.48768091202\n",
      "epoch 3, batch 239, train_loss 3.26709604263\n",
      "epoch 3, batch 240, train_loss 3.77185583115\n",
      "epoch 3, batch 241, train_loss 3.72346735001\n",
      "epoch 3, batch 242, train_loss 3.52674794197\n",
      "epoch 3, batch 243, train_loss 3.7300760746\n",
      "epoch 3, batch 244, train_loss 3.84069943428\n",
      "epoch 3, batch 245, train_loss 3.58479809761\n",
      "epoch 3, batch 246, train_loss 3.5156056881\n",
      "epoch 3, batch 247, train_loss 3.70926117897\n",
      "epoch 3, batch 248, train_loss 3.44234848022\n",
      "epoch 3, batch 249, train_loss 4.21122074127\n",
      "epoch 3, batch 250, train_loss 3.99653673172\n",
      "epoch 3, batch 251, train_loss 4.42435455322\n",
      "epoch 3, batch 252, train_loss 3.0067551136\n",
      "epoch 3, batch 253, train_loss 2.48366832733\n",
      "epoch 3, batch 254, train_loss 3.23056077957\n",
      "epoch 3, batch 255, train_loss 2.49137353897\n",
      "epoch 3, batch 256, train_loss 2.91087579727\n",
      "epoch 3, batch 257, train_loss 3.4331703186\n",
      "epoch 3, batch 258, train_loss 2.74836397171\n",
      "epoch 3, batch 259, train_loss 2.67577838898\n",
      "epoch 3, batch 260, train_loss 2.3528752327\n",
      "epoch 3, batch 261, train_loss 3.91169786453\n",
      "epoch 3, batch 262, train_loss 4.57530069351\n",
      "epoch 3, batch 263, train_loss 3.52466368675\n",
      "epoch 3, batch 264, train_loss 3.18585252762\n",
      "epoch 3, batch 265, train_loss 3.93576955795\n",
      "epoch 3, batch 266, train_loss 4.00846004486\n",
      "epoch 3, batch 267, train_loss 3.15266108513\n",
      "epoch 3, batch 268, train_loss 2.66861748695\n",
      "epoch 3, batch 269, train_loss 2.72721266747\n",
      "epoch 3, batch 270, train_loss 3.06063628197\n",
      "epoch 3, batch 271, train_loss 3.45287179947\n",
      "epoch 3, batch 272, train_loss 3.45964717865\n",
      "epoch 3, batch 273, train_loss 3.60802388191\n",
      "epoch 3, batch 274, train_loss 3.34513688087\n",
      "epoch 3, batch 275, train_loss 3.39638280869\n",
      "epoch 3, batch 276, train_loss 3.11077165604\n",
      "epoch 3, batch 277, train_loss 4.05002641678\n",
      "epoch 3, batch 278, train_loss 3.14699792862\n",
      "epoch 3, batch 279, train_loss 3.72992277145\n",
      "epoch 3, batch 280, train_loss 3.72758030891\n",
      "epoch 3, batch 281, train_loss 3.48321342468\n",
      "epoch 3, batch 282, train_loss 3.08039736748\n",
      "epoch 3, batch 283, train_loss 4.91489076614\n",
      "epoch 3, batch 284, train_loss 3.6745531559\n",
      "epoch 3, batch 285, train_loss 2.2591483593\n",
      "epoch 3, batch 286, train_loss 2.50866508484\n",
      "epoch 3, batch 287, train_loss 2.62195825577\n",
      "epoch 3, batch 288, train_loss 3.321611166\n",
      "epoch 3, batch 289, train_loss 3.40117120743\n",
      "epoch 3, batch 290, train_loss 3.28073191643\n",
      "epoch 3, batch 291, train_loss 3.09592199326\n",
      "epoch 3, batch 292, train_loss 2.52120923996\n",
      "epoch 3, batch 293, train_loss 3.77996373177\n",
      "epoch 3, batch 294, train_loss 3.27197289467\n",
      "epoch 3, batch 295, train_loss 2.94711995125\n",
      "epoch 3, batch 296, train_loss 3.27107930183\n",
      "epoch 3, batch 297, train_loss 2.91433858871\n",
      "epoch 3, batch 298, train_loss 3.25388121605\n",
      "epoch 3, batch 299, train_loss 3.91801857948\n",
      "epoch 3, batch 300, train_loss 3.55989074707\n",
      "epoch 3, batch 301, train_loss 4.71610069275\n",
      "epoch 3, batch 302, train_loss 4.71434640884\n",
      "epoch 3, batch 303, train_loss 4.31981706619\n",
      "epoch 3, batch 304, train_loss 5.09038257599\n",
      "epoch 3, batch 305, train_loss 5.03058433533\n",
      "epoch 3, batch 306, train_loss 3.53273415565\n",
      "epoch 3, batch 307, train_loss 2.81841039658\n",
      "epoch 3, batch 308, train_loss 3.30483388901\n",
      "epoch 3, batch 309, train_loss 3.47290754318\n",
      "epoch 3, batch 310, train_loss 3.25004673004\n",
      "epoch 3, batch 311, train_loss 3.38755202293\n",
      "epoch 3, batch 312, train_loss 3.58422398567\n",
      "epoch 3, batch 313, train_loss 3.24508666992\n",
      "epoch 3, batch 314, train_loss 3.67496967316\n",
      "epoch 3, batch 315, train_loss 3.0761051178\n",
      "epoch 3, batch 316, train_loss 3.23554873466\n",
      "epoch 3, batch 317, train_loss 3.77536845207\n",
      "epoch 3, batch 318, train_loss 2.81648874283\n",
      "epoch 3, batch 319, train_loss 4.80359220505\n",
      "epoch 3, batch 320, train_loss 3.55603885651\n",
      "epoch 3, batch 321, train_loss 4.94927501678\n",
      "epoch 3, batch 322, train_loss 2.72851443291\n",
      "epoch 3, batch 323, train_loss 2.57227945328\n",
      "epoch 3, batch 324, train_loss 3.8588578701\n",
      "epoch 3, batch 325, train_loss 3.42269062996\n",
      "epoch 3, batch 326, train_loss 3.44882583618\n",
      "epoch 3, batch 327, train_loss 3.43698477745\n",
      "epoch 3, batch 328, train_loss 3.29624772072\n",
      "epoch 3, batch 329, train_loss 2.89925193787\n",
      "epoch 3, batch 330, train_loss 4.35439014435\n",
      "epoch 3, batch 331, train_loss 3.96924614906\n",
      "epoch 3, batch 332, train_loss 2.70079374313\n",
      "epoch 3, batch 333, train_loss 3.94186902046\n",
      "epoch 3, batch 334, train_loss 3.60294651985\n",
      "epoch 3, batch 335, train_loss 3.5177924633\n",
      "epoch 3, batch 336, train_loss 4.14593505859\n",
      "epoch 3, batch 337, train_loss 2.53767204285\n",
      "epoch 3, batch 338, train_loss 3.4290561676\n",
      "epoch 3, batch 339, train_loss 3.200060606\n",
      "epoch 3, batch 340, train_loss 3.40337967873\n",
      "epoch 3, batch 341, train_loss 3.30313587189\n",
      "epoch 3, batch 342, train_loss 3.36892700195\n",
      "epoch 3, batch 343, train_loss 3.278881073\n",
      "epoch 3, batch 344, train_loss 4.39644622803\n",
      "epoch 3, batch 345, train_loss 3.79771661758\n",
      "epoch 3, batch 346, train_loss 3.49378442764\n",
      "epoch 3, batch 347, train_loss 3.22638893127\n",
      "epoch 3, batch 348, train_loss 3.45472455025\n",
      "epoch 3, batch 349, train_loss 4.7660369873\n",
      "epoch 3, batch 350, train_loss 4.69983386993\n",
      "epoch 3, batch 351, train_loss 4.35535097122\n",
      "epoch 3, batch 352, train_loss 4.4484667778\n",
      "epoch 3, batch 353, train_loss 3.61011075974\n",
      "epoch 3, batch 354, train_loss 2.92945194244\n",
      "epoch 3, batch 355, train_loss 3.25581407547\n",
      "epoch 3, batch 356, train_loss 3.28675150871\n",
      "epoch 3, batch 357, train_loss 3.2105884552\n",
      "epoch 3, batch 358, train_loss 2.67989087105\n",
      "epoch 3, batch 359, train_loss 3.77981448174\n",
      "epoch 3, batch 360, train_loss 3.23306512833\n",
      "epoch 3, batch 361, train_loss 2.83552598953\n",
      "epoch 3, batch 362, train_loss 4.75362014771\n",
      "epoch 3, batch 363, train_loss 3.4784924984\n",
      "epoch 3, batch 364, train_loss 3.37194824219\n",
      "epoch 3, batch 365, train_loss 4.32060623169\n",
      "epoch 3, batch 366, train_loss 4.51377344131\n",
      "epoch 3, batch 367, train_loss 5.29623603821\n",
      "epoch 3, batch 368, train_loss 2.75784993172\n",
      "epoch 3, batch 369, train_loss 3.76944470406\n",
      "epoch 3, batch 370, train_loss 4.18303060532\n",
      "epoch 3, batch 371, train_loss 5.24928665161\n",
      "epoch 3, batch 372, train_loss 2.55573916435\n",
      "epoch 3, batch 373, train_loss 2.74486541748\n",
      "epoch 3, batch 374, train_loss 3.08794903755\n",
      "epoch 3, batch 375, train_loss 3.51091074944\n",
      "epoch 3, batch 376, train_loss 2.71402549744\n",
      "epoch 3, batch 377, train_loss 3.165984869\n",
      "epoch 3, batch 378, train_loss 3.96002936363\n",
      "epoch 3, batch 379, train_loss 3.05872416496\n",
      "epoch 3, batch 380, train_loss 3.09473156929\n",
      "epoch 3, batch 381, train_loss 3.20782446861\n",
      "epoch 3, batch 382, train_loss 4.21982574463\n",
      "epoch 3, batch 383, train_loss 3.68920969963\n",
      "epoch 3, batch 384, train_loss 3.32444834709\n",
      "epoch 3, batch 385, train_loss 4.86785793304\n",
      "epoch 3, batch 386, train_loss 3.00868940353\n",
      "epoch 3, batch 387, train_loss 5.15091848373\n",
      "epoch 3, batch 388, train_loss 2.99570322037\n",
      "epoch 3, batch 389, train_loss 4.78464984894\n",
      "epoch 3, batch 390, train_loss 4.70127677917\n",
      "epoch 3, batch 391, train_loss 4.7429265976\n",
      "epoch 3, batch 392, train_loss 4.4363284111\n",
      "epoch 3, batch 393, train_loss 3.65424513817\n",
      "epoch 3, batch 394, train_loss 4.36494588852\n",
      "epoch 3, batch 395, train_loss 3.22425627708\n",
      "epoch 3, batch 396, train_loss 4.66532659531\n",
      "epoch 3, batch 397, train_loss 4.82269716263\n",
      "epoch 3, batch 398, train_loss 3.56610035896\n",
      "epoch 3, batch 399, train_loss 3.82808065414\n",
      "epoch 3, batch 400, train_loss 4.93320894241\n",
      "epoch 3, batch 401, train_loss 4.09825563431\n",
      "epoch 3, batch 402, train_loss 3.58185482025\n",
      "epoch 3, batch 403, train_loss 3.56519460678\n",
      "epoch 3, batch 404, train_loss 4.45420455933\n",
      "epoch 3, batch 405, train_loss 3.8738963604\n",
      "epoch 3, batch 406, train_loss 3.69513034821\n",
      "epoch 3, batch 407, train_loss 3.50344848633\n",
      "epoch 3, batch 408, train_loss 3.33532190323\n",
      "epoch 3, batch 409, train_loss 2.7789850235\n",
      "epoch 3, batch 410, train_loss 4.59258651733\n",
      "epoch 3, batch 411, train_loss 3.55882382393\n",
      "epoch 3, batch 412, train_loss 3.08419537544\n",
      "epoch 3, batch 413, train_loss 3.49947881699\n",
      "epoch 3, batch 414, train_loss 3.99409079552\n",
      "epoch 3, batch 415, train_loss 4.06483221054\n",
      "epoch 3, batch 416, train_loss 3.83884978294\n",
      "epoch 3, batch 417, train_loss 3.29931354523\n",
      "epoch 3, batch 418, train_loss 3.74768519402\n",
      "epoch 3, batch 419, train_loss 3.33820676804\n",
      "epoch 3, batch 420, train_loss 2.67348098755\n",
      "epoch 3, batch 421, train_loss 4.73195123672\n",
      "epoch 3, batch 422, train_loss 3.3260910511\n",
      "epoch 3, batch 423, train_loss 3.43613386154\n",
      "epoch 3, batch 424, train_loss 4.65842723846\n",
      "epoch 3, batch 425, train_loss 3.85845732689\n",
      "epoch 3, batch 426, train_loss 5.15610218048\n",
      "epoch 3, batch 427, train_loss 4.19411039352\n",
      "epoch 3, batch 428, train_loss 2.90201497078\n",
      "epoch 3, batch 429, train_loss 4.04455852509\n",
      "epoch 3, batch 430, train_loss 4.11238431931\n",
      "epoch 3, batch 431, train_loss 3.69251298904\n",
      "epoch 3, batch 432, train_loss 3.89103007317\n",
      "epoch 3, batch 433, train_loss 3.22080922127\n",
      "epoch 3, batch 434, train_loss 3.72561740875\n",
      "epoch 3, batch 435, train_loss 3.51050639153\n",
      "epoch 3, batch 436, train_loss 3.90470743179\n",
      "epoch 3, batch 437, train_loss 4.42812728882\n",
      "epoch 3, batch 438, train_loss 3.99626684189\n",
      "epoch 3, batch 439, train_loss 3.73649096489\n",
      "epoch 3, batch 440, train_loss 5.20506381989\n",
      "epoch 3, batch 441, train_loss 5.35633325577\n",
      "epoch 3, batch 442, train_loss 4.37922430038\n",
      "epoch 3, batch 443, train_loss 2.71191906929\n",
      "epoch 3, batch 444, train_loss 3.24435782433\n",
      "epoch 3, batch 445, train_loss 4.95734214783\n",
      "epoch 3, batch 446, train_loss 3.62788438797\n",
      "epoch 3, batch 447, train_loss 3.31532788277\n",
      "epoch 3, batch 448, train_loss 4.84378290176\n",
      "epoch 3, batch 449, train_loss 4.12394618988\n",
      "epoch 3, batch 450, train_loss 3.47264719009\n",
      "epoch 3, batch 451, train_loss 3.25944757462\n",
      "epoch 3, batch 452, train_loss 2.94139695168\n",
      "epoch 3, batch 453, train_loss 5.57256174088\n",
      "epoch 3, batch 454, train_loss 5.53795814514\n",
      "epoch 3, batch 455, train_loss 2.78183722496\n",
      "epoch 3, batch 456, train_loss 3.58023142815\n",
      "epoch 3, batch 457, train_loss 3.88394165039\n",
      "epoch 3, batch 458, train_loss 3.39752697945\n",
      "epoch 3, batch 459, train_loss 3.76438617706\n",
      "epoch 3, batch 460, train_loss 4.36130952835\n",
      "epoch 3, batch 461, train_loss 3.73954558372\n",
      "epoch 3, batch 462, train_loss 3.41659331322\n",
      "epoch 3, batch 463, train_loss 3.58697915077\n",
      "epoch 3, batch 464, train_loss 3.8980910778\n",
      "epoch 3, batch 465, train_loss 3.21331715584\n",
      "epoch 3, batch 466, train_loss 4.06409597397\n",
      "epoch 3, batch 467, train_loss 3.84451532364\n",
      "epoch 3, batch 468, train_loss 4.00127267838\n",
      "epoch 3, batch 469, train_loss 3.50721359253\n",
      "epoch 3, batch 470, train_loss 3.38878846169\n",
      "epoch 3, batch 471, train_loss 3.45092773438\n",
      "epoch 3, batch 472, train_loss 3.3088555336\n",
      "epoch 3, batch 473, train_loss 4.09573411942\n",
      "epoch 3, batch 474, train_loss 4.74177122116\n",
      "epoch 3, batch 475, train_loss 3.26970386505\n",
      "epoch 3, batch 476, train_loss 2.77732157707\n",
      "epoch 3, batch 477, train_loss 2.91339182854\n",
      "epoch 3, batch 478, train_loss 2.74392533302\n",
      "epoch 3, batch 479, train_loss 3.30080890656\n",
      "epoch 3, batch 480, train_loss 3.52031159401\n",
      "epoch 3, batch 481, train_loss 5.26426696777\n",
      "epoch 3, batch 482, train_loss 3.18765878677\n",
      "epoch 3, batch 483, train_loss 3.90141677856\n",
      "epoch 3, batch 484, train_loss 3.51191186905\n",
      "epoch 3, batch 485, train_loss 2.67338490486\n",
      "epoch 3, batch 486, train_loss 2.67073297501\n",
      "epoch 3, batch 487, train_loss 2.81255197525\n",
      "epoch 3, batch 488, train_loss 3.25786519051\n",
      "epoch 3, batch 489, train_loss 3.35932254791\n",
      "epoch 3, batch 490, train_loss 3.31791973114\n",
      "epoch 3, batch 491, train_loss 3.22606062889\n",
      "epoch 3, batch 492, train_loss 3.54207873344\n",
      "epoch 3, batch 493, train_loss 3.08217120171\n",
      "epoch 3, batch 494, train_loss 3.21283626556\n",
      "epoch 3, batch 495, train_loss 3.28604698181\n",
      "epoch 3, batch 496, train_loss 2.88804483414\n",
      "epoch 3, batch 497, train_loss 3.08817648888\n",
      "epoch 3, batch 498, train_loss 3.2491440773\n",
      "epoch 3, batch 499, train_loss 3.19499278069\n",
      "epoch 3, batch 500, train_loss 3.74041748047\n",
      "epoch 3, batch 501, train_loss 3.31356024742\n",
      "epoch 3, batch 502, train_loss 4.76491451263\n",
      "epoch 3, batch 503, train_loss 4.72256231308\n",
      "epoch 3, batch 504, train_loss 4.09730625153\n",
      "epoch 3, batch 505, train_loss 3.87527632713\n",
      "epoch 3, batch 506, train_loss 3.52700567245\n",
      "epoch 3, batch 507, train_loss 4.60153341293\n",
      "epoch 3, batch 508, train_loss 3.26917886734\n",
      "epoch 3, batch 509, train_loss 4.76042795181\n",
      "epoch 3, batch 510, train_loss 4.76036691666\n",
      "epoch 3, batch 511, train_loss 4.77466106415\n",
      "epoch 3, batch 512, train_loss 3.67541861534\n",
      "epoch 3, batch 513, train_loss 5.2055478096\n",
      "epoch 3, batch 514, train_loss 5.14597940445\n",
      "epoch 3, batch 515, train_loss 5.06003665924\n",
      "epoch 3, batch 516, train_loss 3.14813923836\n",
      "epoch 3, batch 517, train_loss 3.31652665138\n",
      "epoch 3, batch 518, train_loss 3.22183275223\n",
      "epoch 3, batch 519, train_loss 2.91225719452\n",
      "epoch 3, batch 520, train_loss 4.18280124664\n",
      "epoch 3, batch 521, train_loss 3.25290298462\n",
      "epoch 3, batch 522, train_loss 2.78480792046\n",
      "epoch 3, batch 523, train_loss 2.8360004425\n",
      "epoch 3, batch 524, train_loss 2.67107367516\n",
      "epoch 3, batch 525, train_loss 2.50758051872\n",
      "epoch 3, batch 526, train_loss 2.58517670631\n",
      "epoch 3, batch 527, train_loss 2.59752464294\n",
      "epoch 3, batch 528, train_loss 2.86539435387\n",
      "epoch 3, batch 529, train_loss 2.74366116524\n",
      "epoch 3, batch 530, train_loss 2.20023965836\n",
      "epoch 3, batch 531, train_loss 1.86789619923\n",
      "epoch 3, batch 532, train_loss 2.88899707794\n",
      "epoch 3, batch 533, train_loss 1.8047798872\n",
      "epoch 3, batch 534, train_loss 1.86002552509\n",
      "epoch 3, batch 535, train_loss 3.47989058495\n",
      "epoch 3, batch 536, train_loss 3.72668647766\n",
      "epoch 3, batch 537, train_loss 3.59253787994\n",
      "epoch 3, batch 538, train_loss 3.80914521217\n",
      "epoch 3, batch 539, train_loss 4.16262149811\n",
      "epoch 3, batch 540, train_loss 4.48590946198\n",
      "epoch 4, batch 0, train_loss 3.62917017937\n",
      "epoch 4, batch 1, train_loss 4.1027340889\n",
      "epoch 4, batch 2, train_loss 3.47658467293\n",
      "epoch 4, batch 3, train_loss 3.68590903282\n",
      "epoch 4, batch 4, train_loss 3.58140683174\n",
      "epoch 4, batch 5, train_loss 3.63954544067\n",
      "epoch 4, batch 6, train_loss 4.8548579216\n",
      "epoch 4, batch 7, train_loss 3.59120082855\n",
      "epoch 4, batch 8, train_loss 3.17530536652\n",
      "epoch 4, batch 9, train_loss 4.31492757797\n",
      "epoch 4, batch 10, train_loss 3.36543750763\n",
      "epoch 4, batch 11, train_loss 3.48238754272\n",
      "epoch 4, batch 12, train_loss 3.16616201401\n",
      "epoch 4, batch 13, train_loss 3.71932291985\n",
      "epoch 4, batch 14, train_loss 3.03967881203\n",
      "epoch 4, batch 15, train_loss 3.82698130608\n",
      "epoch 4, batch 16, train_loss 3.31881952286\n",
      "epoch 4, batch 17, train_loss 3.18495965004\n",
      "epoch 4, batch 18, train_loss 2.7755279541\n",
      "epoch 4, batch 19, train_loss 2.29760289192\n",
      "epoch 4, batch 20, train_loss 3.23271918297\n",
      "epoch 4, batch 21, train_loss 3.18070936203\n",
      "epoch 4, batch 22, train_loss 3.68794870377\n",
      "epoch 4, batch 23, train_loss 3.17617630959\n",
      "epoch 4, batch 24, train_loss 3.16813015938\n",
      "epoch 4, batch 25, train_loss 3.67922234535\n",
      "epoch 4, batch 26, train_loss 3.57965326309\n",
      "epoch 4, batch 27, train_loss 3.49176979065\n",
      "epoch 4, batch 28, train_loss 3.83432626724\n",
      "epoch 4, batch 29, train_loss 3.75067877769\n",
      "epoch 4, batch 30, train_loss 3.83697104454\n",
      "epoch 4, batch 31, train_loss 3.23789930344\n",
      "epoch 4, batch 32, train_loss 2.94659638405\n",
      "epoch 4, batch 33, train_loss 5.012591362\n",
      "epoch 4, batch 34, train_loss 5.08178758621\n",
      "epoch 4, batch 35, train_loss 3.58128523827\n",
      "epoch 4, batch 36, train_loss 3.6016433239\n",
      "epoch 4, batch 37, train_loss 3.58524847031\n",
      "epoch 4, batch 38, train_loss 3.66264724731\n",
      "epoch 4, batch 39, train_loss 4.17147827148\n",
      "epoch 4, batch 40, train_loss 3.23199629784\n",
      "epoch 4, batch 41, train_loss 3.49162364006\n",
      "epoch 4, batch 42, train_loss 3.54647254944\n",
      "epoch 4, batch 43, train_loss 3.31867098808\n",
      "epoch 4, batch 44, train_loss 3.93653583527\n",
      "epoch 4, batch 45, train_loss 3.55277109146\n",
      "epoch 4, batch 46, train_loss 3.49602198601\n",
      "epoch 4, batch 47, train_loss 3.64455270767\n",
      "epoch 4, batch 48, train_loss 3.45703411102\n",
      "epoch 4, batch 49, train_loss 3.61740279198\n",
      "epoch 4, batch 50, train_loss 3.47207808495\n",
      "epoch 4, batch 51, train_loss 3.15067052841\n",
      "epoch 4, batch 52, train_loss 3.62993383408\n",
      "epoch 4, batch 53, train_loss 3.50757908821\n",
      "epoch 4, batch 54, train_loss 3.2047226429\n",
      "epoch 4, batch 55, train_loss 3.24726819992\n",
      "epoch 4, batch 56, train_loss 3.14071035385\n",
      "epoch 4, batch 57, train_loss 3.67905569077\n",
      "epoch 4, batch 58, train_loss 3.46697163582\n",
      "epoch 4, batch 59, train_loss 3.42565703392\n",
      "epoch 4, batch 60, train_loss 3.63275766373\n",
      "epoch 4, batch 61, train_loss 2.1288022995\n",
      "epoch 4, batch 62, train_loss 3.18502736092\n",
      "epoch 4, batch 63, train_loss 3.33881354332\n",
      "epoch 4, batch 64, train_loss 3.54679918289\n",
      "epoch 4, batch 65, train_loss 3.90191721916\n",
      "epoch 4, batch 66, train_loss 3.66520524025\n",
      "epoch 4, batch 67, train_loss 2.36328244209\n",
      "epoch 4, batch 68, train_loss 3.36835408211\n",
      "epoch 4, batch 69, train_loss 2.65055608749\n",
      "epoch 4, batch 70, train_loss 4.45348405838\n",
      "epoch 4, batch 71, train_loss 4.47051286697\n",
      "epoch 4, batch 72, train_loss 3.13778281212\n",
      "epoch 4, batch 73, train_loss 3.61090707779\n",
      "epoch 4, batch 74, train_loss 4.29335927963\n",
      "epoch 4, batch 75, train_loss 3.66195344925\n",
      "epoch 4, batch 76, train_loss 3.91157317162\n",
      "epoch 4, batch 77, train_loss 4.72731637955\n",
      "epoch 4, batch 78, train_loss 3.43219017982\n",
      "epoch 4, batch 79, train_loss 3.45948076248\n",
      "epoch 4, batch 80, train_loss 3.25689697266\n",
      "epoch 4, batch 81, train_loss 3.38788437843\n",
      "epoch 4, batch 82, train_loss 3.2827026844\n",
      "epoch 4, batch 83, train_loss 3.39441037178\n",
      "epoch 4, batch 84, train_loss 3.16503286362\n",
      "epoch 4, batch 85, train_loss 3.20554280281\n",
      "epoch 4, batch 86, train_loss 3.70736575127\n",
      "epoch 4, batch 87, train_loss 2.88485956192\n",
      "epoch 4, batch 88, train_loss 3.18927836418\n",
      "epoch 4, batch 89, train_loss 3.51336956024\n",
      "epoch 4, batch 90, train_loss 3.17705631256\n",
      "epoch 4, batch 91, train_loss 3.27583122253\n",
      "epoch 4, batch 92, train_loss 3.79655909538\n",
      "epoch 4, batch 93, train_loss 4.61171150208\n",
      "epoch 4, batch 94, train_loss 3.28262352943\n",
      "epoch 4, batch 95, train_loss 3.36687088013\n",
      "epoch 4, batch 96, train_loss 2.66522407532\n",
      "epoch 4, batch 97, train_loss 3.35185456276\n",
      "epoch 4, batch 98, train_loss 3.27463006973\n",
      "epoch 4, batch 99, train_loss 3.57515740395\n",
      "epoch 4, batch 100, train_loss 3.29648041725\n",
      "epoch 4, batch 101, train_loss 3.50608086586\n",
      "epoch 4, batch 102, train_loss 3.29847288132\n",
      "epoch 4, batch 103, train_loss 4.58948087692\n",
      "epoch 4, batch 104, train_loss 3.7950835228\n",
      "epoch 4, batch 105, train_loss 3.58911371231\n",
      "epoch 4, batch 106, train_loss 4.02332687378\n",
      "epoch 4, batch 107, train_loss 3.82822704315\n",
      "epoch 4, batch 108, train_loss 3.43345808983\n",
      "epoch 4, batch 109, train_loss 3.68038368225\n",
      "epoch 4, batch 110, train_loss 3.49219655991\n",
      "epoch 4, batch 111, train_loss 3.90731692314\n",
      "epoch 4, batch 112, train_loss 3.88265919685\n",
      "epoch 4, batch 113, train_loss 3.27427911758\n",
      "epoch 4, batch 114, train_loss 3.86048173904\n",
      "epoch 4, batch 115, train_loss 4.72802114487\n",
      "epoch 4, batch 116, train_loss 4.62077522278\n",
      "epoch 4, batch 117, train_loss 4.71582603455\n",
      "epoch 4, batch 118, train_loss 4.09962034225\n",
      "epoch 4, batch 119, train_loss 2.69576764107\n",
      "epoch 4, batch 120, train_loss 3.57570672035\n",
      "epoch 4, batch 121, train_loss 3.44453525543\n",
      "epoch 4, batch 122, train_loss 3.66790604591\n",
      "epoch 4, batch 123, train_loss 3.40030860901\n",
      "epoch 4, batch 124, train_loss 3.1091902256\n",
      "epoch 4, batch 125, train_loss 3.0424990654\n",
      "epoch 4, batch 126, train_loss 3.0248105526\n",
      "epoch 4, batch 127, train_loss 3.11927461624\n",
      "epoch 4, batch 128, train_loss 2.98662424088\n",
      "epoch 4, batch 129, train_loss 3.36497282982\n",
      "epoch 4, batch 130, train_loss 3.28551292419\n",
      "epoch 4, batch 131, train_loss 3.65829396248\n",
      "epoch 4, batch 132, train_loss 3.37280035019\n",
      "epoch 4, batch 133, train_loss 2.53988361359\n",
      "epoch 4, batch 134, train_loss 2.60874700546\n",
      "epoch 4, batch 135, train_loss 4.58459949493\n",
      "epoch 4, batch 136, train_loss 3.88512659073\n",
      "epoch 4, batch 137, train_loss 3.01084589958\n",
      "epoch 4, batch 138, train_loss 2.8587884903\n",
      "epoch 4, batch 139, train_loss 3.3889734745\n",
      "epoch 4, batch 140, train_loss 2.83882498741\n",
      "epoch 4, batch 141, train_loss 3.21988272667\n",
      "epoch 4, batch 142, train_loss 4.58609390259\n",
      "epoch 4, batch 143, train_loss 2.88307142258\n",
      "epoch 4, batch 144, train_loss 2.31553649902\n",
      "epoch 4, batch 145, train_loss 3.40390253067\n",
      "epoch 4, batch 146, train_loss 3.57838821411\n",
      "epoch 4, batch 147, train_loss 3.30617165565\n",
      "epoch 4, batch 148, train_loss 3.17726302147\n",
      "epoch 4, batch 149, train_loss 3.68553233147\n",
      "epoch 4, batch 150, train_loss 3.76750254631\n",
      "epoch 4, batch 151, train_loss 2.30930876732\n",
      "epoch 4, batch 152, train_loss 2.60328841209\n",
      "epoch 4, batch 153, train_loss 4.46910572052\n",
      "epoch 4, batch 154, train_loss 3.50399208069\n",
      "epoch 4, batch 155, train_loss 3.05187606812\n",
      "epoch 4, batch 156, train_loss 3.27394270897\n",
      "epoch 4, batch 157, train_loss 2.79956054688\n",
      "epoch 4, batch 158, train_loss 2.85302662849\n",
      "epoch 4, batch 159, train_loss 3.05147480965\n",
      "epoch 4, batch 160, train_loss 3.18784022331\n",
      "epoch 4, batch 161, train_loss 3.74377727509\n",
      "epoch 4, batch 162, train_loss 3.65097045898\n",
      "epoch 4, batch 163, train_loss 5.25361776352\n",
      "epoch 4, batch 164, train_loss 2.53828787804\n",
      "epoch 4, batch 165, train_loss 2.48846817017\n",
      "epoch 4, batch 166, train_loss 2.90764665604\n",
      "epoch 4, batch 167, train_loss 2.91424536705\n",
      "epoch 4, batch 168, train_loss 3.17210221291\n",
      "epoch 4, batch 169, train_loss 2.90867114067\n",
      "epoch 4, batch 170, train_loss 3.50203728676\n",
      "epoch 4, batch 171, train_loss 2.89003920555\n",
      "epoch 4, batch 172, train_loss 3.55718612671\n",
      "epoch 4, batch 173, train_loss 2.36035323143\n",
      "epoch 4, batch 174, train_loss 4.05176544189\n",
      "epoch 4, batch 175, train_loss 3.47727131844\n",
      "epoch 4, batch 176, train_loss 3.33942365646\n",
      "epoch 4, batch 177, train_loss 3.13897299767\n",
      "epoch 4, batch 178, train_loss 3.04950141907\n",
      "epoch 4, batch 179, train_loss 3.31188511848\n",
      "epoch 4, batch 180, train_loss 3.19808745384\n",
      "epoch 4, batch 181, train_loss 3.91099596024\n",
      "epoch 4, batch 182, train_loss 3.31568098068\n",
      "epoch 4, batch 183, train_loss 4.00020217896\n",
      "epoch 4, batch 184, train_loss 2.59645342827\n",
      "epoch 4, batch 185, train_loss 2.9806292057\n",
      "epoch 4, batch 186, train_loss 3.73110461235\n",
      "epoch 4, batch 187, train_loss 2.74169683456\n",
      "epoch 4, batch 188, train_loss 3.05731773376\n",
      "epoch 4, batch 189, train_loss 3.04156088829\n",
      "epoch 4, batch 190, train_loss 3.87004709244\n",
      "epoch 4, batch 191, train_loss 3.74384331703\n",
      "epoch 4, batch 192, train_loss 4.79432868958\n",
      "epoch 4, batch 193, train_loss 4.477850914\n",
      "epoch 4, batch 194, train_loss 5.14082765579\n",
      "epoch 4, batch 195, train_loss 5.2167096138\n",
      "epoch 4, batch 196, train_loss 3.49337029457\n",
      "epoch 4, batch 197, train_loss 2.79341864586\n",
      "epoch 4, batch 198, train_loss 2.55289268494\n",
      "epoch 4, batch 199, train_loss 3.67864584923\n",
      "epoch 4, batch 200, train_loss 3.06553125381\n",
      "epoch 4, batch 201, train_loss 3.42144203186\n",
      "epoch 4, batch 202, train_loss 3.85613489151\n",
      "epoch 4, batch 203, train_loss 4.00326395035\n",
      "epoch 4, batch 204, train_loss 3.95619487762\n",
      "epoch 4, batch 205, train_loss 3.47839188576\n",
      "epoch 4, batch 206, train_loss 4.6146941185\n",
      "epoch 4, batch 207, train_loss 3.82142567635\n",
      "epoch 4, batch 208, train_loss 4.59708833694\n",
      "epoch 4, batch 209, train_loss 4.3637676239\n",
      "epoch 4, batch 210, train_loss 2.36740088463\n",
      "epoch 4, batch 211, train_loss 3.19574451447\n",
      "epoch 4, batch 212, train_loss 4.16866493225\n",
      "epoch 4, batch 213, train_loss 3.44744420052\n",
      "epoch 4, batch 214, train_loss 3.29046869278\n",
      "epoch 4, batch 215, train_loss 2.44929409027\n",
      "epoch 4, batch 216, train_loss 3.71224713326\n",
      "epoch 4, batch 217, train_loss 3.08631324768\n",
      "epoch 4, batch 218, train_loss 3.59927201271\n",
      "epoch 4, batch 219, train_loss 3.72042441368\n",
      "epoch 4, batch 220, train_loss 3.76333451271\n",
      "epoch 4, batch 221, train_loss 3.95264577866\n",
      "epoch 4, batch 222, train_loss 2.56308484077\n",
      "epoch 4, batch 223, train_loss 2.57326579094\n",
      "epoch 4, batch 224, train_loss 2.92910408974\n",
      "epoch 4, batch 225, train_loss 3.06650972366\n",
      "epoch 4, batch 226, train_loss 3.66416716576\n",
      "epoch 4, batch 227, train_loss 3.3742518425\n",
      "epoch 4, batch 228, train_loss 3.56180214882\n",
      "epoch 4, batch 229, train_loss 3.37199807167\n",
      "epoch 4, batch 230, train_loss 3.2148630619\n",
      "epoch 4, batch 231, train_loss 2.90478610992\n",
      "epoch 4, batch 232, train_loss 3.5939245224\n",
      "epoch 4, batch 233, train_loss 3.86096763611\n",
      "epoch 4, batch 234, train_loss 3.38422513008\n",
      "epoch 4, batch 235, train_loss 4.02564334869\n",
      "epoch 4, batch 236, train_loss 3.47547054291\n",
      "epoch 4, batch 237, train_loss 3.40680932999\n",
      "epoch 4, batch 238, train_loss 3.40518426895\n",
      "epoch 4, batch 239, train_loss 3.19756889343\n",
      "epoch 4, batch 240, train_loss 3.68617463112\n",
      "epoch 4, batch 241, train_loss 3.62637829781\n",
      "epoch 4, batch 242, train_loss 3.43688511848\n",
      "epoch 4, batch 243, train_loss 3.63484239578\n",
      "epoch 4, batch 244, train_loss 3.76087880135\n",
      "epoch 4, batch 245, train_loss 3.49998021126\n",
      "epoch 4, batch 246, train_loss 3.42916369438\n",
      "epoch 4, batch 247, train_loss 3.62631964684\n",
      "epoch 4, batch 248, train_loss 3.36167407036\n",
      "epoch 4, batch 249, train_loss 4.12072038651\n",
      "epoch 4, batch 250, train_loss 3.90176153183\n",
      "epoch 4, batch 251, train_loss 4.29927110672\n",
      "epoch 4, batch 252, train_loss 2.93425583839\n",
      "epoch 4, batch 253, train_loss 2.42177176476\n",
      "epoch 4, batch 254, train_loss 3.14185714722\n",
      "epoch 4, batch 255, train_loss 2.42464518547\n",
      "epoch 4, batch 256, train_loss 2.83975744247\n",
      "epoch 4, batch 257, train_loss 3.34288549423\n",
      "epoch 4, batch 258, train_loss 2.6761097908\n",
      "epoch 4, batch 259, train_loss 2.60105895996\n",
      "epoch 4, batch 260, train_loss 2.29829335213\n",
      "epoch 4, batch 261, train_loss 3.8130633831\n",
      "epoch 4, batch 262, train_loss 4.45990657806\n",
      "epoch 4, batch 263, train_loss 3.44141888618\n",
      "epoch 4, batch 264, train_loss 3.1158645153\n",
      "epoch 4, batch 265, train_loss 3.84790277481\n",
      "epoch 4, batch 266, train_loss 3.90264058113\n",
      "epoch 4, batch 267, train_loss 3.07389783859\n",
      "epoch 4, batch 268, train_loss 2.58041238785\n",
      "epoch 4, batch 269, train_loss 2.65009713173\n",
      "epoch 4, batch 270, train_loss 2.97796821594\n",
      "epoch 4, batch 271, train_loss 3.35941624641\n",
      "epoch 4, batch 272, train_loss 3.37727642059\n",
      "epoch 4, batch 273, train_loss 3.51969790459\n",
      "epoch 4, batch 274, train_loss 3.26721572876\n",
      "epoch 4, batch 275, train_loss 3.31069564819\n",
      "epoch 4, batch 276, train_loss 3.04229474068\n",
      "epoch 4, batch 277, train_loss 3.94509601593\n",
      "epoch 4, batch 278, train_loss 3.05913925171\n",
      "epoch 4, batch 279, train_loss 3.64621090889\n",
      "epoch 4, batch 280, train_loss 3.63173937798\n",
      "epoch 4, batch 281, train_loss 3.39043593407\n",
      "epoch 4, batch 282, train_loss 2.99704027176\n",
      "epoch 4, batch 283, train_loss 4.78749370575\n",
      "epoch 4, batch 284, train_loss 3.57707834244\n",
      "epoch 4, batch 285, train_loss 2.19547867775\n",
      "epoch 4, batch 286, train_loss 2.44330739975\n",
      "epoch 4, batch 287, train_loss 2.55428314209\n",
      "epoch 4, batch 288, train_loss 3.24822354317\n",
      "epoch 4, batch 289, train_loss 3.31685328484\n",
      "epoch 4, batch 290, train_loss 3.18720936775\n",
      "epoch 4, batch 291, train_loss 3.01352000237\n",
      "epoch 4, batch 292, train_loss 2.45615553856\n",
      "epoch 4, batch 293, train_loss 3.68217253685\n",
      "epoch 4, batch 294, train_loss 3.18460011482\n",
      "epoch 4, batch 295, train_loss 2.87355136871\n",
      "epoch 4, batch 296, train_loss 3.19380307198\n",
      "epoch 4, batch 297, train_loss 2.83253884315\n",
      "epoch 4, batch 298, train_loss 3.14459061623\n",
      "epoch 4, batch 299, train_loss 3.7963116169\n",
      "epoch 4, batch 300, train_loss 3.45615434647\n",
      "epoch 4, batch 301, train_loss 4.5808134079\n",
      "epoch 4, batch 302, train_loss 4.57685995102\n",
      "epoch 4, batch 303, train_loss 4.1908493042\n",
      "epoch 4, batch 304, train_loss 4.93965911865\n",
      "epoch 4, batch 305, train_loss 4.8797082901\n",
      "epoch 4, batch 306, train_loss 3.42094802856\n",
      "epoch 4, batch 307, train_loss 2.74655342102\n",
      "epoch 4, batch 308, train_loss 3.22811126709\n",
      "epoch 4, batch 309, train_loss 3.39559912682\n",
      "epoch 4, batch 310, train_loss 3.17016673088\n",
      "epoch 4, batch 311, train_loss 3.30950045586\n",
      "epoch 4, batch 312, train_loss 3.50506210327\n",
      "epoch 4, batch 313, train_loss 3.17857289314\n",
      "epoch 4, batch 314, train_loss 3.58212518692\n",
      "epoch 4, batch 315, train_loss 3.00248861313\n",
      "epoch 4, batch 316, train_loss 3.16199111938\n",
      "epoch 4, batch 317, train_loss 3.68700528145\n",
      "epoch 4, batch 318, train_loss 2.74556112289\n",
      "epoch 4, batch 319, train_loss 4.67457818985\n",
      "epoch 4, batch 320, train_loss 3.44776201248\n",
      "epoch 4, batch 321, train_loss 4.80146408081\n",
      "epoch 4, batch 322, train_loss 2.65467143059\n",
      "epoch 4, batch 323, train_loss 2.49597835541\n",
      "epoch 4, batch 324, train_loss 3.76359176636\n",
      "epoch 4, batch 325, train_loss 3.34215092659\n",
      "epoch 4, batch 326, train_loss 3.35912752151\n",
      "epoch 4, batch 327, train_loss 3.34725117683\n",
      "epoch 4, batch 328, train_loss 3.20889401436\n",
      "epoch 4, batch 329, train_loss 2.8305683136\n",
      "epoch 4, batch 330, train_loss 4.25892162323\n",
      "epoch 4, batch 331, train_loss 3.88751149178\n",
      "epoch 4, batch 332, train_loss 2.63932418823\n",
      "epoch 4, batch 333, train_loss 3.84362077713\n",
      "epoch 4, batch 334, train_loss 3.51161146164\n",
      "epoch 4, batch 335, train_loss 3.42726588249\n",
      "epoch 4, batch 336, train_loss 4.03774547577\n",
      "epoch 4, batch 337, train_loss 2.46314907074\n",
      "epoch 4, batch 338, train_loss 3.34971523285\n",
      "epoch 4, batch 339, train_loss 3.12655758858\n",
      "epoch 4, batch 340, train_loss 3.32644534111\n",
      "epoch 4, batch 341, train_loss 3.2303583622\n",
      "epoch 4, batch 342, train_loss 3.29495739937\n",
      "epoch 4, batch 343, train_loss 3.19779586792\n",
      "epoch 4, batch 344, train_loss 4.27671098709\n",
      "epoch 4, batch 345, train_loss 3.6772210598\n",
      "epoch 4, batch 346, train_loss 3.391933918\n",
      "epoch 4, batch 347, train_loss 3.14610028267\n",
      "epoch 4, batch 348, train_loss 3.3784506321\n",
      "epoch 4, batch 349, train_loss 4.60935115814\n",
      "epoch 4, batch 350, train_loss 4.55117034912\n",
      "epoch 4, batch 351, train_loss 4.25536966324\n",
      "epoch 4, batch 352, train_loss 4.3390007019\n",
      "epoch 4, batch 353, train_loss 3.51836371422\n",
      "epoch 4, batch 354, train_loss 2.85523152351\n",
      "epoch 4, batch 355, train_loss 3.17074346542\n",
      "epoch 4, batch 356, train_loss 3.19006633759\n",
      "epoch 4, batch 357, train_loss 3.11247110367\n",
      "epoch 4, batch 358, train_loss 2.61165571213\n",
      "epoch 4, batch 359, train_loss 3.67510294914\n",
      "epoch 4, batch 360, train_loss 3.14643740654\n",
      "epoch 4, batch 361, train_loss 2.76303100586\n",
      "epoch 4, batch 362, train_loss 4.64672374725\n",
      "epoch 4, batch 363, train_loss 3.40163826942\n",
      "epoch 4, batch 364, train_loss 3.29112935066\n",
      "epoch 4, batch 365, train_loss 4.24482488632\n",
      "epoch 4, batch 366, train_loss 4.40618467331\n",
      "epoch 4, batch 367, train_loss 5.18327331543\n",
      "epoch 4, batch 368, train_loss 2.69726991653\n",
      "epoch 4, batch 369, train_loss 3.68863129616\n",
      "epoch 4, batch 370, train_loss 4.09824943542\n",
      "epoch 4, batch 371, train_loss 5.13863706589\n",
      "epoch 4, batch 372, train_loss 2.49560618401\n",
      "epoch 4, batch 373, train_loss 2.68129444122\n",
      "epoch 4, batch 374, train_loss 3.0149409771\n",
      "epoch 4, batch 375, train_loss 3.43402767181\n",
      "epoch 4, batch 376, train_loss 2.65508174896\n",
      "epoch 4, batch 377, train_loss 3.09214401245\n",
      "epoch 4, batch 378, train_loss 3.86506438255\n",
      "epoch 4, batch 379, train_loss 2.97950387001\n",
      "epoch 4, batch 380, train_loss 3.00411939621\n",
      "epoch 4, batch 381, train_loss 3.11105108261\n",
      "epoch 4, batch 382, train_loss 4.09559631348\n",
      "epoch 4, batch 383, train_loss 3.58752131462\n",
      "epoch 4, batch 384, train_loss 3.23485732079\n",
      "epoch 4, batch 385, train_loss 4.74138212204\n",
      "epoch 4, batch 386, train_loss 2.91910743713\n",
      "epoch 4, batch 387, train_loss 4.99283123016\n",
      "epoch 4, batch 388, train_loss 2.91321086884\n",
      "epoch 4, batch 389, train_loss 4.65202379227\n",
      "epoch 4, batch 390, train_loss 4.59295654297\n",
      "epoch 4, batch 391, train_loss 4.62369537354\n",
      "epoch 4, batch 392, train_loss 4.33682823181\n",
      "epoch 4, batch 393, train_loss 3.56031560898\n",
      "epoch 4, batch 394, train_loss 4.2472949028\n",
      "epoch 4, batch 395, train_loss 3.12397933006\n",
      "epoch 4, batch 396, train_loss 4.54128360748\n",
      "epoch 4, batch 397, train_loss 4.68377971649\n",
      "epoch 4, batch 398, train_loss 3.47662234306\n",
      "epoch 4, batch 399, train_loss 3.73157000542\n",
      "epoch 4, batch 400, train_loss 4.80711507797\n",
      "epoch 4, batch 401, train_loss 3.97658276558\n",
      "epoch 4, batch 402, train_loss 3.49346876144\n",
      "epoch 4, batch 403, train_loss 3.45690393448\n",
      "epoch 4, batch 404, train_loss 4.34615421295\n",
      "epoch 4, batch 405, train_loss 3.78431391716\n",
      "epoch 4, batch 406, train_loss 3.61267066002\n",
      "epoch 4, batch 407, train_loss 3.41641616821\n",
      "epoch 4, batch 408, train_loss 3.25963616371\n",
      "epoch 4, batch 409, train_loss 2.71127033234\n",
      "epoch 4, batch 410, train_loss 4.47454404831\n",
      "epoch 4, batch 411, train_loss 3.48232126236\n",
      "epoch 4, batch 412, train_loss 2.99229884148\n",
      "epoch 4, batch 413, train_loss 3.38716197014\n",
      "epoch 4, batch 414, train_loss 3.91079711914\n",
      "epoch 4, batch 415, train_loss 3.97183799744\n",
      "epoch 4, batch 416, train_loss 3.73863101006\n",
      "epoch 4, batch 417, train_loss 3.21866250038\n",
      "epoch 4, batch 418, train_loss 3.65207743645\n",
      "epoch 4, batch 419, train_loss 3.25666737556\n",
      "epoch 4, batch 420, train_loss 2.61070299149\n",
      "epoch 4, batch 421, train_loss 4.60750246048\n",
      "epoch 4, batch 422, train_loss 3.23831582069\n",
      "epoch 4, batch 423, train_loss 3.35490703583\n",
      "epoch 4, batch 424, train_loss 4.53774547577\n",
      "epoch 4, batch 425, train_loss 3.76647830009\n",
      "epoch 4, batch 426, train_loss 5.03805160522\n",
      "epoch 4, batch 427, train_loss 4.08851194382\n",
      "epoch 4, batch 428, train_loss 2.8212890625\n",
      "epoch 4, batch 429, train_loss 3.93055725098\n",
      "epoch 4, batch 430, train_loss 3.99849033356\n",
      "epoch 4, batch 431, train_loss 3.57591223717\n",
      "epoch 4, batch 432, train_loss 3.77712154388\n",
      "epoch 4, batch 433, train_loss 3.12592244148\n",
      "epoch 4, batch 434, train_loss 3.63742017746\n",
      "epoch 4, batch 435, train_loss 3.42030620575\n",
      "epoch 4, batch 436, train_loss 3.81312608719\n",
      "epoch 4, batch 437, train_loss 4.31697940826\n",
      "epoch 4, batch 438, train_loss 3.89906644821\n",
      "epoch 4, batch 439, train_loss 3.65306138992\n",
      "epoch 4, batch 440, train_loss 5.07996368408\n",
      "epoch 4, batch 441, train_loss 5.22711658478\n",
      "epoch 4, batch 442, train_loss 4.2800693512\n",
      "epoch 4, batch 443, train_loss 2.64721560478\n",
      "epoch 4, batch 444, train_loss 3.17123866081\n",
      "epoch 4, batch 445, train_loss 4.86246967316\n",
      "epoch 4, batch 446, train_loss 3.54678106308\n",
      "epoch 4, batch 447, train_loss 3.2379450798\n",
      "epoch 4, batch 448, train_loss 4.75138139725\n",
      "epoch 4, batch 449, train_loss 4.04950046539\n",
      "epoch 4, batch 450, train_loss 3.3864171505\n",
      "epoch 4, batch 451, train_loss 3.16132283211\n",
      "epoch 4, batch 452, train_loss 2.87128162384\n",
      "epoch 4, batch 453, train_loss 5.47222518921\n",
      "epoch 4, batch 454, train_loss 5.42024707794\n",
      "epoch 4, batch 455, train_loss 2.72181892395\n",
      "epoch 4, batch 456, train_loss 3.4901676178\n",
      "epoch 4, batch 457, train_loss 3.77467083931\n",
      "epoch 4, batch 458, train_loss 3.32075619698\n",
      "epoch 4, batch 459, train_loss 3.66236996651\n",
      "epoch 4, batch 460, train_loss 4.23039722443\n",
      "epoch 4, batch 461, train_loss 3.65225458145\n",
      "epoch 4, batch 462, train_loss 3.31668376923\n",
      "epoch 4, batch 463, train_loss 3.47921991348\n",
      "epoch 4, batch 464, train_loss 3.78722834587\n",
      "epoch 4, batch 465, train_loss 3.11994338036\n",
      "epoch 4, batch 466, train_loss 3.96042513847\n",
      "epoch 4, batch 467, train_loss 3.74667072296\n",
      "epoch 4, batch 468, train_loss 3.90092682838\n",
      "epoch 4, batch 469, train_loss 3.42787337303\n",
      "epoch 4, batch 470, train_loss 3.29859423637\n",
      "epoch 4, batch 471, train_loss 3.36546087265\n",
      "epoch 4, batch 472, train_loss 3.22621560097\n",
      "epoch 4, batch 473, train_loss 3.98922133446\n",
      "epoch 4, batch 474, train_loss 4.62557220459\n",
      "epoch 4, batch 475, train_loss 3.20003509521\n",
      "epoch 4, batch 476, train_loss 2.70751929283\n",
      "epoch 4, batch 477, train_loss 2.83947968483\n",
      "epoch 4, batch 478, train_loss 2.66650605202\n",
      "epoch 4, batch 479, train_loss 3.20860671997\n",
      "epoch 4, batch 480, train_loss 3.43841075897\n",
      "epoch 4, batch 481, train_loss 5.14219903946\n",
      "epoch 4, batch 482, train_loss 3.10589647293\n",
      "epoch 4, batch 483, train_loss 3.79437661171\n",
      "epoch 4, batch 484, train_loss 3.42619943619\n",
      "epoch 4, batch 485, train_loss 2.59569692612\n",
      "epoch 4, batch 486, train_loss 2.59704422951\n",
      "epoch 4, batch 487, train_loss 2.74538445473\n",
      "epoch 4, batch 488, train_loss 3.17851924896\n",
      "epoch 4, batch 489, train_loss 3.28785777092\n",
      "epoch 4, batch 490, train_loss 3.2304494381\n",
      "epoch 4, batch 491, train_loss 3.14587521553\n",
      "epoch 4, batch 492, train_loss 3.47787189484\n",
      "epoch 4, batch 493, train_loss 3.0218269825\n",
      "epoch 4, batch 494, train_loss 3.14294958115\n",
      "epoch 4, batch 495, train_loss 3.20947337151\n",
      "epoch 4, batch 496, train_loss 2.82269620895\n",
      "epoch 4, batch 497, train_loss 3.01603579521\n",
      "epoch 4, batch 498, train_loss 3.17112159729\n",
      "epoch 4, batch 499, train_loss 3.12005925179\n",
      "epoch 4, batch 500, train_loss 3.65041875839\n",
      "epoch 4, batch 501, train_loss 3.23711800575\n",
      "epoch 4, batch 502, train_loss 4.65596294403\n",
      "epoch 4, batch 503, train_loss 4.61098098755\n",
      "epoch 4, batch 504, train_loss 3.99871182442\n",
      "epoch 4, batch 505, train_loss 3.77288651466\n",
      "epoch 4, batch 506, train_loss 3.43374466896\n",
      "epoch 4, batch 507, train_loss 4.50183820724\n",
      "epoch 4, batch 508, train_loss 3.19551157951\n",
      "epoch 4, batch 509, train_loss 4.65327596664\n",
      "epoch 4, batch 510, train_loss 4.63782787323\n",
      "epoch 4, batch 511, train_loss 4.66141796112\n",
      "epoch 4, batch 512, train_loss 3.59699344635\n",
      "epoch 4, batch 513, train_loss 5.09207773209\n",
      "epoch 4, batch 514, train_loss 5.01692295074\n",
      "epoch 4, batch 515, train_loss 4.9353351593\n",
      "epoch 4, batch 516, train_loss 3.07464456558\n",
      "epoch 4, batch 517, train_loss 3.24546980858\n",
      "epoch 4, batch 518, train_loss 3.14853930473\n",
      "epoch 4, batch 519, train_loss 2.83855080605\n",
      "epoch 4, batch 520, train_loss 4.07806968689\n",
      "epoch 4, batch 521, train_loss 3.18116188049\n",
      "epoch 4, batch 522, train_loss 2.73013782501\n",
      "epoch 4, batch 523, train_loss 2.75904917717\n",
      "epoch 4, batch 524, train_loss 2.60436964035\n",
      "epoch 4, batch 525, train_loss 2.43618822098\n",
      "epoch 4, batch 526, train_loss 2.52730464935\n",
      "epoch 4, batch 527, train_loss 2.54229903221\n",
      "epoch 4, batch 528, train_loss 2.80263519287\n",
      "epoch 4, batch 529, train_loss 2.68498682976\n",
      "epoch 4, batch 530, train_loss 2.15181708336\n",
      "epoch 4, batch 531, train_loss 1.84237086773\n",
      "epoch 4, batch 532, train_loss 2.8389081955\n",
      "epoch 4, batch 533, train_loss 1.77502906322\n",
      "epoch 4, batch 534, train_loss 1.83103060722\n",
      "epoch 4, batch 535, train_loss 3.38888239861\n",
      "epoch 4, batch 536, train_loss 3.6246213913\n",
      "epoch 4, batch 537, train_loss 3.50099563599\n",
      "epoch 4, batch 538, train_loss 3.69849538803\n",
      "epoch 4, batch 539, train_loss 4.03843021393\n",
      "epoch 4, batch 540, train_loss 4.36467981339\n",
      "epoch 5, batch 0, train_loss 3.57152080536\n",
      "epoch 5, batch 1, train_loss 4.01200723648\n",
      "epoch 5, batch 2, train_loss 3.38357830048\n",
      "epoch 5, batch 3, train_loss 3.59666275978\n",
      "epoch 5, batch 4, train_loss 3.44737529755\n",
      "epoch 5, batch 5, train_loss 3.50940728188\n",
      "epoch 5, batch 6, train_loss 4.64772605896\n",
      "epoch 5, batch 7, train_loss 3.446808815\n",
      "epoch 5, batch 8, train_loss 3.05607843399\n",
      "epoch 5, batch 9, train_loss 4.1542339325\n",
      "epoch 5, batch 10, train_loss 3.2674036026\n",
      "epoch 5, batch 11, train_loss 3.37553095818\n",
      "epoch 5, batch 12, train_loss 3.07686090469\n",
      "epoch 5, batch 13, train_loss 3.62136793137\n",
      "epoch 5, batch 14, train_loss 2.96512269974\n",
      "epoch 5, batch 15, train_loss 3.72573709488\n",
      "epoch 5, batch 16, train_loss 3.24639463425\n",
      "epoch 5, batch 17, train_loss 3.1000931263\n",
      "epoch 5, batch 18, train_loss 2.70407056808\n",
      "epoch 5, batch 19, train_loss 2.24203252792\n",
      "epoch 5, batch 20, train_loss 3.14664006233\n",
      "epoch 5, batch 21, train_loss 3.07389998436\n",
      "epoch 5, batch 22, train_loss 3.60064697266\n",
      "epoch 5, batch 23, train_loss 3.11200833321\n",
      "epoch 5, batch 24, train_loss 3.10151791573\n",
      "epoch 5, batch 25, train_loss 3.59415245056\n",
      "epoch 5, batch 26, train_loss 3.51257157326\n",
      "epoch 5, batch 27, train_loss 3.41886782646\n",
      "epoch 5, batch 28, train_loss 3.73896169662\n",
      "epoch 5, batch 29, train_loss 3.65065789223\n",
      "epoch 5, batch 30, train_loss 3.73907589912\n",
      "epoch 5, batch 31, train_loss 3.14755749702\n",
      "epoch 5, batch 32, train_loss 2.87626481056\n",
      "epoch 5, batch 33, train_loss 4.89025163651\n",
      "epoch 5, batch 34, train_loss 4.95819664001\n",
      "epoch 5, batch 35, train_loss 3.49387907982\n",
      "epoch 5, batch 36, train_loss 3.52456092834\n",
      "epoch 5, batch 37, train_loss 3.48793363571\n",
      "epoch 5, batch 38, train_loss 3.59372448921\n",
      "epoch 5, batch 39, train_loss 4.09095430374\n",
      "epoch 5, batch 40, train_loss 3.15397715569\n",
      "epoch 5, batch 41, train_loss 3.4210896492\n",
      "epoch 5, batch 42, train_loss 3.46645116806\n",
      "epoch 5, batch 43, train_loss 3.23519062996\n",
      "epoch 5, batch 44, train_loss 3.86922025681\n",
      "epoch 5, batch 45, train_loss 3.47344398499\n",
      "epoch 5, batch 46, train_loss 3.4221508503\n",
      "epoch 5, batch 47, train_loss 3.55315089226\n",
      "epoch 5, batch 48, train_loss 3.3725206852\n",
      "epoch 5, batch 49, train_loss 3.52474808693\n",
      "epoch 5, batch 50, train_loss 3.39473986626\n",
      "epoch 5, batch 51, train_loss 3.06665635109\n",
      "epoch 5, batch 52, train_loss 3.54225993156\n",
      "epoch 5, batch 53, train_loss 3.40821075439\n",
      "epoch 5, batch 54, train_loss 3.1287047863\n",
      "epoch 5, batch 55, train_loss 3.16824102402\n",
      "epoch 5, batch 56, train_loss 3.05727934837\n",
      "epoch 5, batch 57, train_loss 3.58527112007\n",
      "epoch 5, batch 58, train_loss 3.37726950645\n",
      "epoch 5, batch 59, train_loss 3.34169268608\n",
      "epoch 5, batch 60, train_loss 3.54662752151\n",
      "epoch 5, batch 61, train_loss 2.0648047924\n",
      "epoch 5, batch 62, train_loss 3.10497760773\n",
      "epoch 5, batch 63, train_loss 3.24900650978\n",
      "epoch 5, batch 64, train_loss 3.46365022659\n",
      "epoch 5, batch 65, train_loss 3.80984330177\n",
      "epoch 5, batch 66, train_loss 3.57754445076\n",
      "epoch 5, batch 67, train_loss 2.30111479759\n",
      "epoch 5, batch 68, train_loss 3.27715992928\n",
      "epoch 5, batch 69, train_loss 2.56876373291\n",
      "epoch 5, batch 70, train_loss 4.29371929169\n",
      "epoch 5, batch 71, train_loss 4.32352972031\n",
      "epoch 5, batch 72, train_loss 3.04420924187\n",
      "epoch 5, batch 73, train_loss 3.49518203735\n",
      "epoch 5, batch 74, train_loss 4.14336299896\n",
      "epoch 5, batch 75, train_loss 3.55428123474\n",
      "epoch 5, batch 76, train_loss 3.82761907578\n",
      "epoch 5, batch 77, train_loss 4.60594558716\n",
      "epoch 5, batch 78, train_loss 3.34509515762\n",
      "epoch 5, batch 79, train_loss 3.36781454086\n",
      "epoch 5, batch 80, train_loss 3.17846250534\n",
      "epoch 5, batch 81, train_loss 3.30092716217\n",
      "epoch 5, batch 82, train_loss 3.20246076584\n",
      "epoch 5, batch 83, train_loss 3.31717443466\n",
      "epoch 5, batch 84, train_loss 3.07791852951\n",
      "epoch 5, batch 85, train_loss 3.12071704865\n",
      "epoch 5, batch 86, train_loss 3.63106060028\n",
      "epoch 5, batch 87, train_loss 2.814650774\n",
      "epoch 5, batch 88, train_loss 3.11765480042\n",
      "epoch 5, batch 89, train_loss 3.43269371986\n",
      "epoch 5, batch 90, train_loss 3.10969519615\n",
      "epoch 5, batch 91, train_loss 3.18871831894\n",
      "epoch 5, batch 92, train_loss 3.6987349987\n",
      "epoch 5, batch 93, train_loss 4.49485492706\n",
      "epoch 5, batch 94, train_loss 3.19288802147\n",
      "epoch 5, batch 95, train_loss 3.272772789\n",
      "epoch 5, batch 96, train_loss 2.58906364441\n",
      "epoch 5, batch 97, train_loss 3.26639866829\n",
      "epoch 5, batch 98, train_loss 3.19052958488\n",
      "epoch 5, batch 99, train_loss 3.46876168251\n",
      "epoch 5, batch 100, train_loss 3.21000623703\n",
      "epoch 5, batch 101, train_loss 3.40256524086\n",
      "epoch 5, batch 102, train_loss 3.19950342178\n",
      "epoch 5, batch 103, train_loss 4.49385976791\n",
      "epoch 5, batch 104, train_loss 3.7051551342\n",
      "epoch 5, batch 105, train_loss 3.50707173347\n",
      "epoch 5, batch 106, train_loss 3.928992033\n",
      "epoch 5, batch 107, train_loss 3.74120235443\n",
      "epoch 5, batch 108, train_loss 3.34948801994\n",
      "epoch 5, batch 109, train_loss 3.60484266281\n",
      "epoch 5, batch 110, train_loss 3.41167521477\n",
      "epoch 5, batch 111, train_loss 3.82217645645\n",
      "epoch 5, batch 112, train_loss 3.79531240463\n",
      "epoch 5, batch 113, train_loss 3.18857097626\n",
      "epoch 5, batch 114, train_loss 3.7691552639\n",
      "epoch 5, batch 115, train_loss 4.60289621353\n",
      "epoch 5, batch 116, train_loss 4.49626922607\n",
      "epoch 5, batch 117, train_loss 4.61025762558\n",
      "epoch 5, batch 118, train_loss 3.99468898773\n",
      "epoch 5, batch 119, train_loss 2.62507343292\n",
      "epoch 5, batch 120, train_loss 3.49606084824\n",
      "epoch 5, batch 121, train_loss 3.35183119774\n",
      "epoch 5, batch 122, train_loss 3.57107138634\n",
      "epoch 5, batch 123, train_loss 3.32109165192\n",
      "epoch 5, batch 124, train_loss 3.01351857185\n",
      "epoch 5, batch 125, train_loss 2.96340298653\n",
      "epoch 5, batch 126, train_loss 2.93973112106\n",
      "epoch 5, batch 127, train_loss 3.02581095695\n",
      "epoch 5, batch 128, train_loss 2.89516472816\n",
      "epoch 5, batch 129, train_loss 3.28331136703\n",
      "epoch 5, batch 130, train_loss 3.20555591583\n",
      "epoch 5, batch 131, train_loss 3.57068943977\n",
      "epoch 5, batch 132, train_loss 3.29968190193\n",
      "epoch 5, batch 133, train_loss 2.48728108406\n",
      "epoch 5, batch 134, train_loss 2.54464435577\n",
      "epoch 5, batch 135, train_loss 4.45508956909\n",
      "epoch 5, batch 136, train_loss 3.78552007675\n",
      "epoch 5, batch 137, train_loss 2.92501449585\n",
      "epoch 5, batch 138, train_loss 2.78087925911\n",
      "epoch 5, batch 139, train_loss 3.30809855461\n",
      "epoch 5, batch 140, train_loss 2.76631689072\n",
      "epoch 5, batch 141, train_loss 3.12774538994\n",
      "epoch 5, batch 142, train_loss 4.45872116089\n",
      "epoch 5, batch 143, train_loss 2.79964709282\n",
      "epoch 5, batch 144, train_loss 2.25195550919\n",
      "epoch 5, batch 145, train_loss 3.31500029564\n",
      "epoch 5, batch 146, train_loss 3.49130630493\n",
      "epoch 5, batch 147, train_loss 3.22161030769\n",
      "epoch 5, batch 148, train_loss 3.09745621681\n",
      "epoch 5, batch 149, train_loss 3.58688282967\n",
      "epoch 5, batch 150, train_loss 3.67984509468\n",
      "epoch 5, batch 151, train_loss 2.25102901459\n",
      "epoch 5, batch 152, train_loss 2.53603720665\n",
      "epoch 5, batch 153, train_loss 4.33903074265\n",
      "epoch 5, batch 154, train_loss 3.40972065926\n",
      "epoch 5, batch 155, train_loss 2.97443366051\n",
      "epoch 5, batch 156, train_loss 3.19248437881\n",
      "epoch 5, batch 157, train_loss 2.73223948479\n",
      "epoch 5, batch 158, train_loss 2.77640104294\n",
      "epoch 5, batch 159, train_loss 2.97097945213\n",
      "epoch 5, batch 160, train_loss 3.1019411087\n",
      "epoch 5, batch 161, train_loss 3.64869904518\n",
      "epoch 5, batch 162, train_loss 3.58125805855\n",
      "epoch 5, batch 163, train_loss 5.136490345\n",
      "epoch 5, batch 164, train_loss 2.48183965683\n",
      "epoch 5, batch 165, train_loss 2.43033719063\n",
      "epoch 5, batch 166, train_loss 2.82129645348\n",
      "epoch 5, batch 167, train_loss 2.83321332932\n",
      "epoch 5, batch 168, train_loss 3.08734083176\n",
      "epoch 5, batch 169, train_loss 2.83885765076\n",
      "epoch 5, batch 170, train_loss 3.40837526321\n",
      "epoch 5, batch 171, train_loss 2.81030535698\n",
      "epoch 5, batch 172, train_loss 3.45845866203\n",
      "epoch 5, batch 173, train_loss 2.29449367523\n",
      "epoch 5, batch 174, train_loss 3.95254063606\n",
      "epoch 5, batch 175, train_loss 3.38204073906\n",
      "epoch 5, batch 176, train_loss 3.26293301582\n",
      "epoch 5, batch 177, train_loss 3.06034517288\n",
      "epoch 5, batch 178, train_loss 2.97391796112\n",
      "epoch 5, batch 179, train_loss 3.2315928936\n",
      "epoch 5, batch 180, train_loss 3.1168513298\n",
      "epoch 5, batch 181, train_loss 3.80128145218\n",
      "epoch 5, batch 182, train_loss 3.23230028152\n",
      "epoch 5, batch 183, train_loss 3.91531181335\n",
      "epoch 5, batch 184, train_loss 2.5443148613\n",
      "epoch 5, batch 185, train_loss 2.9257376194\n",
      "epoch 5, batch 186, train_loss 3.63784122467\n",
      "epoch 5, batch 187, train_loss 2.65344452858\n",
      "epoch 5, batch 188, train_loss 2.98597025871\n",
      "epoch 5, batch 189, train_loss 2.96931862831\n",
      "epoch 5, batch 190, train_loss 3.77883696556\n",
      "epoch 5, batch 191, train_loss 3.65302681923\n",
      "epoch 5, batch 192, train_loss 4.67809343338\n",
      "epoch 5, batch 193, train_loss 4.36776351929\n",
      "epoch 5, batch 194, train_loss 5.01008605957\n",
      "epoch 5, batch 195, train_loss 5.09776258469\n",
      "epoch 5, batch 196, train_loss 3.40931963921\n",
      "epoch 5, batch 197, train_loss 2.72805571556\n",
      "epoch 5, batch 198, train_loss 2.48792457581\n",
      "epoch 5, batch 199, train_loss 3.59779644012\n",
      "epoch 5, batch 200, train_loss 2.99472427368\n",
      "epoch 5, batch 201, train_loss 3.3417263031\n",
      "epoch 5, batch 202, train_loss 3.77947258949\n",
      "epoch 5, batch 203, train_loss 3.91682100296\n",
      "epoch 5, batch 204, train_loss 3.87615895271\n",
      "epoch 5, batch 205, train_loss 3.37687134743\n",
      "epoch 5, batch 206, train_loss 4.50681018829\n",
      "epoch 5, batch 207, train_loss 3.73615646362\n",
      "epoch 5, batch 208, train_loss 4.49147367477\n",
      "epoch 5, batch 209, train_loss 4.22944450378\n",
      "epoch 5, batch 210, train_loss 2.31219410896\n",
      "epoch 5, batch 211, train_loss 3.13427066803\n",
      "epoch 5, batch 212, train_loss 4.08309602737\n",
      "epoch 5, batch 213, train_loss 3.38183999062\n",
      "epoch 5, batch 214, train_loss 3.2145011425\n",
      "epoch 5, batch 215, train_loss 2.38808274269\n",
      "epoch 5, batch 216, train_loss 3.62435936928\n",
      "epoch 5, batch 217, train_loss 3.01072740555\n",
      "epoch 5, batch 218, train_loss 3.51245570183\n",
      "epoch 5, batch 219, train_loss 3.64206671715\n",
      "epoch 5, batch 220, train_loss 3.67834234238\n",
      "epoch 5, batch 221, train_loss 3.85647821426\n",
      "epoch 5, batch 222, train_loss 2.49293684959\n",
      "epoch 5, batch 223, train_loss 2.50005793571\n",
      "epoch 5, batch 224, train_loss 2.85515975952\n",
      "epoch 5, batch 225, train_loss 2.99594831467\n",
      "epoch 5, batch 226, train_loss 3.57330656052\n",
      "epoch 5, batch 227, train_loss 3.29562783241\n",
      "epoch 5, batch 228, train_loss 3.47185754776\n",
      "epoch 5, batch 229, train_loss 3.28764653206\n",
      "epoch 5, batch 230, train_loss 3.13581323624\n",
      "epoch 5, batch 231, train_loss 2.83184814453\n",
      "epoch 5, batch 232, train_loss 3.51064229012\n",
      "epoch 5, batch 233, train_loss 3.78005456924\n",
      "epoch 5, batch 234, train_loss 3.30361866951\n",
      "epoch 5, batch 235, train_loss 3.92571616173\n",
      "epoch 5, batch 236, train_loss 3.39536118507\n",
      "epoch 5, batch 237, train_loss 3.31538796425\n",
      "epoch 5, batch 238, train_loss 3.32708978653\n",
      "epoch 5, batch 239, train_loss 3.12905359268\n",
      "epoch 5, batch 240, train_loss 3.60715866089\n",
      "epoch 5, batch 241, train_loss 3.53515839577\n",
      "epoch 5, batch 242, train_loss 3.35187005997\n",
      "epoch 5, batch 243, train_loss 3.55079126358\n",
      "epoch 5, batch 244, train_loss 3.67403030396\n",
      "epoch 5, batch 245, train_loss 3.42701649666\n",
      "epoch 5, batch 246, train_loss 3.35348916054\n",
      "epoch 5, batch 247, train_loss 3.55110239983\n",
      "epoch 5, batch 248, train_loss 3.28642010689\n",
      "epoch 5, batch 249, train_loss 4.03601455688\n",
      "epoch 5, batch 250, train_loss 3.81573987007\n",
      "epoch 5, batch 251, train_loss 4.18600559235\n",
      "epoch 5, batch 252, train_loss 2.86498332024\n",
      "epoch 5, batch 253, train_loss 2.36364912987\n",
      "epoch 5, batch 254, train_loss 3.06431221962\n",
      "epoch 5, batch 255, train_loss 2.36465907097\n",
      "epoch 5, batch 256, train_loss 2.76924204826\n",
      "epoch 5, batch 257, train_loss 3.2676217556\n",
      "epoch 5, batch 258, train_loss 2.60543489456\n",
      "epoch 5, batch 259, train_loss 2.53626251221\n",
      "epoch 5, batch 260, train_loss 2.25052666664\n",
      "epoch 5, batch 261, train_loss 3.72414159775\n",
      "epoch 5, batch 262, train_loss 4.3528213501\n",
      "epoch 5, batch 263, train_loss 3.36397457123\n",
      "epoch 5, batch 264, train_loss 3.05522418022\n",
      "epoch 5, batch 265, train_loss 3.76219344139\n",
      "epoch 5, batch 266, train_loss 3.81365513802\n",
      "epoch 5, batch 267, train_loss 3.00213575363\n",
      "epoch 5, batch 268, train_loss 2.51474738121\n",
      "epoch 5, batch 269, train_loss 2.5860888958\n",
      "epoch 5, batch 270, train_loss 2.90753722191\n",
      "epoch 5, batch 271, train_loss 3.27524638176\n",
      "epoch 5, batch 272, train_loss 3.29810929298\n",
      "epoch 5, batch 273, train_loss 3.4339594841\n",
      "epoch 5, batch 274, train_loss 3.20272707939\n",
      "epoch 5, batch 275, train_loss 3.23320174217\n",
      "epoch 5, batch 276, train_loss 2.98211431503\n",
      "epoch 5, batch 277, train_loss 3.85983920097\n",
      "epoch 5, batch 278, train_loss 2.9792509079\n",
      "epoch 5, batch 279, train_loss 3.56159901619\n",
      "epoch 5, batch 280, train_loss 3.54305195808\n",
      "epoch 5, batch 281, train_loss 3.30407857895\n",
      "epoch 5, batch 282, train_loss 2.92121434212\n",
      "epoch 5, batch 283, train_loss 4.66393041611\n",
      "epoch 5, batch 284, train_loss 3.48215508461\n",
      "epoch 5, batch 285, train_loss 2.13461089134\n",
      "epoch 5, batch 286, train_loss 2.38552451134\n",
      "epoch 5, batch 287, train_loss 2.49266052246\n",
      "epoch 5, batch 288, train_loss 3.17674541473\n",
      "epoch 5, batch 289, train_loss 3.23728632927\n",
      "epoch 5, batch 290, train_loss 3.10135531425\n",
      "epoch 5, batch 291, train_loss 2.93839764595\n",
      "epoch 5, batch 292, train_loss 2.40174818039\n",
      "epoch 5, batch 293, train_loss 3.5916030407\n",
      "epoch 5, batch 294, train_loss 3.10593748093\n",
      "epoch 5, batch 295, train_loss 2.81046128273\n",
      "epoch 5, batch 296, train_loss 3.12662434578\n",
      "epoch 5, batch 297, train_loss 2.76516151428\n",
      "epoch 5, batch 298, train_loss 3.04816198349\n",
      "epoch 5, batch 299, train_loss 3.68073654175\n",
      "epoch 5, batch 300, train_loss 3.35917544365\n",
      "epoch 5, batch 301, train_loss 4.44049835205\n",
      "epoch 5, batch 302, train_loss 4.4479970932\n",
      "epoch 5, batch 303, train_loss 4.07300615311\n",
      "epoch 5, batch 304, train_loss 4.8049030304\n",
      "epoch 5, batch 305, train_loss 4.74879693985\n",
      "epoch 5, batch 306, train_loss 3.30932402611\n",
      "epoch 5, batch 307, train_loss 2.68753433228\n",
      "epoch 5, batch 308, train_loss 3.15668821335\n",
      "epoch 5, batch 309, train_loss 3.32459354401\n",
      "epoch 5, batch 310, train_loss 3.10453009605\n",
      "epoch 5, batch 311, train_loss 3.23847007751\n",
      "epoch 5, batch 312, train_loss 3.43077468872\n",
      "epoch 5, batch 313, train_loss 3.11640763283\n",
      "epoch 5, batch 314, train_loss 3.50412321091\n",
      "epoch 5, batch 315, train_loss 2.93548560143\n",
      "epoch 5, batch 316, train_loss 3.09910869598\n",
      "epoch 5, batch 317, train_loss 3.61028885841\n",
      "epoch 5, batch 318, train_loss 2.6850028038\n",
      "epoch 5, batch 319, train_loss 4.56223535538\n",
      "epoch 5, batch 320, train_loss 3.35364890099\n",
      "epoch 5, batch 321, train_loss 4.67687797546\n",
      "epoch 5, batch 322, train_loss 2.58557343483\n",
      "epoch 5, batch 323, train_loss 2.43673753738\n",
      "epoch 5, batch 324, train_loss 3.67188858986\n",
      "epoch 5, batch 325, train_loss 3.27035045624\n",
      "epoch 5, batch 326, train_loss 3.2777929306\n",
      "epoch 5, batch 327, train_loss 3.26357316971\n",
      "epoch 5, batch 328, train_loss 3.1360270977\n",
      "epoch 5, batch 329, train_loss 2.7691245079\n",
      "epoch 5, batch 330, train_loss 4.17319631577\n",
      "epoch 5, batch 331, train_loss 3.81346797943\n",
      "epoch 5, batch 332, train_loss 2.58882260323\n",
      "epoch 5, batch 333, train_loss 3.75076723099\n",
      "epoch 5, batch 334, train_loss 3.43319988251\n",
      "epoch 5, batch 335, train_loss 3.34070658684\n",
      "epoch 5, batch 336, train_loss 3.94256520271\n",
      "epoch 5, batch 337, train_loss 2.40261793137\n",
      "epoch 5, batch 338, train_loss 3.27304077148\n",
      "epoch 5, batch 339, train_loss 3.05537128448\n",
      "epoch 5, batch 340, train_loss 3.25369334221\n",
      "epoch 5, batch 341, train_loss 3.16868019104\n",
      "epoch 5, batch 342, train_loss 3.2288081646\n",
      "epoch 5, batch 343, train_loss 3.12733769417\n",
      "epoch 5, batch 344, train_loss 4.17642784119\n",
      "epoch 5, batch 345, train_loss 3.57194399834\n",
      "epoch 5, batch 346, train_loss 3.2965323925\n",
      "epoch 5, batch 347, train_loss 3.07072401047\n",
      "epoch 5, batch 348, train_loss 3.31037116051\n",
      "epoch 5, batch 349, train_loss 4.47103404999\n",
      "epoch 5, batch 350, train_loss 4.42352151871\n",
      "epoch 5, batch 351, train_loss 4.15978622437\n",
      "epoch 5, batch 352, train_loss 4.23335361481\n",
      "epoch 5, batch 353, train_loss 3.43420410156\n",
      "epoch 5, batch 354, train_loss 2.79039335251\n",
      "epoch 5, batch 355, train_loss 3.09841680527\n",
      "epoch 5, batch 356, train_loss 3.09977579117\n",
      "epoch 5, batch 357, train_loss 3.02749156952\n",
      "epoch 5, batch 358, train_loss 2.54918694496\n",
      "epoch 5, batch 359, train_loss 3.59068083763\n",
      "epoch 5, batch 360, train_loss 3.0667757988\n",
      "epoch 5, batch 361, train_loss 2.69809317589\n",
      "epoch 5, batch 362, train_loss 4.55060386658\n",
      "epoch 5, batch 363, train_loss 3.33047986031\n",
      "epoch 5, batch 364, train_loss 3.22116279602\n",
      "epoch 5, batch 365, train_loss 4.17160177231\n",
      "epoch 5, batch 366, train_loss 4.31358957291\n",
      "epoch 5, batch 367, train_loss 5.08029508591\n",
      "epoch 5, batch 368, train_loss 2.64392232895\n",
      "epoch 5, batch 369, train_loss 3.62000012398\n",
      "epoch 5, batch 370, train_loss 4.02082109451\n",
      "epoch 5, batch 371, train_loss 5.04188299179\n",
      "epoch 5, batch 372, train_loss 2.44103312492\n",
      "epoch 5, batch 373, train_loss 2.62332630157\n",
      "epoch 5, batch 374, train_loss 2.95370936394\n",
      "epoch 5, batch 375, train_loss 3.36640691757\n",
      "epoch 5, batch 376, train_loss 2.59977626801\n",
      "epoch 5, batch 377, train_loss 3.02574634552\n",
      "epoch 5, batch 378, train_loss 3.76950240135\n",
      "epoch 5, batch 379, train_loss 2.90937566757\n",
      "epoch 5, batch 380, train_loss 2.93166399002\n",
      "epoch 5, batch 381, train_loss 3.03688883781\n",
      "epoch 5, batch 382, train_loss 3.99699950218\n",
      "epoch 5, batch 383, train_loss 3.50472307205\n",
      "epoch 5, batch 384, train_loss 3.15237808228\n",
      "epoch 5, batch 385, train_loss 4.62997150421\n",
      "epoch 5, batch 386, train_loss 2.85085678101\n",
      "epoch 5, batch 387, train_loss 4.86947441101\n",
      "epoch 5, batch 388, train_loss 2.84857559204\n",
      "epoch 5, batch 389, train_loss 4.53095436096\n",
      "epoch 5, batch 390, train_loss 4.48888301849\n",
      "epoch 5, batch 391, train_loss 4.52127933502\n",
      "epoch 5, batch 392, train_loss 4.25112104416\n",
      "epoch 5, batch 393, train_loss 3.47671937943\n",
      "epoch 5, batch 394, train_loss 4.14802026749\n",
      "epoch 5, batch 395, train_loss 3.04486680031\n",
      "epoch 5, batch 396, train_loss 4.42655658722\n",
      "epoch 5, batch 397, train_loss 4.5531578064\n",
      "epoch 5, batch 398, train_loss 3.3933634758\n",
      "epoch 5, batch 399, train_loss 3.64623856544\n",
      "epoch 5, batch 400, train_loss 4.69481420517\n",
      "epoch 5, batch 401, train_loss 3.87893295288\n",
      "epoch 5, batch 402, train_loss 3.41270446777\n",
      "epoch 5, batch 403, train_loss 3.36703157425\n",
      "epoch 5, batch 404, train_loss 4.24462747574\n",
      "epoch 5, batch 405, train_loss 3.70200824738\n",
      "epoch 5, batch 406, train_loss 3.53828477859\n",
      "epoch 5, batch 407, train_loss 3.3404083252\n",
      "epoch 5, batch 408, train_loss 3.18876409531\n",
      "epoch 5, batch 409, train_loss 2.65419697762\n",
      "epoch 5, batch 410, train_loss 4.36853504181\n",
      "epoch 5, batch 411, train_loss 3.41125607491\n",
      "epoch 5, batch 412, train_loss 2.91552734375\n",
      "epoch 5, batch 413, train_loss 3.29297161102\n",
      "epoch 5, batch 414, train_loss 3.83172607422\n",
      "epoch 5, batch 415, train_loss 3.88693404198\n",
      "epoch 5, batch 416, train_loss 3.64820361137\n",
      "epoch 5, batch 417, train_loss 3.14298796654\n",
      "epoch 5, batch 418, train_loss 3.57032990456\n",
      "epoch 5, batch 419, train_loss 3.18485236168\n",
      "epoch 5, batch 420, train_loss 2.5618956089\n",
      "epoch 5, batch 421, train_loss 4.50326299667\n",
      "epoch 5, batch 422, train_loss 3.16124343872\n",
      "epoch 5, batch 423, train_loss 3.28643131256\n",
      "epoch 5, batch 424, train_loss 4.43614625931\n",
      "epoch 5, batch 425, train_loss 3.68848323822\n",
      "epoch 5, batch 426, train_loss 4.93245792389\n",
      "epoch 5, batch 427, train_loss 3.99693346024\n",
      "epoch 5, batch 428, train_loss 2.75260162354\n",
      "epoch 5, batch 429, train_loss 3.83008170128\n",
      "epoch 5, batch 430, train_loss 3.89813137054\n",
      "epoch 5, batch 431, train_loss 3.48124670982\n",
      "epoch 5, batch 432, train_loss 3.67694020271\n",
      "epoch 5, batch 433, train_loss 3.04375815392\n",
      "epoch 5, batch 434, train_loss 3.56191515923\n",
      "epoch 5, batch 435, train_loss 3.34818148613\n",
      "epoch 5, batch 436, train_loss 3.72829818726\n",
      "epoch 5, batch 437, train_loss 4.21403741837\n",
      "epoch 5, batch 438, train_loss 3.81437063217\n",
      "epoch 5, batch 439, train_loss 3.57599449158\n",
      "epoch 5, batch 440, train_loss 4.96382856369\n",
      "epoch 5, batch 441, train_loss 5.10872983932\n",
      "epoch 5, batch 442, train_loss 4.19164562225\n",
      "epoch 5, batch 443, train_loss 2.58677363396\n",
      "epoch 5, batch 444, train_loss 3.11267900467\n",
      "epoch 5, batch 445, train_loss 4.779712677\n",
      "epoch 5, batch 446, train_loss 3.4793586731\n",
      "epoch 5, batch 447, train_loss 3.1728758812\n",
      "epoch 5, batch 448, train_loss 4.66403007507\n",
      "epoch 5, batch 449, train_loss 3.98110961914\n",
      "epoch 5, batch 450, train_loss 3.31642794609\n",
      "epoch 5, batch 451, train_loss 3.08450484276\n",
      "epoch 5, batch 452, train_loss 2.80795645714\n",
      "epoch 5, batch 453, train_loss 5.38264131546\n",
      "epoch 5, batch 454, train_loss 5.29513502121\n",
      "epoch 5, batch 455, train_loss 2.66865682602\n",
      "epoch 5, batch 456, train_loss 3.40902352333\n",
      "epoch 5, batch 457, train_loss 3.67827177048\n",
      "epoch 5, batch 458, train_loss 3.2549302578\n",
      "epoch 5, batch 459, train_loss 3.57140636444\n",
      "epoch 5, batch 460, train_loss 4.11567544937\n",
      "epoch 5, batch 461, train_loss 3.56618070602\n",
      "epoch 5, batch 462, train_loss 3.227414608\n",
      "epoch 5, batch 463, train_loss 3.38954544067\n",
      "epoch 5, batch 464, train_loss 3.68931174278\n",
      "epoch 5, batch 465, train_loss 3.03941488266\n",
      "epoch 5, batch 466, train_loss 3.86457180977\n",
      "epoch 5, batch 467, train_loss 3.66319680214\n",
      "epoch 5, batch 468, train_loss 3.81362509727\n",
      "epoch 5, batch 469, train_loss 3.35280537605\n",
      "epoch 5, batch 470, train_loss 3.21441078186\n",
      "epoch 5, batch 471, train_loss 3.28784584999\n",
      "epoch 5, batch 472, train_loss 3.15062117577\n",
      "epoch 5, batch 473, train_loss 3.89526128769\n",
      "epoch 5, batch 474, train_loss 4.51823663712\n",
      "epoch 5, batch 475, train_loss 3.14329695702\n",
      "epoch 5, batch 476, train_loss 2.64860725403\n",
      "epoch 5, batch 477, train_loss 2.76697802544\n",
      "epoch 5, batch 478, train_loss 2.59875559807\n",
      "epoch 5, batch 479, train_loss 3.12857937813\n",
      "epoch 5, batch 480, train_loss 3.35213780403\n",
      "epoch 5, batch 481, train_loss 5.0168504715\n",
      "epoch 5, batch 482, train_loss 3.0294687748\n",
      "epoch 5, batch 483, train_loss 3.70122885704\n",
      "epoch 5, batch 484, train_loss 3.34369301796\n",
      "epoch 5, batch 485, train_loss 2.52745747566\n",
      "epoch 5, batch 486, train_loss 2.52991843224\n",
      "epoch 5, batch 487, train_loss 2.68398523331\n",
      "epoch 5, batch 488, train_loss 3.10534095764\n",
      "epoch 5, batch 489, train_loss 3.21849536896\n",
      "epoch 5, batch 490, train_loss 3.15329909325\n",
      "epoch 5, batch 491, train_loss 3.07471871376\n",
      "epoch 5, batch 492, train_loss 3.41007852554\n",
      "epoch 5, batch 493, train_loss 2.9668302536\n",
      "epoch 5, batch 494, train_loss 3.07800674438\n",
      "epoch 5, batch 495, train_loss 3.14559412003\n",
      "epoch 5, batch 496, train_loss 2.76287150383\n",
      "epoch 5, batch 497, train_loss 2.95181107521\n",
      "epoch 5, batch 498, train_loss 3.10817503929\n",
      "epoch 5, batch 499, train_loss 3.0564968586\n",
      "epoch 5, batch 500, train_loss 3.5697696209\n",
      "epoch 5, batch 501, train_loss 3.1718711853\n",
      "epoch 5, batch 502, train_loss 4.5549120903\n",
      "epoch 5, batch 503, train_loss 4.50260591507\n",
      "epoch 5, batch 504, train_loss 3.90997028351\n",
      "epoch 5, batch 505, train_loss 3.69009852409\n",
      "epoch 5, batch 506, train_loss 3.3502612114\n",
      "epoch 5, batch 507, train_loss 4.40332317352\n",
      "epoch 5, batch 508, train_loss 3.13188242912\n",
      "epoch 5, batch 509, train_loss 4.56075763702\n",
      "epoch 5, batch 510, train_loss 4.53490257263\n",
      "epoch 5, batch 511, train_loss 4.55611896515\n",
      "epoch 5, batch 512, train_loss 3.52556967735\n",
      "epoch 5, batch 513, train_loss 4.98917341232\n",
      "epoch 5, batch 514, train_loss 4.91777515411\n",
      "epoch 5, batch 515, train_loss 4.82371902466\n",
      "epoch 5, batch 516, train_loss 3.00641608238\n",
      "epoch 5, batch 517, train_loss 3.18353319168\n",
      "epoch 5, batch 518, train_loss 3.08719587326\n",
      "epoch 5, batch 519, train_loss 2.77487802505\n",
      "epoch 5, batch 520, train_loss 3.98369336128\n",
      "epoch 5, batch 521, train_loss 3.11703944206\n",
      "epoch 5, batch 522, train_loss 2.68824648857\n",
      "epoch 5, batch 523, train_loss 2.70120763779\n",
      "epoch 5, batch 524, train_loss 2.54742836952\n",
      "epoch 5, batch 525, train_loss 2.3842792511\n",
      "epoch 5, batch 526, train_loss 2.47164440155\n",
      "epoch 5, batch 527, train_loss 2.49596309662\n",
      "epoch 5, batch 528, train_loss 2.74834394455\n",
      "epoch 5, batch 529, train_loss 2.63759708405\n",
      "epoch 5, batch 530, train_loss 2.10867261887\n",
      "epoch 5, batch 531, train_loss 1.81629085541\n",
      "epoch 5, batch 532, train_loss 2.78078603745\n",
      "epoch 5, batch 533, train_loss 1.74311578274\n",
      "epoch 5, batch 534, train_loss 1.79605054855\n",
      "epoch 5, batch 535, train_loss 3.32211136818\n",
      "epoch 5, batch 536, train_loss 3.55234646797\n",
      "epoch 5, batch 537, train_loss 3.424492836\n",
      "epoch 5, batch 538, train_loss 3.60122847557\n",
      "epoch 5, batch 539, train_loss 3.92542719841\n",
      "epoch 5, batch 540, train_loss 4.25737380981\n",
      "epoch 6, batch 0, train_loss 3.517131567\n",
      "epoch 6, batch 1, train_loss 3.9285929203\n",
      "epoch 6, batch 2, train_loss 3.29295110703\n",
      "epoch 6, batch 3, train_loss 3.52392864227\n",
      "epoch 6, batch 4, train_loss 3.3346145153\n",
      "epoch 6, batch 5, train_loss 3.39999485016\n",
      "epoch 6, batch 6, train_loss 4.48391342163\n",
      "epoch 6, batch 7, train_loss 3.34609341621\n",
      "epoch 6, batch 8, train_loss 2.96832108498\n",
      "epoch 6, batch 9, train_loss 4.02888917923\n",
      "epoch 6, batch 10, train_loss 3.19155955315\n",
      "epoch 6, batch 11, train_loss 3.28233146667\n",
      "epoch 6, batch 12, train_loss 2.99138116837\n",
      "epoch 6, batch 13, train_loss 3.51955485344\n",
      "epoch 6, batch 14, train_loss 2.89193987846\n",
      "epoch 6, batch 15, train_loss 3.64141225815\n",
      "epoch 6, batch 16, train_loss 3.17663478851\n",
      "epoch 6, batch 17, train_loss 3.02326393127\n",
      "epoch 6, batch 18, train_loss 2.64390349388\n",
      "epoch 6, batch 19, train_loss 2.18633890152\n",
      "epoch 6, batch 20, train_loss 3.07441830635\n",
      "epoch 6, batch 21, train_loss 3.00210046768\n",
      "epoch 6, batch 22, train_loss 3.51681065559\n",
      "epoch 6, batch 23, train_loss 3.04985952377\n",
      "epoch 6, batch 24, train_loss 3.04127264023\n",
      "epoch 6, batch 25, train_loss 3.51763248444\n",
      "epoch 6, batch 26, train_loss 3.44863414764\n",
      "epoch 6, batch 27, train_loss 3.34830546379\n",
      "epoch 6, batch 28, train_loss 3.65895080566\n",
      "epoch 6, batch 29, train_loss 3.56737208366\n",
      "epoch 6, batch 30, train_loss 3.6589653492\n",
      "epoch 6, batch 31, train_loss 3.07503914833\n",
      "epoch 6, batch 32, train_loss 2.81172943115\n",
      "epoch 6, batch 33, train_loss 4.77745103836\n",
      "epoch 6, batch 34, train_loss 4.85740089417\n",
      "epoch 6, batch 35, train_loss 3.42377448082\n",
      "epoch 6, batch 36, train_loss 3.45140480995\n",
      "epoch 6, batch 37, train_loss 3.4045176506\n",
      "epoch 6, batch 38, train_loss 3.53077244759\n",
      "epoch 6, batch 39, train_loss 4.01860523224\n",
      "epoch 6, batch 40, train_loss 3.08710074425\n",
      "epoch 6, batch 41, train_loss 3.35581374168\n",
      "epoch 6, batch 42, train_loss 3.39144015312\n",
      "epoch 6, batch 43, train_loss 3.15870738029\n",
      "epoch 6, batch 44, train_loss 3.79837608337\n",
      "epoch 6, batch 45, train_loss 3.40449690819\n",
      "epoch 6, batch 46, train_loss 3.35609316826\n",
      "epoch 6, batch 47, train_loss 3.47983908653\n",
      "epoch 6, batch 48, train_loss 3.29486083984\n",
      "epoch 6, batch 49, train_loss 3.44838237762\n",
      "epoch 6, batch 50, train_loss 3.32879209518\n",
      "epoch 6, batch 51, train_loss 2.99835824966\n",
      "epoch 6, batch 52, train_loss 3.46762871742\n",
      "epoch 6, batch 53, train_loss 3.32982325554\n",
      "epoch 6, batch 54, train_loss 3.0562722683\n",
      "epoch 6, batch 55, train_loss 3.09523224831\n",
      "epoch 6, batch 56, train_loss 2.98787307739\n",
      "epoch 6, batch 57, train_loss 3.50701785088\n",
      "epoch 6, batch 58, train_loss 3.29469656944\n",
      "epoch 6, batch 59, train_loss 3.26205062866\n",
      "epoch 6, batch 60, train_loss 3.46182823181\n",
      "epoch 6, batch 61, train_loss 2.00698518753\n",
      "epoch 6, batch 62, train_loss 3.02540636063\n",
      "epoch 6, batch 63, train_loss 3.16758275032\n",
      "epoch 6, batch 64, train_loss 3.38730120659\n",
      "epoch 6, batch 65, train_loss 3.72559118271\n",
      "epoch 6, batch 66, train_loss 3.49768400192\n",
      "epoch 6, batch 67, train_loss 2.24562120438\n",
      "epoch 6, batch 68, train_loss 3.19604229927\n",
      "epoch 6, batch 69, train_loss 2.49913716316\n",
      "epoch 6, batch 70, train_loss 4.15611839294\n",
      "epoch 6, batch 71, train_loss 4.20010089874\n",
      "epoch 6, batch 72, train_loss 2.96820759773\n",
      "epoch 6, batch 73, train_loss 3.40336990356\n",
      "epoch 6, batch 74, train_loss 4.01768255234\n",
      "epoch 6, batch 75, train_loss 3.45911645889\n",
      "epoch 6, batch 76, train_loss 3.74275898933\n",
      "epoch 6, batch 77, train_loss 4.48959159851\n",
      "epoch 6, batch 78, train_loss 3.26366972923\n",
      "epoch 6, batch 79, train_loss 3.28855371475\n",
      "epoch 6, batch 80, train_loss 3.11005616188\n",
      "epoch 6, batch 81, train_loss 3.23026657104\n",
      "epoch 6, batch 82, train_loss 3.13066005707\n",
      "epoch 6, batch 83, train_loss 3.25076556206\n",
      "epoch 6, batch 84, train_loss 3.0086042881\n",
      "epoch 6, batch 85, train_loss 3.04787635803\n",
      "epoch 6, batch 86, train_loss 3.56215929985\n",
      "epoch 6, batch 87, train_loss 2.7514603138\n",
      "epoch 6, batch 88, train_loss 3.05577373505\n",
      "epoch 6, batch 89, train_loss 3.3622136116\n",
      "epoch 6, batch 90, train_loss 3.04893541336\n",
      "epoch 6, batch 91, train_loss 3.11466312408\n",
      "epoch 6, batch 92, train_loss 3.61122751236\n",
      "epoch 6, batch 93, train_loss 4.39706754684\n",
      "epoch 6, batch 94, train_loss 3.11975002289\n",
      "epoch 6, batch 95, train_loss 3.19833302498\n",
      "epoch 6, batch 96, train_loss 2.53873205185\n",
      "epoch 6, batch 97, train_loss 3.19331526756\n",
      "epoch 6, batch 98, train_loss 3.11911916733\n",
      "epoch 6, batch 99, train_loss 3.38211297989\n",
      "epoch 6, batch 100, train_loss 3.14212751389\n",
      "epoch 6, batch 101, train_loss 3.31539678574\n",
      "epoch 6, batch 102, train_loss 3.1193420887\n",
      "epoch 6, batch 103, train_loss 4.41017532349\n",
      "epoch 6, batch 104, train_loss 3.63364100456\n",
      "epoch 6, batch 105, train_loss 3.43980073929\n",
      "epoch 6, batch 106, train_loss 3.85200953484\n",
      "epoch 6, batch 107, train_loss 3.67038178444\n",
      "epoch 6, batch 108, train_loss 3.28086924553\n",
      "epoch 6, batch 109, train_loss 3.5385324955\n",
      "epoch 6, batch 110, train_loss 3.34478974342\n",
      "epoch 6, batch 111, train_loss 3.75546097755\n",
      "epoch 6, batch 112, train_loss 3.72097611427\n",
      "epoch 6, batch 113, train_loss 3.1171913147\n",
      "epoch 6, batch 114, train_loss 3.69572639465\n",
      "epoch 6, batch 115, train_loss 4.49875020981\n",
      "epoch 6, batch 116, train_loss 4.3867354393\n",
      "epoch 6, batch 117, train_loss 4.51176643372\n",
      "epoch 6, batch 118, train_loss 3.90713596344\n",
      "epoch 6, batch 119, train_loss 2.57772421837\n",
      "epoch 6, batch 120, train_loss 3.42972755432\n",
      "epoch 6, batch 121, train_loss 3.27551865578\n",
      "epoch 6, batch 122, train_loss 3.48874139786\n",
      "epoch 6, batch 123, train_loss 3.25049066544\n",
      "epoch 6, batch 124, train_loss 2.93838000298\n",
      "epoch 6, batch 125, train_loss 2.8985953331\n",
      "epoch 6, batch 126, train_loss 2.87294244766\n",
      "epoch 6, batch 127, train_loss 2.95111989975\n",
      "epoch 6, batch 128, train_loss 2.81467676163\n",
      "epoch 6, batch 129, train_loss 3.20717787743\n",
      "epoch 6, batch 130, train_loss 3.13738870621\n",
      "epoch 6, batch 131, train_loss 3.49671435356\n",
      "epoch 6, batch 132, train_loss 3.23081064224\n",
      "epoch 6, batch 133, train_loss 2.44305944443\n",
      "epoch 6, batch 134, train_loss 2.48839235306\n",
      "epoch 6, batch 135, train_loss 4.35231971741\n",
      "epoch 6, batch 136, train_loss 3.70518040657\n",
      "epoch 6, batch 137, train_loss 2.8581135273\n",
      "epoch 6, batch 138, train_loss 2.7140378952\n",
      "epoch 6, batch 139, train_loss 3.23944616318\n",
      "epoch 6, batch 140, train_loss 2.70877933502\n",
      "epoch 6, batch 141, train_loss 3.05254340172\n",
      "epoch 6, batch 142, train_loss 4.3457403183\n",
      "epoch 6, batch 143, train_loss 2.72973561287\n",
      "epoch 6, batch 144, train_loss 2.20248413086\n",
      "epoch 6, batch 145, train_loss 3.24044775963\n",
      "epoch 6, batch 146, train_loss 3.414935112\n",
      "epoch 6, batch 147, train_loss 3.14948558807\n",
      "epoch 6, batch 148, train_loss 3.03577065468\n",
      "epoch 6, batch 149, train_loss 3.50357103348\n",
      "epoch 6, batch 150, train_loss 3.60833668709\n",
      "epoch 6, batch 151, train_loss 2.20605397224\n",
      "epoch 6, batch 152, train_loss 2.48111629486\n",
      "epoch 6, batch 153, train_loss 4.23286676407\n",
      "epoch 6, batch 154, train_loss 3.33778834343\n",
      "epoch 6, batch 155, train_loss 2.91035223007\n",
      "epoch 6, batch 156, train_loss 3.12082648277\n",
      "epoch 6, batch 157, train_loss 2.67565631866\n",
      "epoch 6, batch 158, train_loss 2.7117729187\n",
      "epoch 6, batch 159, train_loss 2.90647959709\n",
      "epoch 6, batch 160, train_loss 3.0322470665\n",
      "epoch 6, batch 161, train_loss 3.5737695694\n",
      "epoch 6, batch 162, train_loss 3.52217507362\n",
      "epoch 6, batch 163, train_loss 5.04339313507\n",
      "epoch 6, batch 164, train_loss 2.43602228165\n",
      "epoch 6, batch 165, train_loss 2.38292312622\n",
      "epoch 6, batch 166, train_loss 2.75306129456\n",
      "epoch 6, batch 167, train_loss 2.77078008652\n",
      "epoch 6, batch 168, train_loss 3.01807761192\n",
      "epoch 6, batch 169, train_loss 2.7821586132\n",
      "epoch 6, batch 170, train_loss 3.33668756485\n",
      "epoch 6, batch 171, train_loss 2.73768377304\n",
      "epoch 6, batch 172, train_loss 3.3729300499\n",
      "epoch 6, batch 173, train_loss 2.24089574814\n",
      "epoch 6, batch 174, train_loss 3.86540007591\n",
      "epoch 6, batch 175, train_loss 3.30799388885\n",
      "epoch 6, batch 176, train_loss 3.19822049141\n",
      "epoch 6, batch 177, train_loss 2.99373984337\n",
      "epoch 6, batch 178, train_loss 2.91344332695\n",
      "epoch 6, batch 179, train_loss 3.16541242599\n",
      "epoch 6, batch 180, train_loss 3.04979920387\n",
      "epoch 6, batch 181, train_loss 3.71083021164\n",
      "epoch 6, batch 182, train_loss 3.16958332062\n",
      "epoch 6, batch 183, train_loss 3.84596300125\n",
      "epoch 6, batch 184, train_loss 2.49959802628\n",
      "epoch 6, batch 185, train_loss 2.87789511681\n",
      "epoch 6, batch 186, train_loss 3.55294847488\n",
      "epoch 6, batch 187, train_loss 2.58774352074\n",
      "epoch 6, batch 188, train_loss 2.92760658264\n",
      "epoch 6, batch 189, train_loss 2.90768814087\n",
      "epoch 6, batch 190, train_loss 3.70412039757\n",
      "epoch 6, batch 191, train_loss 3.56844639778\n",
      "epoch 6, batch 192, train_loss 4.57707452774\n",
      "epoch 6, batch 193, train_loss 4.27442455292\n",
      "epoch 6, batch 194, train_loss 4.90713405609\n",
      "epoch 6, batch 195, train_loss 4.99307441711\n",
      "epoch 6, batch 196, train_loss 3.33814120293\n",
      "epoch 6, batch 197, train_loss 2.66919445992\n",
      "epoch 6, batch 198, train_loss 2.43821358681\n",
      "epoch 6, batch 199, train_loss 3.52394795418\n",
      "epoch 6, batch 200, train_loss 2.93475222588\n",
      "epoch 6, batch 201, train_loss 3.27190160751\n",
      "epoch 6, batch 202, train_loss 3.71359658241\n",
      "epoch 6, batch 203, train_loss 3.84753251076\n",
      "epoch 6, batch 204, train_loss 3.81208467484\n",
      "epoch 6, batch 205, train_loss 3.29389953613\n",
      "epoch 6, batch 206, train_loss 4.40254402161\n",
      "epoch 6, batch 207, train_loss 3.6739461422\n",
      "epoch 6, batch 208, train_loss 4.41054058075\n",
      "epoch 6, batch 209, train_loss 4.13305044174\n",
      "epoch 6, batch 210, train_loss 2.26295018196\n",
      "epoch 6, batch 211, train_loss 3.06676959991\n",
      "epoch 6, batch 212, train_loss 4.00083208084\n",
      "epoch 6, batch 213, train_loss 3.3189136982\n",
      "epoch 6, batch 214, train_loss 3.15184736252\n",
      "epoch 6, batch 215, train_loss 2.33921647072\n",
      "epoch 6, batch 216, train_loss 3.54531908035\n",
      "epoch 6, batch 217, train_loss 2.95194602013\n",
      "epoch 6, batch 218, train_loss 3.44278454781\n",
      "epoch 6, batch 219, train_loss 3.57291889191\n",
      "epoch 6, batch 220, train_loss 3.60579371452\n",
      "epoch 6, batch 221, train_loss 3.77295660973\n",
      "epoch 6, batch 222, train_loss 2.44006419182\n",
      "epoch 6, batch 223, train_loss 2.44129133224\n",
      "epoch 6, batch 224, train_loss 2.79289054871\n",
      "epoch 6, batch 225, train_loss 2.93607854843\n",
      "epoch 6, batch 226, train_loss 3.49678945541\n",
      "epoch 6, batch 227, train_loss 3.23139929771\n",
      "epoch 6, batch 228, train_loss 3.40248703957\n",
      "epoch 6, batch 229, train_loss 3.22266697884\n",
      "epoch 6, batch 230, train_loss 3.07316970825\n",
      "epoch 6, batch 231, train_loss 2.76523900032\n",
      "epoch 6, batch 232, train_loss 3.44011545181\n",
      "epoch 6, batch 233, train_loss 3.70601797104\n",
      "epoch 6, batch 234, train_loss 3.24077367783\n",
      "epoch 6, batch 235, train_loss 3.84699940681\n",
      "epoch 6, batch 236, train_loss 3.32873654366\n",
      "epoch 6, batch 237, train_loss 3.2472679615\n",
      "epoch 6, batch 238, train_loss 3.26712107658\n",
      "epoch 6, batch 239, train_loss 3.07359623909\n",
      "epoch 6, batch 240, train_loss 3.5408437252\n",
      "epoch 6, batch 241, train_loss 3.46352815628\n",
      "epoch 6, batch 242, train_loss 3.28472685814\n",
      "epoch 6, batch 243, train_loss 3.48337364197\n",
      "epoch 6, batch 244, train_loss 3.6085960865\n",
      "epoch 6, batch 245, train_loss 3.36469578743\n",
      "epoch 6, batch 246, train_loss 3.29341220856\n",
      "epoch 6, batch 247, train_loss 3.48916864395\n",
      "epoch 6, batch 248, train_loss 3.2235853672\n",
      "epoch 6, batch 249, train_loss 3.95926761627\n",
      "epoch 6, batch 250, train_loss 3.73576688766\n",
      "epoch 6, batch 251, train_loss 4.09482574463\n",
      "epoch 6, batch 252, train_loss 2.80900168419\n",
      "epoch 6, batch 253, train_loss 2.3209373951\n",
      "epoch 6, batch 254, train_loss 3.00267267227\n",
      "epoch 6, batch 255, train_loss 2.31918430328\n",
      "epoch 6, batch 256, train_loss 2.71402096748\n",
      "epoch 6, batch 257, train_loss 3.20511269569\n",
      "epoch 6, batch 258, train_loss 2.55291581154\n",
      "epoch 6, batch 259, train_loss 2.48594164848\n",
      "epoch 6, batch 260, train_loss 2.21297264099\n",
      "epoch 6, batch 261, train_loss 3.64708399773\n",
      "epoch 6, batch 262, train_loss 4.26193332672\n",
      "epoch 6, batch 263, train_loss 3.29953718185\n",
      "epoch 6, batch 264, train_loss 3.00093770027\n",
      "epoch 6, batch 265, train_loss 3.68904328346\n",
      "epoch 6, batch 266, train_loss 3.74105501175\n",
      "epoch 6, batch 267, train_loss 2.94126677513\n",
      "epoch 6, batch 268, train_loss 2.46470570564\n",
      "epoch 6, batch 269, train_loss 2.53407621384\n",
      "epoch 6, batch 270, train_loss 2.84908223152\n",
      "epoch 6, batch 271, train_loss 3.21396613121\n",
      "epoch 6, batch 272, train_loss 3.23381781578\n",
      "epoch 6, batch 273, train_loss 3.3626396656\n",
      "epoch 6, batch 274, train_loss 3.14693427086\n",
      "epoch 6, batch 275, train_loss 3.17009806633\n",
      "epoch 6, batch 276, train_loss 2.92954301834\n",
      "epoch 6, batch 277, train_loss 3.78863120079\n",
      "epoch 6, batch 278, train_loss 2.91247963905\n",
      "epoch 6, batch 279, train_loss 3.48463797569\n",
      "epoch 6, batch 280, train_loss 3.46876859665\n",
      "epoch 6, batch 281, train_loss 3.23731303215\n",
      "epoch 6, batch 282, train_loss 2.85985422134\n",
      "epoch 6, batch 283, train_loss 4.56424427032\n",
      "epoch 6, batch 284, train_loss 3.40788555145\n",
      "epoch 6, batch 285, train_loss 2.08917570114\n",
      "epoch 6, batch 286, train_loss 2.33958506584\n",
      "epoch 6, batch 287, train_loss 2.44821023941\n",
      "epoch 6, batch 288, train_loss 3.12049603462\n",
      "epoch 6, batch 289, train_loss 3.17420244217\n",
      "epoch 6, batch 290, train_loss 3.03435969353\n",
      "epoch 6, batch 291, train_loss 2.87815928459\n",
      "epoch 6, batch 292, train_loss 2.35557508469\n",
      "epoch 6, batch 293, train_loss 3.51389122009\n",
      "epoch 6, batch 294, train_loss 3.04835367203\n",
      "epoch 6, batch 295, train_loss 2.75712275505\n",
      "epoch 6, batch 296, train_loss 3.07034540176\n",
      "epoch 6, batch 297, train_loss 2.71368503571\n",
      "epoch 6, batch 298, train_loss 2.97862768173\n",
      "epoch 6, batch 299, train_loss 3.58817791939\n",
      "epoch 6, batch 300, train_loss 3.28119587898\n",
      "epoch 6, batch 301, train_loss 4.32340049744\n",
      "epoch 6, batch 302, train_loss 4.34155654907\n",
      "epoch 6, batch 303, train_loss 3.97954297066\n",
      "epoch 6, batch 304, train_loss 4.69274425507\n",
      "epoch 6, batch 305, train_loss 4.63752603531\n",
      "epoch 6, batch 306, train_loss 3.21897149086\n",
      "epoch 6, batch 307, train_loss 2.63828754425\n",
      "epoch 6, batch 308, train_loss 3.09670948982\n",
      "epoch 6, batch 309, train_loss 3.26739907265\n",
      "epoch 6, batch 310, train_loss 3.05288982391\n",
      "epoch 6, batch 311, train_loss 3.1839594841\n",
      "epoch 6, batch 312, train_loss 3.3703854084\n",
      "epoch 6, batch 313, train_loss 3.06485700607\n",
      "epoch 6, batch 314, train_loss 3.44094061852\n",
      "epoch 6, batch 315, train_loss 2.87803339958\n",
      "epoch 6, batch 316, train_loss 3.04322195053\n",
      "epoch 6, batch 317, train_loss 3.54648923874\n",
      "epoch 6, batch 318, train_loss 2.6348297596\n",
      "epoch 6, batch 319, train_loss 4.46912527084\n",
      "epoch 6, batch 320, train_loss 3.27828860283\n",
      "epoch 6, batch 321, train_loss 4.57802200317\n",
      "epoch 6, batch 322, train_loss 2.52964258194\n",
      "epoch 6, batch 323, train_loss 2.38628411293\n",
      "epoch 6, batch 324, train_loss 3.59386873245\n",
      "epoch 6, batch 325, train_loss 3.21538305283\n",
      "epoch 6, batch 326, train_loss 3.21153831482\n",
      "epoch 6, batch 327, train_loss 3.19486141205\n",
      "epoch 6, batch 328, train_loss 3.07364296913\n",
      "epoch 6, batch 329, train_loss 2.71841669083\n",
      "epoch 6, batch 330, train_loss 4.10261678696\n",
      "epoch 6, batch 331, train_loss 3.75336074829\n",
      "epoch 6, batch 332, train_loss 2.54539728165\n",
      "epoch 6, batch 333, train_loss 3.67510819435\n",
      "epoch 6, batch 334, train_loss 3.36652803421\n",
      "epoch 6, batch 335, train_loss 3.26918673515\n",
      "epoch 6, batch 336, train_loss 3.86598730087\n",
      "epoch 6, batch 337, train_loss 2.358066082\n",
      "epoch 6, batch 338, train_loss 3.21629786491\n",
      "epoch 6, batch 339, train_loss 2.99646377563\n",
      "epoch 6, batch 340, train_loss 3.19309163094\n",
      "epoch 6, batch 341, train_loss 3.11890006065\n",
      "epoch 6, batch 342, train_loss 3.17245721817\n",
      "epoch 6, batch 343, train_loss 3.07081437111\n",
      "epoch 6, batch 344, train_loss 4.09004974365\n",
      "epoch 6, batch 345, train_loss 3.48599910736\n",
      "epoch 6, batch 346, train_loss 3.22368860245\n",
      "epoch 6, batch 347, train_loss 3.0094306469\n",
      "epoch 6, batch 348, train_loss 3.25190901756\n",
      "epoch 6, batch 349, train_loss 4.35929393768\n",
      "epoch 6, batch 350, train_loss 4.32297134399\n",
      "epoch 6, batch 351, train_loss 4.08143424988\n",
      "epoch 6, batch 352, train_loss 4.14607000351\n",
      "epoch 6, batch 353, train_loss 3.36680269241\n",
      "epoch 6, batch 354, train_loss 2.73512434959\n",
      "epoch 6, batch 355, train_loss 3.04070425034\n",
      "epoch 6, batch 356, train_loss 3.02813196182\n",
      "epoch 6, batch 357, train_loss 2.96229314804\n",
      "epoch 6, batch 358, train_loss 2.50238347054\n",
      "epoch 6, batch 359, train_loss 3.52271485329\n",
      "epoch 6, batch 360, train_loss 3.00396490097\n",
      "epoch 6, batch 361, train_loss 2.64801812172\n",
      "epoch 6, batch 362, train_loss 4.46457195282\n",
      "epoch 6, batch 363, train_loss 3.27192163467\n",
      "epoch 6, batch 364, train_loss 3.16548848152\n",
      "epoch 6, batch 365, train_loss 4.10340452194\n",
      "epoch 6, batch 366, train_loss 4.23780107498\n",
      "epoch 6, batch 367, train_loss 4.99629926682\n",
      "epoch 6, batch 368, train_loss 2.60180449486\n",
      "epoch 6, batch 369, train_loss 3.56609153748\n",
      "epoch 6, batch 370, train_loss 3.96236395836\n",
      "epoch 6, batch 371, train_loss 4.9636554718\n",
      "epoch 6, batch 372, train_loss 2.39853310585\n",
      "epoch 6, batch 373, train_loss 2.57643890381\n",
      "epoch 6, batch 374, train_loss 2.90194988251\n",
      "epoch 6, batch 375, train_loss 3.3114528656\n",
      "epoch 6, batch 376, train_loss 2.5531771183\n",
      "epoch 6, batch 377, train_loss 2.97051787376\n",
      "epoch 6, batch 378, train_loss 3.70321202278\n",
      "epoch 6, batch 379, train_loss 2.85784602165\n",
      "epoch 6, batch 380, train_loss 2.87636327744\n",
      "epoch 6, batch 381, train_loss 2.9786336422\n",
      "epoch 6, batch 382, train_loss 3.91653728485\n",
      "epoch 6, batch 383, train_loss 3.43932723999\n",
      "epoch 6, batch 384, train_loss 3.09150147438\n",
      "epoch 6, batch 385, train_loss 4.54233217239\n",
      "epoch 6, batch 386, train_loss 2.79367303848\n",
      "epoch 6, batch 387, train_loss 4.75980901718\n",
      "epoch 6, batch 388, train_loss 2.79433655739\n",
      "epoch 6, batch 389, train_loss 4.43736982346\n",
      "epoch 6, batch 390, train_loss 4.40309667587\n",
      "epoch 6, batch 391, train_loss 4.43754148483\n",
      "epoch 6, batch 392, train_loss 4.17892694473\n",
      "epoch 6, batch 393, train_loss 3.40812015533\n",
      "epoch 6, batch 394, train_loss 4.0635881424\n",
      "epoch 6, batch 395, train_loss 2.98150444031\n",
      "epoch 6, batch 396, train_loss 4.32930517197\n",
      "epoch 6, batch 397, train_loss 4.44804048538\n",
      "epoch 6, batch 398, train_loss 3.3222849369\n",
      "epoch 6, batch 399, train_loss 3.57201385498\n",
      "epoch 6, batch 400, train_loss 4.59631490707\n",
      "epoch 6, batch 401, train_loss 3.79843783379\n",
      "epoch 6, batch 402, train_loss 3.34496998787\n",
      "epoch 6, batch 403, train_loss 3.29391098022\n",
      "epoch 6, batch 404, train_loss 4.16042804718\n",
      "epoch 6, batch 405, train_loss 3.63404345512\n",
      "epoch 6, batch 406, train_loss 3.47695493698\n",
      "epoch 6, batch 407, train_loss 3.27384305\n",
      "epoch 6, batch 408, train_loss 3.12925171852\n",
      "epoch 6, batch 409, train_loss 2.609821558\n",
      "epoch 6, batch 410, train_loss 4.28719043732\n",
      "epoch 6, batch 411, train_loss 3.3523876667\n",
      "epoch 6, batch 412, train_loss 2.85376524925\n",
      "epoch 6, batch 413, train_loss 3.22052383423\n",
      "epoch 6, batch 414, train_loss 3.76124072075\n",
      "epoch 6, batch 415, train_loss 3.81655049324\n",
      "epoch 6, batch 416, train_loss 3.56950950623\n",
      "epoch 6, batch 417, train_loss 3.08323097229\n",
      "epoch 6, batch 418, train_loss 3.5061237812\n",
      "epoch 6, batch 419, train_loss 3.12285685539\n",
      "epoch 6, batch 420, train_loss 2.51960802078\n",
      "epoch 6, batch 421, train_loss 4.41629076004\n",
      "epoch 6, batch 422, train_loss 3.09995651245\n",
      "epoch 6, batch 423, train_loss 3.22389602661\n",
      "epoch 6, batch 424, train_loss 4.35063743591\n",
      "epoch 6, batch 425, train_loss 3.62267494202\n",
      "epoch 6, batch 426, train_loss 4.8376121521\n",
      "epoch 6, batch 427, train_loss 3.92602348328\n",
      "epoch 6, batch 428, train_loss 2.69922876358\n",
      "epoch 6, batch 429, train_loss 3.75171375275\n",
      "epoch 6, batch 430, train_loss 3.81440329552\n",
      "epoch 6, batch 431, train_loss 3.40075278282\n",
      "epoch 6, batch 432, train_loss 3.60122513771\n",
      "epoch 6, batch 433, train_loss 2.98060250282\n",
      "epoch 6, batch 434, train_loss 3.49886393547\n",
      "epoch 6, batch 435, train_loss 3.29047656059\n",
      "epoch 6, batch 436, train_loss 3.65566039085\n",
      "epoch 6, batch 437, train_loss 4.13258743286\n",
      "epoch 6, batch 438, train_loss 3.75300478935\n",
      "epoch 6, batch 439, train_loss 3.51323628426\n",
      "epoch 6, batch 440, train_loss 4.87029361725\n",
      "epoch 6, batch 441, train_loss 5.00739192963\n",
      "epoch 6, batch 442, train_loss 4.12166070938\n",
      "epoch 6, batch 443, train_loss 2.53606438637\n",
      "epoch 6, batch 444, train_loss 3.06428575516\n",
      "epoch 6, batch 445, train_loss 4.70662736893\n",
      "epoch 6, batch 446, train_loss 3.42818713188\n",
      "epoch 6, batch 447, train_loss 3.11922073364\n",
      "epoch 6, batch 448, train_loss 4.58860635757\n",
      "epoch 6, batch 449, train_loss 3.92811989784\n",
      "epoch 6, batch 450, train_loss 3.26074194908\n",
      "epoch 6, batch 451, train_loss 3.01796412468\n",
      "epoch 6, batch 452, train_loss 2.7546517849\n",
      "epoch 6, batch 453, train_loss 5.30353832245\n",
      "epoch 6, batch 454, train_loss 5.19133234024\n",
      "epoch 6, batch 455, train_loss 2.6233921051\n",
      "epoch 6, batch 456, train_loss 3.3408946991\n",
      "epoch 6, batch 457, train_loss 3.60078549385\n",
      "epoch 6, batch 458, train_loss 3.19572281837\n",
      "epoch 6, batch 459, train_loss 3.49988985062\n",
      "epoch 6, batch 460, train_loss 4.02339220047\n",
      "epoch 6, batch 461, train_loss 3.4944589138\n",
      "epoch 6, batch 462, train_loss 3.1582710743\n",
      "epoch 6, batch 463, train_loss 3.31806850433\n",
      "epoch 6, batch 464, train_loss 3.61017012596\n",
      "epoch 6, batch 465, train_loss 2.97520828247\n",
      "epoch 6, batch 466, train_loss 3.78312969208\n",
      "epoch 6, batch 467, train_loss 3.59162044525\n",
      "epoch 6, batch 468, train_loss 3.73941373825\n",
      "epoch 6, batch 469, train_loss 3.28759598732\n",
      "epoch 6, batch 470, train_loss 3.14711928368\n",
      "epoch 6, batch 471, train_loss 3.22373461723\n",
      "epoch 6, batch 472, train_loss 3.08789730072\n",
      "epoch 6, batch 473, train_loss 3.81484222412\n",
      "epoch 6, batch 474, train_loss 4.42514896393\n",
      "epoch 6, batch 475, train_loss 3.09203481674\n",
      "epoch 6, batch 476, train_loss 2.59957480431\n",
      "epoch 6, batch 477, train_loss 2.70796370506\n",
      "epoch 6, batch 478, train_loss 2.54202342033\n",
      "epoch 6, batch 479, train_loss 3.06286430359\n",
      "epoch 6, batch 480, train_loss 3.2772667408\n",
      "epoch 6, batch 481, train_loss 4.91342878342\n",
      "epoch 6, batch 482, train_loss 2.96596765518\n",
      "epoch 6, batch 483, train_loss 3.61853861809\n",
      "epoch 6, batch 484, train_loss 3.27319574356\n",
      "epoch 6, batch 485, train_loss 2.47157621384\n",
      "epoch 6, batch 486, train_loss 2.47824931145\n",
      "epoch 6, batch 487, train_loss 2.63441705704\n",
      "epoch 6, batch 488, train_loss 3.04442167282\n",
      "epoch 6, batch 489, train_loss 3.16402459145\n",
      "epoch 6, batch 490, train_loss 3.09445881844\n",
      "epoch 6, batch 491, train_loss 3.01420545578\n",
      "epoch 6, batch 492, train_loss 3.35570549965\n",
      "epoch 6, batch 493, train_loss 2.92297339439\n",
      "epoch 6, batch 494, train_loss 3.02800035477\n",
      "epoch 6, batch 495, train_loss 3.09564781189\n",
      "epoch 6, batch 496, train_loss 2.71897268295\n",
      "epoch 6, batch 497, train_loss 2.9021320343\n",
      "epoch 6, batch 498, train_loss 3.05365729332\n",
      "epoch 6, batch 499, train_loss 3.00347518921\n",
      "epoch 6, batch 500, train_loss 3.49520611763\n",
      "epoch 6, batch 501, train_loss 3.11792778969\n",
      "epoch 6, batch 502, train_loss 4.46364927292\n",
      "epoch 6, batch 503, train_loss 4.40661382675\n",
      "epoch 6, batch 504, train_loss 3.83683228493\n",
      "epoch 6, batch 505, train_loss 3.6229865551\n",
      "epoch 6, batch 506, train_loss 3.2827706337\n",
      "epoch 6, batch 507, train_loss 4.32224178314\n",
      "epoch 6, batch 508, train_loss 3.07984113693\n",
      "epoch 6, batch 509, train_loss 4.48097848892\n",
      "epoch 6, batch 510, train_loss 4.45008230209\n",
      "epoch 6, batch 511, train_loss 4.46905946732\n",
      "epoch 6, batch 512, train_loss 3.46168899536\n",
      "epoch 6, batch 513, train_loss 4.89877223969\n",
      "epoch 6, batch 514, train_loss 4.83449745178\n",
      "epoch 6, batch 515, train_loss 4.73209285736\n",
      "epoch 6, batch 516, train_loss 2.95113348961\n",
      "epoch 6, batch 517, train_loss 3.13172531128\n",
      "epoch 6, batch 518, train_loss 3.03662347794\n",
      "epoch 6, batch 519, train_loss 2.72231674194\n",
      "epoch 6, batch 520, train_loss 3.91324138641\n",
      "epoch 6, batch 521, train_loss 3.06265807152\n",
      "epoch 6, batch 522, train_loss 2.64929580688\n",
      "epoch 6, batch 523, train_loss 2.65848278999\n",
      "epoch 6, batch 524, train_loss 2.5023522377\n",
      "epoch 6, batch 525, train_loss 2.33589482307\n",
      "epoch 6, batch 526, train_loss 2.42887210846\n",
      "epoch 6, batch 527, train_loss 2.45667648315\n",
      "epoch 6, batch 528, train_loss 2.70472955704\n",
      "epoch 6, batch 529, train_loss 2.59289836884\n",
      "epoch 6, batch 530, train_loss 2.07025384903\n",
      "epoch 6, batch 531, train_loss 1.79679870605\n",
      "epoch 6, batch 532, train_loss 2.72573590279\n",
      "epoch 6, batch 533, train_loss 1.71524083614\n",
      "epoch 6, batch 534, train_loss 1.76596772671\n",
      "epoch 6, batch 535, train_loss 3.27198243141\n",
      "epoch 6, batch 536, train_loss 3.49183344841\n",
      "epoch 6, batch 537, train_loss 3.36081051826\n",
      "epoch 6, batch 538, train_loss 3.51394581795\n",
      "epoch 6, batch 539, train_loss 3.82661390305\n",
      "epoch 6, batch 540, train_loss 4.15802049637\n",
      "epoch 7, batch 0, train_loss 3.46609044075\n",
      "epoch 7, batch 1, train_loss 3.85908460617\n",
      "epoch 7, batch 2, train_loss 3.21904063225\n",
      "epoch 7, batch 3, train_loss 3.45509576797\n",
      "epoch 7, batch 4, train_loss 3.24896836281\n",
      "epoch 7, batch 5, train_loss 3.30853199959\n",
      "epoch 7, batch 6, train_loss 4.35928630829\n",
      "epoch 7, batch 7, train_loss 3.26749610901\n",
      "epoch 7, batch 8, train_loss 2.89362072945\n",
      "epoch 7, batch 9, train_loss 3.92395710945\n",
      "epoch 7, batch 10, train_loss 3.12370276451\n",
      "epoch 7, batch 11, train_loss 3.19945502281\n",
      "epoch 7, batch 12, train_loss 2.91914367676\n",
      "epoch 7, batch 13, train_loss 3.43539357185\n",
      "epoch 7, batch 14, train_loss 2.82735610008\n",
      "epoch 7, batch 15, train_loss 3.56944918633\n",
      "epoch 7, batch 16, train_loss 3.10990095139\n",
      "epoch 7, batch 17, train_loss 2.95948982239\n",
      "epoch 7, batch 18, train_loss 2.59110307693\n",
      "epoch 7, batch 19, train_loss 2.13963651657\n",
      "epoch 7, batch 20, train_loss 3.01371717453\n",
      "epoch 7, batch 21, train_loss 2.94410514832\n",
      "epoch 7, batch 22, train_loss 3.44891381264\n",
      "epoch 7, batch 23, train_loss 2.9952814579\n",
      "epoch 7, batch 24, train_loss 2.98810195923\n",
      "epoch 7, batch 25, train_loss 3.45511317253\n",
      "epoch 7, batch 26, train_loss 3.39259576797\n",
      "epoch 7, batch 27, train_loss 3.28867793083\n",
      "epoch 7, batch 28, train_loss 3.59598970413\n",
      "epoch 7, batch 29, train_loss 3.49973940849\n",
      "epoch 7, batch 30, train_loss 3.59055757523\n",
      "epoch 7, batch 31, train_loss 3.0184173584\n",
      "epoch 7, batch 32, train_loss 2.76695990562\n",
      "epoch 7, batch 33, train_loss 4.68662166595\n",
      "epoch 7, batch 34, train_loss 4.76749706268\n",
      "epoch 7, batch 35, train_loss 3.36783838272\n",
      "epoch 7, batch 36, train_loss 3.39133763313\n",
      "epoch 7, batch 37, train_loss 3.33769154549\n",
      "epoch 7, batch 38, train_loss 3.47449064255\n",
      "epoch 7, batch 39, train_loss 3.95585918427\n",
      "epoch 7, batch 40, train_loss 3.02912449837\n",
      "epoch 7, batch 41, train_loss 3.30607199669\n",
      "epoch 7, batch 42, train_loss 3.3315615654\n",
      "epoch 7, batch 43, train_loss 3.10074973106\n",
      "epoch 7, batch 44, train_loss 3.74385142326\n",
      "epoch 7, batch 45, train_loss 3.35301947594\n",
      "epoch 7, batch 46, train_loss 3.30701541901\n",
      "epoch 7, batch 47, train_loss 3.4237241745\n",
      "epoch 7, batch 48, train_loss 3.23504400253\n",
      "epoch 7, batch 49, train_loss 3.38662934303\n",
      "epoch 7, batch 50, train_loss 3.27891111374\n",
      "epoch 7, batch 51, train_loss 2.9428396225\n",
      "epoch 7, batch 52, train_loss 3.40780711174\n",
      "epoch 7, batch 53, train_loss 3.26537394524\n",
      "epoch 7, batch 54, train_loss 3.00088834763\n",
      "epoch 7, batch 55, train_loss 3.04092693329\n",
      "epoch 7, batch 56, train_loss 2.93387269974\n",
      "epoch 7, batch 57, train_loss 3.43832850456\n",
      "epoch 7, batch 58, train_loss 3.23076057434\n",
      "epoch 7, batch 59, train_loss 3.19611167908\n",
      "epoch 7, batch 60, train_loss 3.39259028435\n",
      "epoch 7, batch 61, train_loss 1.96038961411\n",
      "epoch 7, batch 62, train_loss 2.96395540237\n",
      "epoch 7, batch 63, train_loss 3.10099458694\n",
      "epoch 7, batch 64, train_loss 3.32922887802\n",
      "epoch 7, batch 65, train_loss 3.65969514847\n",
      "epoch 7, batch 66, train_loss 3.42914056778\n",
      "epoch 7, batch 67, train_loss 2.20889544487\n",
      "epoch 7, batch 68, train_loss 3.12520074844\n",
      "epoch 7, batch 69, train_loss 2.4464392662\n",
      "epoch 7, batch 70, train_loss 4.05189275742\n",
      "epoch 7, batch 71, train_loss 4.09879732132\n",
      "epoch 7, batch 72, train_loss 2.90609049797\n",
      "epoch 7, batch 73, train_loss 3.33409738541\n",
      "epoch 7, batch 74, train_loss 3.9226808548\n",
      "epoch 7, batch 75, train_loss 3.38315296173\n",
      "epoch 7, batch 76, train_loss 3.66861462593\n",
      "epoch 7, batch 77, train_loss 4.39239692688\n",
      "epoch 7, batch 78, train_loss 3.19824194908\n",
      "epoch 7, batch 79, train_loss 3.22172617912\n",
      "epoch 7, batch 80, train_loss 3.05034637451\n",
      "epoch 7, batch 81, train_loss 3.17441129684\n",
      "epoch 7, batch 82, train_loss 3.07370996475\n",
      "epoch 7, batch 83, train_loss 3.19931745529\n",
      "epoch 7, batch 84, train_loss 2.95495247841\n",
      "epoch 7, batch 85, train_loss 2.98730540276\n",
      "epoch 7, batch 86, train_loss 3.50208592415\n",
      "epoch 7, batch 87, train_loss 2.69635128975\n",
      "epoch 7, batch 88, train_loss 3.00331950188\n",
      "epoch 7, batch 89, train_loss 3.30747151375\n",
      "epoch 7, batch 90, train_loss 2.99813961983\n",
      "epoch 7, batch 91, train_loss 3.05666017532\n",
      "epoch 7, batch 92, train_loss 3.53447079659\n",
      "epoch 7, batch 93, train_loss 4.3133263588\n",
      "epoch 7, batch 94, train_loss 3.06463313103\n",
      "epoch 7, batch 95, train_loss 3.13995313644\n",
      "epoch 7, batch 96, train_loss 2.49524259567\n",
      "epoch 7, batch 97, train_loss 3.13439416885\n",
      "epoch 7, batch 98, train_loss 3.05972075462\n",
      "epoch 7, batch 99, train_loss 3.31143569946\n",
      "epoch 7, batch 100, train_loss 3.08281683922\n",
      "epoch 7, batch 101, train_loss 3.24549484253\n",
      "epoch 7, batch 102, train_loss 3.05667638779\n",
      "epoch 7, batch 103, train_loss 4.33890724182\n",
      "epoch 7, batch 104, train_loss 3.57427191734\n",
      "epoch 7, batch 105, train_loss 3.38380718231\n",
      "epoch 7, batch 106, train_loss 3.78693246841\n",
      "epoch 7, batch 107, train_loss 3.60899972916\n",
      "epoch 7, batch 108, train_loss 3.21955823898\n",
      "epoch 7, batch 109, train_loss 3.48157405853\n",
      "epoch 7, batch 110, train_loss 3.29229450226\n",
      "epoch 7, batch 111, train_loss 3.70066761971\n",
      "epoch 7, batch 112, train_loss 3.66135883331\n",
      "epoch 7, batch 113, train_loss 3.05999875069\n",
      "epoch 7, batch 114, train_loss 3.63844656944\n",
      "epoch 7, batch 115, train_loss 4.40796089172\n",
      "epoch 7, batch 116, train_loss 4.29504203796\n",
      "epoch 7, batch 117, train_loss 4.43004465103\n",
      "epoch 7, batch 118, train_loss 3.83546566963\n",
      "epoch 7, batch 119, train_loss 2.53874468803\n",
      "epoch 7, batch 120, train_loss 3.37701034546\n",
      "epoch 7, batch 121, train_loss 3.21143579483\n",
      "epoch 7, batch 122, train_loss 3.42703938484\n",
      "epoch 7, batch 123, train_loss 3.18967151642\n",
      "epoch 7, batch 124, train_loss 2.88097763062\n",
      "epoch 7, batch 125, train_loss 2.84471273422\n",
      "epoch 7, batch 126, train_loss 2.8189227581\n",
      "epoch 7, batch 127, train_loss 2.89207410812\n",
      "epoch 7, batch 128, train_loss 2.75244021416\n",
      "epoch 7, batch 129, train_loss 3.14407062531\n",
      "epoch 7, batch 130, train_loss 3.08047008514\n",
      "epoch 7, batch 131, train_loss 3.44021844864\n",
      "epoch 7, batch 132, train_loss 3.17651724815\n",
      "epoch 7, batch 133, train_loss 2.40153551102\n",
      "epoch 7, batch 134, train_loss 2.43960142136\n",
      "epoch 7, batch 135, train_loss 4.2627363205\n",
      "epoch 7, batch 136, train_loss 3.64372301102\n",
      "epoch 7, batch 137, train_loss 2.80714535713\n",
      "epoch 7, batch 138, train_loss 2.65787696838\n",
      "epoch 7, batch 139, train_loss 3.18296122551\n",
      "epoch 7, batch 140, train_loss 2.65934062004\n",
      "epoch 7, batch 141, train_loss 2.99295163155\n",
      "epoch 7, batch 142, train_loss 4.25337505341\n",
      "epoch 7, batch 143, train_loss 2.67081141472\n",
      "epoch 7, batch 144, train_loss 2.16496992111\n",
      "epoch 7, batch 145, train_loss 3.18094778061\n",
      "epoch 7, batch 146, train_loss 3.35952806473\n",
      "epoch 7, batch 147, train_loss 3.09155058861\n",
      "epoch 7, batch 148, train_loss 2.98573684692\n",
      "epoch 7, batch 149, train_loss 3.43558192253\n",
      "epoch 7, batch 150, train_loss 3.55132102966\n",
      "epoch 7, batch 151, train_loss 2.17260718346\n",
      "epoch 7, batch 152, train_loss 2.43294119835\n",
      "epoch 7, batch 153, train_loss 4.15079975128\n",
      "epoch 7, batch 154, train_loss 3.27802634239\n",
      "epoch 7, batch 155, train_loss 2.85580396652\n",
      "epoch 7, batch 156, train_loss 3.06235527992\n",
      "epoch 7, batch 157, train_loss 2.63284659386\n",
      "epoch 7, batch 158, train_loss 2.65956449509\n",
      "epoch 7, batch 159, train_loss 2.85752892494\n",
      "epoch 7, batch 160, train_loss 2.97577428818\n",
      "epoch 7, batch 161, train_loss 3.51686263084\n",
      "epoch 7, batch 162, train_loss 3.47234988213\n",
      "epoch 7, batch 163, train_loss 4.96699476242\n",
      "epoch 7, batch 164, train_loss 2.3998477459\n",
      "epoch 7, batch 165, train_loss 2.34516429901\n",
      "epoch 7, batch 166, train_loss 2.69635844231\n",
      "epoch 7, batch 167, train_loss 2.72031664848\n",
      "epoch 7, batch 168, train_loss 2.96088385582\n",
      "epoch 7, batch 169, train_loss 2.73477244377\n",
      "epoch 7, batch 170, train_loss 3.27702999115\n",
      "epoch 7, batch 171, train_loss 2.6773211956\n",
      "epoch 7, batch 172, train_loss 3.30270147324\n",
      "epoch 7, batch 173, train_loss 2.20056557655\n",
      "epoch 7, batch 174, train_loss 3.79331970215\n",
      "epoch 7, batch 175, train_loss 3.2474925518\n",
      "epoch 7, batch 176, train_loss 3.14308547974\n",
      "epoch 7, batch 177, train_loss 2.93915629387\n",
      "epoch 7, batch 178, train_loss 2.86516880989\n",
      "epoch 7, batch 179, train_loss 3.1139550209\n",
      "epoch 7, batch 180, train_loss 2.99439120293\n",
      "epoch 7, batch 181, train_loss 3.64037179947\n",
      "epoch 7, batch 182, train_loss 3.11544632912\n",
      "epoch 7, batch 183, train_loss 3.78755903244\n",
      "epoch 7, batch 184, train_loss 2.46125245094\n",
      "epoch 7, batch 185, train_loss 2.83655405045\n",
      "epoch 7, batch 186, train_loss 3.48015618324\n",
      "epoch 7, batch 187, train_loss 2.53861260414\n",
      "epoch 7, batch 188, train_loss 2.87916064262\n",
      "epoch 7, batch 189, train_loss 2.85846471786\n",
      "epoch 7, batch 190, train_loss 3.64615082741\n",
      "epoch 7, batch 191, train_loss 3.50220990181\n",
      "epoch 7, batch 192, train_loss 4.49534034729\n",
      "epoch 7, batch 193, train_loss 4.19729423523\n",
      "epoch 7, batch 194, train_loss 4.81979656219\n",
      "epoch 7, batch 195, train_loss 4.90703392029\n",
      "epoch 7, batch 196, train_loss 3.28178167343\n",
      "epoch 7, batch 197, train_loss 2.62090659142\n",
      "epoch 7, batch 198, train_loss 2.3983771801\n",
      "epoch 7, batch 199, train_loss 3.4607963562\n",
      "epoch 7, batch 200, train_loss 2.88794398308\n",
      "epoch 7, batch 201, train_loss 3.21318912506\n",
      "epoch 7, batch 202, train_loss 3.65817952156\n",
      "epoch 7, batch 203, train_loss 3.79256296158\n",
      "epoch 7, batch 204, train_loss 3.76003980637\n",
      "epoch 7, batch 205, train_loss 3.23018741608\n",
      "epoch 7, batch 206, train_loss 4.30825185776\n",
      "epoch 7, batch 207, train_loss 3.61568975449\n",
      "epoch 7, batch 208, train_loss 4.34308815002\n",
      "epoch 7, batch 209, train_loss 4.05933952332\n",
      "epoch 7, batch 210, train_loss 2.22285580635\n",
      "epoch 7, batch 211, train_loss 3.00977730751\n",
      "epoch 7, batch 212, train_loss 3.935120821\n",
      "epoch 7, batch 213, train_loss 3.26230812073\n",
      "epoch 7, batch 214, train_loss 3.10093283653\n",
      "epoch 7, batch 215, train_loss 2.30015397072\n",
      "epoch 7, batch 216, train_loss 3.47927641869\n",
      "epoch 7, batch 217, train_loss 2.90443134308\n",
      "epoch 7, batch 218, train_loss 3.38935399055\n",
      "epoch 7, batch 219, train_loss 3.51576924324\n",
      "epoch 7, batch 220, train_loss 3.54593014717\n",
      "epoch 7, batch 221, train_loss 3.7053194046\n",
      "epoch 7, batch 222, train_loss 2.39658713341\n",
      "epoch 7, batch 223, train_loss 2.39461827278\n",
      "epoch 7, batch 224, train_loss 2.74426627159\n",
      "epoch 7, batch 225, train_loss 2.88405847549\n",
      "epoch 7, batch 226, train_loss 3.43456459045\n",
      "epoch 7, batch 227, train_loss 3.17858719826\n",
      "epoch 7, batch 228, train_loss 3.34433960915\n",
      "epoch 7, batch 229, train_loss 3.16569924355\n",
      "epoch 7, batch 230, train_loss 3.02278923988\n",
      "epoch 7, batch 231, train_loss 2.71001505852\n",
      "epoch 7, batch 232, train_loss 3.38173031807\n",
      "epoch 7, batch 233, train_loss 3.64326930046\n",
      "epoch 7, batch 234, train_loss 3.19255328178\n",
      "epoch 7, batch 235, train_loss 3.7818274498\n",
      "epoch 7, batch 236, train_loss 3.27706599236\n",
      "epoch 7, batch 237, train_loss 3.19593858719\n",
      "epoch 7, batch 238, train_loss 3.21509790421\n",
      "epoch 7, batch 239, train_loss 3.0297191143\n",
      "epoch 7, batch 240, train_loss 3.48683214188\n",
      "epoch 7, batch 241, train_loss 3.4055583477\n",
      "epoch 7, batch 242, train_loss 3.22925496101\n",
      "epoch 7, batch 243, train_loss 3.42675113678\n",
      "epoch 7, batch 244, train_loss 3.55939483643\n",
      "epoch 7, batch 245, train_loss 3.31443047523\n",
      "epoch 7, batch 246, train_loss 3.2404191494\n",
      "epoch 7, batch 247, train_loss 3.43334770203\n",
      "epoch 7, batch 248, train_loss 3.16846227646\n",
      "epoch 7, batch 249, train_loss 3.8925306797\n",
      "epoch 7, batch 250, train_loss 3.66240382195\n",
      "epoch 7, batch 251, train_loss 4.01858663559\n",
      "epoch 7, batch 252, train_loss 2.76177597046\n",
      "epoch 7, batch 253, train_loss 2.28593993187\n",
      "epoch 7, batch 254, train_loss 2.94760942459\n",
      "epoch 7, batch 255, train_loss 2.27869033813\n",
      "epoch 7, batch 256, train_loss 2.6683204174\n",
      "epoch 7, batch 257, train_loss 3.15067672729\n",
      "epoch 7, batch 258, train_loss 2.50738120079\n",
      "epoch 7, batch 259, train_loss 2.44226574898\n",
      "epoch 7, batch 260, train_loss 2.18057084084\n",
      "epoch 7, batch 261, train_loss 3.58164906502\n",
      "epoch 7, batch 262, train_loss 4.18916988373\n",
      "epoch 7, batch 263, train_loss 3.25006651878\n",
      "epoch 7, batch 264, train_loss 2.95614147186\n",
      "epoch 7, batch 265, train_loss 3.62556838989\n",
      "epoch 7, batch 266, train_loss 3.67632842064\n",
      "epoch 7, batch 267, train_loss 2.89156985283\n",
      "epoch 7, batch 268, train_loss 2.4230222702\n",
      "epoch 7, batch 269, train_loss 2.49037313461\n",
      "epoch 7, batch 270, train_loss 2.80025959015\n",
      "epoch 7, batch 271, train_loss 3.16078090668\n",
      "epoch 7, batch 272, train_loss 3.17908978462\n",
      "epoch 7, batch 273, train_loss 3.30402183533\n",
      "epoch 7, batch 274, train_loss 3.09822845459\n",
      "epoch 7, batch 275, train_loss 3.11664390564\n",
      "epoch 7, batch 276, train_loss 2.88523983955\n",
      "epoch 7, batch 277, train_loss 3.73030495644\n",
      "epoch 7, batch 278, train_loss 2.85738444328\n",
      "epoch 7, batch 279, train_loss 3.42021536827\n",
      "epoch 7, batch 280, train_loss 3.40469074249\n",
      "epoch 7, batch 281, train_loss 3.18411827087\n",
      "epoch 7, batch 282, train_loss 2.80937004089\n",
      "epoch 7, batch 283, train_loss 4.47619771957\n",
      "epoch 7, batch 284, train_loss 3.34613490105\n",
      "epoch 7, batch 285, train_loss 2.05376791954\n",
      "epoch 7, batch 286, train_loss 2.30240774155\n",
      "epoch 7, batch 287, train_loss 2.4110519886\n",
      "epoch 7, batch 288, train_loss 3.07263541222\n",
      "epoch 7, batch 289, train_loss 3.12212920189\n",
      "epoch 7, batch 290, train_loss 2.9768986702\n",
      "epoch 7, batch 291, train_loss 2.82940721512\n",
      "epoch 7, batch 292, train_loss 2.3171145916\n",
      "epoch 7, batch 293, train_loss 3.45055317879\n",
      "epoch 7, batch 294, train_loss 3.00197792053\n",
      "epoch 7, batch 295, train_loss 2.712900877\n",
      "epoch 7, batch 296, train_loss 3.0204179287\n",
      "epoch 7, batch 297, train_loss 2.66962265968\n",
      "epoch 7, batch 298, train_loss 2.92453169823\n",
      "epoch 7, batch 299, train_loss 3.51277804375\n",
      "epoch 7, batch 300, train_loss 3.21659803391\n",
      "epoch 7, batch 301, train_loss 4.2292265892\n",
      "epoch 7, batch 302, train_loss 4.24904298782\n",
      "epoch 7, batch 303, train_loss 3.89908123016\n",
      "epoch 7, batch 304, train_loss 4.59713459015\n",
      "epoch 7, batch 305, train_loss 4.54350996017\n",
      "epoch 7, batch 306, train_loss 3.14778065681\n",
      "epoch 7, batch 307, train_loss 2.59545230865\n",
      "epoch 7, batch 308, train_loss 3.04877996445\n",
      "epoch 7, batch 309, train_loss 3.21864962578\n",
      "epoch 7, batch 310, train_loss 3.00977540016\n",
      "epoch 7, batch 311, train_loss 3.13744688034\n",
      "epoch 7, batch 312, train_loss 3.32088041306\n",
      "epoch 7, batch 313, train_loss 3.02111315727\n",
      "epoch 7, batch 314, train_loss 3.38521933556\n",
      "epoch 7, batch 315, train_loss 2.82967686653\n",
      "epoch 7, batch 316, train_loss 2.99318003654\n",
      "epoch 7, batch 317, train_loss 3.49017739296\n",
      "epoch 7, batch 318, train_loss 2.59302568436\n",
      "epoch 7, batch 319, train_loss 4.3879532814\n",
      "epoch 7, batch 320, train_loss 3.21642494202\n",
      "epoch 7, batch 321, train_loss 4.48933076859\n",
      "epoch 7, batch 322, train_loss 2.48527646065\n",
      "epoch 7, batch 323, train_loss 2.34834361076\n",
      "epoch 7, batch 324, train_loss 3.52693390846\n",
      "epoch 7, batch 325, train_loss 3.16577267647\n",
      "epoch 7, batch 326, train_loss 3.15765523911\n",
      "epoch 7, batch 327, train_loss 3.13892388344\n",
      "epoch 7, batch 328, train_loss 3.0185444355\n",
      "epoch 7, batch 329, train_loss 2.67510867119\n",
      "epoch 7, batch 330, train_loss 4.04452180862\n",
      "epoch 7, batch 331, train_loss 3.70435810089\n",
      "epoch 7, batch 332, train_loss 2.50669121742\n",
      "epoch 7, batch 333, train_loss 3.60922574997\n",
      "epoch 7, batch 334, train_loss 3.30882954597\n",
      "epoch 7, batch 335, train_loss 3.21365427971\n",
      "epoch 7, batch 336, train_loss 3.79923152924\n",
      "epoch 7, batch 337, train_loss 2.31965994835\n",
      "epoch 7, batch 338, train_loss 3.16911840439\n",
      "epoch 7, batch 339, train_loss 2.94943761826\n",
      "epoch 7, batch 340, train_loss 3.14403820038\n",
      "epoch 7, batch 341, train_loss 3.07603216171\n",
      "epoch 7, batch 342, train_loss 3.12586140633\n",
      "epoch 7, batch 343, train_loss 3.02252817154\n",
      "epoch 7, batch 344, train_loss 4.01769685745\n",
      "epoch 7, batch 345, train_loss 3.41470575333\n",
      "epoch 7, batch 346, train_loss 3.1650788784\n",
      "epoch 7, batch 347, train_loss 2.95760416985\n",
      "epoch 7, batch 348, train_loss 3.20745444298\n",
      "epoch 7, batch 349, train_loss 4.26923036575\n",
      "epoch 7, batch 350, train_loss 4.24177265167\n",
      "epoch 7, batch 351, train_loss 4.01605367661\n",
      "epoch 7, batch 352, train_loss 4.07336330414\n",
      "epoch 7, batch 353, train_loss 3.31246900558\n",
      "epoch 7, batch 354, train_loss 2.6882019043\n",
      "epoch 7, batch 355, train_loss 2.99263095856\n",
      "epoch 7, batch 356, train_loss 2.97078871727\n",
      "epoch 7, batch 357, train_loss 2.91093921661\n",
      "epoch 7, batch 358, train_loss 2.46351385117\n",
      "epoch 7, batch 359, train_loss 3.46429371834\n",
      "epoch 7, batch 360, train_loss 2.95098876953\n",
      "epoch 7, batch 361, train_loss 2.60804080963\n",
      "epoch 7, batch 362, train_loss 4.38906574249\n",
      "epoch 7, batch 363, train_loss 3.22321557999\n",
      "epoch 7, batch 364, train_loss 3.12078523636\n",
      "epoch 7, batch 365, train_loss 4.04395055771\n",
      "epoch 7, batch 366, train_loss 4.17250108719\n",
      "epoch 7, batch 367, train_loss 4.92760753632\n",
      "epoch 7, batch 368, train_loss 2.56686019897\n",
      "epoch 7, batch 369, train_loss 3.51941156387\n",
      "epoch 7, batch 370, train_loss 3.91026830673\n",
      "epoch 7, batch 371, train_loss 4.89344072342\n",
      "epoch 7, batch 372, train_loss 2.36063027382\n",
      "epoch 7, batch 373, train_loss 2.53416156769\n",
      "epoch 7, batch 374, train_loss 2.86116266251\n",
      "epoch 7, batch 375, train_loss 3.26589274406\n",
      "epoch 7, batch 376, train_loss 2.51357030869\n",
      "epoch 7, batch 377, train_loss 2.92547941208\n",
      "epoch 7, batch 378, train_loss 3.65139269829\n",
      "epoch 7, batch 379, train_loss 2.80970644951\n",
      "epoch 7, batch 380, train_loss 2.82758760452\n",
      "epoch 7, batch 381, train_loss 2.92715597153\n",
      "epoch 7, batch 382, train_loss 3.84219503403\n",
      "epoch 7, batch 383, train_loss 3.38540959358\n",
      "epoch 7, batch 384, train_loss 3.04049539566\n",
      "epoch 7, batch 385, train_loss 4.46940946579\n",
      "epoch 7, batch 386, train_loss 2.74193286896\n",
      "epoch 7, batch 387, train_loss 4.66029930115\n",
      "epoch 7, batch 388, train_loss 2.74515080452\n",
      "epoch 7, batch 389, train_loss 4.35841751099\n",
      "epoch 7, batch 390, train_loss 4.33546686172\n",
      "epoch 7, batch 391, train_loss 4.36595344543\n",
      "epoch 7, batch 392, train_loss 4.11668348312\n",
      "epoch 7, batch 393, train_loss 3.35002613068\n",
      "epoch 7, batch 394, train_loss 3.99211764336\n",
      "epoch 7, batch 395, train_loss 2.92884659767\n",
      "epoch 7, batch 396, train_loss 4.25088977814\n",
      "epoch 7, batch 397, train_loss 4.36566257477\n",
      "epoch 7, batch 398, train_loss 3.26577353477\n",
      "epoch 7, batch 399, train_loss 3.50983548164\n",
      "epoch 7, batch 400, train_loss 4.51081848145\n",
      "epoch 7, batch 401, train_loss 3.72921729088\n",
      "epoch 7, batch 402, train_loss 3.28992605209\n",
      "epoch 7, batch 403, train_loss 3.23319721222\n",
      "epoch 7, batch 404, train_loss 4.09167718887\n",
      "epoch 7, batch 405, train_loss 3.5758395195\n",
      "epoch 7, batch 406, train_loss 3.42356920242\n",
      "epoch 7, batch 407, train_loss 3.21906995773\n",
      "epoch 7, batch 408, train_loss 3.07680845261\n",
      "epoch 7, batch 409, train_loss 2.57313966751\n",
      "epoch 7, batch 410, train_loss 4.22049379349\n",
      "epoch 7, batch 411, train_loss 3.30044317245\n",
      "epoch 7, batch 412, train_loss 2.80300569534\n",
      "epoch 7, batch 413, train_loss 3.15931987762\n",
      "epoch 7, batch 414, train_loss 3.69739961624\n",
      "epoch 7, batch 415, train_loss 3.75746393204\n",
      "epoch 7, batch 416, train_loss 3.50622725487\n",
      "epoch 7, batch 417, train_loss 3.03657770157\n",
      "epoch 7, batch 418, train_loss 3.4510383606\n",
      "epoch 7, batch 419, train_loss 3.07057571411\n",
      "epoch 7, batch 420, train_loss 2.4858481884\n",
      "epoch 7, batch 421, train_loss 4.34247922897\n",
      "epoch 7, batch 422, train_loss 3.04824733734\n",
      "epoch 7, batch 423, train_loss 3.17164587975\n",
      "epoch 7, batch 424, train_loss 4.27909851074\n",
      "epoch 7, batch 425, train_loss 3.56684803963\n",
      "epoch 7, batch 426, train_loss 4.75849151611\n",
      "epoch 7, batch 427, train_loss 3.86661505699\n",
      "epoch 7, batch 428, train_loss 2.65226578712\n",
      "epoch 7, batch 429, train_loss 3.68546819687\n",
      "epoch 7, batch 430, train_loss 3.74645280838\n",
      "epoch 7, batch 431, train_loss 3.3359401226\n",
      "epoch 7, batch 432, train_loss 3.539270401\n",
      "epoch 7, batch 433, train_loss 2.92981314659\n",
      "epoch 7, batch 434, train_loss 3.44634532928\n",
      "epoch 7, batch 435, train_loss 3.24170255661\n",
      "epoch 7, batch 436, train_loss 3.59900736809\n",
      "epoch 7, batch 437, train_loss 4.06567573547\n",
      "epoch 7, batch 438, train_loss 3.70290231705\n",
      "epoch 7, batch 439, train_loss 3.46140933037\n",
      "epoch 7, batch 440, train_loss 4.791867733\n",
      "epoch 7, batch 441, train_loss 4.91986083984\n",
      "epoch 7, batch 442, train_loss 4.06362056732\n",
      "epoch 7, batch 443, train_loss 2.49512171745\n",
      "epoch 7, batch 444, train_loss 3.02320861816\n",
      "epoch 7, batch 445, train_loss 4.64074230194\n",
      "epoch 7, batch 446, train_loss 3.38237690926\n",
      "epoch 7, batch 447, train_loss 3.07244682312\n",
      "epoch 7, batch 448, train_loss 4.5238161087\n",
      "epoch 7, batch 449, train_loss 3.8812520504\n",
      "epoch 7, batch 450, train_loss 3.21208405495\n",
      "epoch 7, batch 451, train_loss 2.96025371552\n",
      "epoch 7, batch 452, train_loss 2.70952367783\n",
      "epoch 7, batch 453, train_loss 5.22779035568\n",
      "epoch 7, batch 454, train_loss 5.10063409805\n",
      "epoch 7, batch 455, train_loss 2.58359146118\n",
      "epoch 7, batch 456, train_loss 3.28404402733\n",
      "epoch 7, batch 457, train_loss 3.53322815895\n",
      "epoch 7, batch 458, train_loss 3.14776420593\n",
      "epoch 7, batch 459, train_loss 3.43921232224\n",
      "epoch 7, batch 460, train_loss 3.94590997696\n",
      "epoch 7, batch 461, train_loss 3.43451833725\n",
      "epoch 7, batch 462, train_loss 3.10103058815\n",
      "epoch 7, batch 463, train_loss 3.25716733932\n",
      "epoch 7, batch 464, train_loss 3.54530668259\n",
      "epoch 7, batch 465, train_loss 2.92170548439\n",
      "epoch 7, batch 466, train_loss 3.71466374397\n",
      "epoch 7, batch 467, train_loss 3.53000545502\n",
      "epoch 7, batch 468, train_loss 3.6739449501\n",
      "epoch 7, batch 469, train_loss 3.23101115227\n",
      "epoch 7, batch 470, train_loss 3.09348940849\n",
      "epoch 7, batch 471, train_loss 3.17034649849\n",
      "epoch 7, batch 472, train_loss 3.03501605988\n",
      "epoch 7, batch 473, train_loss 3.75064110756\n",
      "epoch 7, batch 474, train_loss 4.34886074066\n",
      "epoch 7, batch 475, train_loss 3.04719924927\n",
      "epoch 7, batch 476, train_loss 2.55626487732\n",
      "epoch 7, batch 477, train_loss 2.65730237961\n",
      "epoch 7, batch 478, train_loss 2.492872715\n",
      "epoch 7, batch 479, train_loss 3.00487041473\n",
      "epoch 7, batch 480, train_loss 3.21326780319\n",
      "epoch 7, batch 481, train_loss 4.82149362564\n",
      "epoch 7, batch 482, train_loss 2.91434836388\n",
      "epoch 7, batch 483, train_loss 3.55262637138\n",
      "epoch 7, batch 484, train_loss 3.2114546299\n",
      "epoch 7, batch 485, train_loss 2.42524790764\n",
      "epoch 7, batch 486, train_loss 2.43331575394\n",
      "epoch 7, batch 487, train_loss 2.59202241898\n",
      "epoch 7, batch 488, train_loss 2.99709606171\n",
      "epoch 7, batch 489, train_loss 3.11757564545\n",
      "epoch 7, batch 490, train_loss 3.04355335236\n",
      "epoch 7, batch 491, train_loss 2.96428132057\n",
      "epoch 7, batch 492, train_loss 3.30692672729\n",
      "epoch 7, batch 493, train_loss 2.88591241837\n",
      "epoch 7, batch 494, train_loss 2.98606109619\n",
      "epoch 7, batch 495, train_loss 3.05669021606\n",
      "epoch 7, batch 496, train_loss 2.68436741829\n",
      "epoch 7, batch 497, train_loss 2.86194419861\n",
      "epoch 7, batch 498, train_loss 3.00610947609\n",
      "epoch 7, batch 499, train_loss 2.960303545\n",
      "epoch 7, batch 500, train_loss 3.43311452866\n",
      "epoch 7, batch 501, train_loss 3.0727353096\n",
      "epoch 7, batch 502, train_loss 4.3887720108\n",
      "epoch 7, batch 503, train_loss 4.32659196854\n",
      "epoch 7, batch 504, train_loss 3.77076029778\n",
      "epoch 7, batch 505, train_loss 3.56365299225\n",
      "epoch 7, batch 506, train_loss 3.22887849808\n",
      "epoch 7, batch 507, train_loss 4.25257635117\n",
      "epoch 7, batch 508, train_loss 3.03654742241\n",
      "epoch 7, batch 509, train_loss 4.40862226486\n",
      "epoch 7, batch 510, train_loss 4.37909984589\n",
      "epoch 7, batch 511, train_loss 4.39391899109\n",
      "epoch 7, batch 512, train_loss 3.41076207161\n",
      "epoch 7, batch 513, train_loss 4.82533216476\n",
      "epoch 7, batch 514, train_loss 4.76308107376\n",
      "epoch 7, batch 515, train_loss 4.65704107285\n",
      "epoch 7, batch 516, train_loss 2.90489315987\n",
      "epoch 7, batch 517, train_loss 3.09006142616\n",
      "epoch 7, batch 518, train_loss 2.99333429337\n",
      "epoch 7, batch 519, train_loss 2.67810153961\n",
      "epoch 7, batch 520, train_loss 3.85159945488\n",
      "epoch 7, batch 521, train_loss 3.01272439957\n",
      "epoch 7, batch 522, train_loss 2.61754751205\n",
      "epoch 7, batch 523, train_loss 2.61481118202\n",
      "epoch 7, batch 524, train_loss 2.46440458298\n",
      "epoch 7, batch 525, train_loss 2.2944457531\n",
      "epoch 7, batch 526, train_loss 2.38908672333\n",
      "epoch 7, batch 527, train_loss 2.42028427124\n",
      "epoch 7, batch 528, train_loss 2.66609096527\n",
      "epoch 7, batch 529, train_loss 2.55312681198\n",
      "epoch 7, batch 530, train_loss 2.03938627243\n",
      "epoch 7, batch 531, train_loss 1.78006374836\n",
      "epoch 7, batch 532, train_loss 2.6744685173\n",
      "epoch 7, batch 533, train_loss 1.68854522705\n",
      "epoch 7, batch 534, train_loss 1.74206650257\n",
      "epoch 7, batch 535, train_loss 3.21499681473\n",
      "epoch 7, batch 536, train_loss 3.44075012207\n",
      "epoch 7, batch 537, train_loss 3.31286978722\n",
      "epoch 7, batch 538, train_loss 3.45193839073\n",
      "epoch 7, batch 539, train_loss 3.74221944809\n",
      "epoch 7, batch 540, train_loss 4.08074331284\n",
      "epoch 8, batch 0, train_loss 3.41930651665\n",
      "epoch 8, batch 1, train_loss 3.79501414299\n",
      "epoch 8, batch 2, train_loss 3.15202403069\n",
      "epoch 8, batch 3, train_loss 3.39647912979\n",
      "epoch 8, batch 4, train_loss 3.17067670822\n",
      "epoch 8, batch 5, train_loss 3.23291540146\n",
      "epoch 8, batch 6, train_loss 4.23276138306\n",
      "epoch 8, batch 7, train_loss 3.19562649727\n",
      "epoch 8, batch 8, train_loss 2.8364982605\n",
      "epoch 8, batch 9, train_loss 3.83614754677\n",
      "epoch 8, batch 10, train_loss 3.06375718117\n",
      "epoch 8, batch 11, train_loss 3.14408874512\n",
      "epoch 8, batch 12, train_loss 2.86414074898\n",
      "epoch 8, batch 13, train_loss 3.363717556\n",
      "epoch 8, batch 14, train_loss 2.77233958244\n",
      "epoch 8, batch 15, train_loss 3.5077521801\n",
      "epoch 8, batch 16, train_loss 3.04841661453\n",
      "epoch 8, batch 17, train_loss 2.89817595482\n",
      "epoch 8, batch 18, train_loss 2.54743051529\n",
      "epoch 8, batch 19, train_loss 2.10054254532\n",
      "epoch 8, batch 20, train_loss 2.96088957787\n",
      "epoch 8, batch 21, train_loss 2.90943264961\n",
      "epoch 8, batch 22, train_loss 3.39189887047\n",
      "epoch 8, batch 23, train_loss 2.94888830185\n",
      "epoch 8, batch 24, train_loss 2.9424033165\n",
      "epoch 8, batch 25, train_loss 3.39897656441\n",
      "epoch 8, batch 26, train_loss 3.33735990524\n",
      "epoch 8, batch 27, train_loss 3.240899086\n",
      "epoch 8, batch 28, train_loss 3.54224491119\n",
      "epoch 8, batch 29, train_loss 3.44433927536\n",
      "epoch 8, batch 30, train_loss 3.53289723396\n",
      "epoch 8, batch 31, train_loss 2.97132945061\n",
      "epoch 8, batch 32, train_loss 2.72938251495\n",
      "epoch 8, batch 33, train_loss 4.6030664444\n",
      "epoch 8, batch 34, train_loss 4.69072151184\n",
      "epoch 8, batch 35, train_loss 3.31523537636\n",
      "epoch 8, batch 36, train_loss 3.33915019035\n",
      "epoch 8, batch 37, train_loss 3.28054738045\n",
      "epoch 8, batch 38, train_loss 3.42719388008\n",
      "epoch 8, batch 39, train_loss 3.9034473896\n",
      "epoch 8, batch 40, train_loss 2.9804816246\n",
      "epoch 8, batch 41, train_loss 3.26279139519\n",
      "epoch 8, batch 42, train_loss 3.28148436546\n",
      "epoch 8, batch 43, train_loss 3.05137109756\n",
      "epoch 8, batch 44, train_loss 3.69607543945\n",
      "epoch 8, batch 45, train_loss 3.30965900421\n",
      "epoch 8, batch 46, train_loss 3.26306200027\n",
      "epoch 8, batch 47, train_loss 3.36702656746\n",
      "epoch 8, batch 48, train_loss 3.18349099159\n",
      "epoch 8, batch 49, train_loss 3.32932639122\n",
      "epoch 8, batch 50, train_loss 3.23324394226\n",
      "epoch 8, batch 51, train_loss 2.89362716675\n",
      "epoch 8, batch 52, train_loss 3.35273861885\n",
      "epoch 8, batch 53, train_loss 3.20881485939\n",
      "epoch 8, batch 54, train_loss 2.95505332947\n",
      "epoch 8, batch 55, train_loss 2.9947078228\n",
      "epoch 8, batch 56, train_loss 2.88555622101\n",
      "epoch 8, batch 57, train_loss 3.37601613998\n",
      "epoch 8, batch 58, train_loss 3.17909932137\n",
      "epoch 8, batch 59, train_loss 3.13804244995\n",
      "epoch 8, batch 60, train_loss 3.335698843\n",
      "epoch 8, batch 61, train_loss 1.92287409306\n",
      "epoch 8, batch 62, train_loss 2.90791392326\n",
      "epoch 8, batch 63, train_loss 3.04967355728\n",
      "epoch 8, batch 64, train_loss 3.28373789787\n",
      "epoch 8, batch 65, train_loss 3.60637426376\n",
      "epoch 8, batch 66, train_loss 3.37152981758\n",
      "epoch 8, batch 67, train_loss 2.17545580864\n",
      "epoch 8, batch 68, train_loss 3.069211483\n",
      "epoch 8, batch 69, train_loss 2.40395593643\n",
      "epoch 8, batch 70, train_loss 3.97157287598\n",
      "epoch 8, batch 71, train_loss 4.02350997925\n",
      "epoch 8, batch 72, train_loss 2.8551735878\n",
      "epoch 8, batch 73, train_loss 3.27845191956\n",
      "epoch 8, batch 74, train_loss 3.84884214401\n",
      "epoch 8, batch 75, train_loss 3.31992053986\n",
      "epoch 8, batch 76, train_loss 3.60816574097\n",
      "epoch 8, batch 77, train_loss 4.30949306488\n",
      "epoch 8, batch 78, train_loss 3.14536094666\n",
      "epoch 8, batch 79, train_loss 3.16759443283\n",
      "epoch 8, batch 80, train_loss 2.99734520912\n",
      "epoch 8, batch 81, train_loss 3.12695908546\n",
      "epoch 8, batch 82, train_loss 3.02503228188\n",
      "epoch 8, batch 83, train_loss 3.15772867203\n",
      "epoch 8, batch 84, train_loss 2.90894174576\n",
      "epoch 8, batch 85, train_loss 2.93634033203\n",
      "epoch 8, batch 86, train_loss 3.45125365257\n",
      "epoch 8, batch 87, train_loss 2.65107822418\n",
      "epoch 8, batch 88, train_loss 2.95973443985\n",
      "epoch 8, batch 89, train_loss 3.26035237312\n",
      "epoch 8, batch 90, train_loss 2.95741391182\n",
      "epoch 8, batch 91, train_loss 3.0084528923\n",
      "epoch 8, batch 92, train_loss 3.47319245338\n",
      "epoch 8, batch 93, train_loss 4.23725032806\n",
      "epoch 8, batch 94, train_loss 3.01855564117\n",
      "epoch 8, batch 95, train_loss 3.09041023254\n",
      "epoch 8, batch 96, train_loss 2.45675373077\n",
      "epoch 8, batch 97, train_loss 3.08142232895\n",
      "epoch 8, batch 98, train_loss 3.01037573814\n",
      "epoch 8, batch 99, train_loss 3.25266218185\n",
      "epoch 8, batch 100, train_loss 3.03073811531\n",
      "epoch 8, batch 101, train_loss 3.18840718269\n",
      "epoch 8, batch 102, train_loss 3.00529813766\n",
      "epoch 8, batch 103, train_loss 4.27201938629\n",
      "epoch 8, batch 104, train_loss 3.52189803123\n",
      "epoch 8, batch 105, train_loss 3.33411741257\n",
      "epoch 8, batch 106, train_loss 3.73223114014\n",
      "epoch 8, batch 107, train_loss 3.55759048462\n",
      "epoch 8, batch 108, train_loss 3.16874980927\n",
      "epoch 8, batch 109, train_loss 3.43208503723\n",
      "epoch 8, batch 110, train_loss 3.24431228638\n",
      "epoch 8, batch 111, train_loss 3.65346288681\n",
      "epoch 8, batch 112, train_loss 3.60839104652\n",
      "epoch 8, batch 113, train_loss 3.0118522644\n",
      "epoch 8, batch 114, train_loss 3.59218215942\n",
      "epoch 8, batch 115, train_loss 4.33391952515\n",
      "epoch 8, batch 116, train_loss 4.21885538101\n",
      "epoch 8, batch 117, train_loss 4.36122274399\n",
      "epoch 8, batch 118, train_loss 3.77882599831\n",
      "epoch 8, batch 119, train_loss 2.50213956833\n",
      "epoch 8, batch 120, train_loss 3.33008599281\n",
      "epoch 8, batch 121, train_loss 3.15489125252\n",
      "epoch 8, batch 122, train_loss 3.37363791466\n",
      "epoch 8, batch 123, train_loss 3.13967370987\n",
      "epoch 8, batch 124, train_loss 2.83438897133\n",
      "epoch 8, batch 125, train_loss 2.80172109604\n",
      "epoch 8, batch 126, train_loss 2.77505970001\n",
      "epoch 8, batch 127, train_loss 2.84443902969\n",
      "epoch 8, batch 128, train_loss 2.6956679821\n",
      "epoch 8, batch 129, train_loss 3.09241557121\n",
      "epoch 8, batch 130, train_loss 3.0314810276\n",
      "epoch 8, batch 131, train_loss 3.39286494255\n",
      "epoch 8, batch 132, train_loss 3.13474774361\n",
      "epoch 8, batch 133, train_loss 2.36550951004\n",
      "epoch 8, batch 134, train_loss 2.39943170547\n",
      "epoch 8, batch 135, train_loss 4.18685865402\n",
      "epoch 8, batch 136, train_loss 3.59013366699\n",
      "epoch 8, batch 137, train_loss 2.76212000847\n",
      "epoch 8, batch 138, train_loss 2.61235642433\n",
      "epoch 8, batch 139, train_loss 3.13613390923\n",
      "epoch 8, batch 140, train_loss 2.61840462685\n",
      "epoch 8, batch 141, train_loss 2.94274973869\n",
      "epoch 8, batch 142, train_loss 4.17723274231\n",
      "epoch 8, batch 143, train_loss 2.62561106682\n",
      "epoch 8, batch 144, train_loss 2.13463783264\n",
      "epoch 8, batch 145, train_loss 3.13083386421\n",
      "epoch 8, batch 146, train_loss 3.31208729744\n",
      "epoch 8, batch 147, train_loss 3.04336071014\n",
      "epoch 8, batch 148, train_loss 2.94132614136\n",
      "epoch 8, batch 149, train_loss 3.37812590599\n",
      "epoch 8, batch 150, train_loss 3.49981021881\n",
      "epoch 8, batch 151, train_loss 2.14530825615\n",
      "epoch 8, batch 152, train_loss 2.39351916313\n",
      "epoch 8, batch 153, train_loss 4.08033895493\n",
      "epoch 8, batch 154, train_loss 3.22653222084\n",
      "epoch 8, batch 155, train_loss 2.81119322777\n",
      "epoch 8, batch 156, train_loss 3.01481366158\n",
      "epoch 8, batch 157, train_loss 2.59680104256\n",
      "epoch 8, batch 158, train_loss 2.61663889885\n",
      "epoch 8, batch 159, train_loss 2.81946158409\n",
      "epoch 8, batch 160, train_loss 2.9278626442\n",
      "epoch 8, batch 161, train_loss 3.46659398079\n",
      "epoch 8, batch 162, train_loss 3.43039655685\n",
      "epoch 8, batch 163, train_loss 4.90208292007\n",
      "epoch 8, batch 164, train_loss 2.36785984039\n",
      "epoch 8, batch 165, train_loss 2.31506752968\n",
      "epoch 8, batch 166, train_loss 2.64886713028\n",
      "epoch 8, batch 167, train_loss 2.67702555656\n",
      "epoch 8, batch 168, train_loss 2.90891242027\n",
      "epoch 8, batch 169, train_loss 2.69314408302\n",
      "epoch 8, batch 170, train_loss 3.22535085678\n",
      "epoch 8, batch 171, train_loss 2.63064622879\n",
      "epoch 8, batch 172, train_loss 3.2427251339\n",
      "epoch 8, batch 173, train_loss 2.1674888134\n",
      "epoch 8, batch 174, train_loss 3.73108816147\n",
      "epoch 8, batch 175, train_loss 3.19688987732\n",
      "epoch 8, batch 176, train_loss 3.09353232384\n",
      "epoch 8, batch 177, train_loss 2.89414572716\n",
      "epoch 8, batch 178, train_loss 2.82346487045\n",
      "epoch 8, batch 179, train_loss 3.0734500885\n",
      "epoch 8, batch 180, train_loss 2.94778966904\n",
      "epoch 8, batch 181, train_loss 3.5835776329\n",
      "epoch 8, batch 182, train_loss 3.06930065155\n",
      "epoch 8, batch 183, train_loss 3.7363948822\n",
      "epoch 8, batch 184, train_loss 2.42942285538\n",
      "epoch 8, batch 185, train_loss 2.79787969589\n",
      "epoch 8, batch 186, train_loss 3.42337870598\n",
      "epoch 8, batch 187, train_loss 2.49562907219\n",
      "epoch 8, batch 188, train_loss 2.83802604675\n",
      "epoch 8, batch 189, train_loss 2.81746292114\n",
      "epoch 8, batch 190, train_loss 3.5958135128\n",
      "epoch 8, batch 191, train_loss 3.44873070717\n",
      "epoch 8, batch 192, train_loss 4.42345714569\n",
      "epoch 8, batch 193, train_loss 4.13201618195\n",
      "epoch 8, batch 194, train_loss 4.7485370636\n",
      "epoch 8, batch 195, train_loss 4.83185577393\n",
      "epoch 8, batch 196, train_loss 3.23368096352\n",
      "epoch 8, batch 197, train_loss 2.57893466949\n",
      "epoch 8, batch 198, train_loss 2.36475563049\n",
      "epoch 8, batch 199, train_loss 3.40359449387\n",
      "epoch 8, batch 200, train_loss 2.84864521027\n",
      "epoch 8, batch 201, train_loss 3.16357469559\n",
      "epoch 8, batch 202, train_loss 3.60705161095\n",
      "epoch 8, batch 203, train_loss 3.7424287796\n",
      "epoch 8, batch 204, train_loss 3.71945643425\n",
      "epoch 8, batch 205, train_loss 3.18013429642\n",
      "epoch 8, batch 206, train_loss 4.22717571259\n",
      "epoch 8, batch 207, train_loss 3.55988049507\n",
      "epoch 8, batch 208, train_loss 4.28140640259\n",
      "epoch 8, batch 209, train_loss 4.00444078445\n",
      "epoch 8, batch 210, train_loss 2.19042134285\n",
      "epoch 8, batch 211, train_loss 2.96342420578\n",
      "epoch 8, batch 212, train_loss 3.88698196411\n",
      "epoch 8, batch 213, train_loss 3.21379995346\n",
      "epoch 8, batch 214, train_loss 3.0625808239\n",
      "epoch 8, batch 215, train_loss 2.26978230476\n",
      "epoch 8, batch 216, train_loss 3.43138599396\n",
      "epoch 8, batch 217, train_loss 2.86648440361\n",
      "epoch 8, batch 218, train_loss 3.34698557854\n",
      "epoch 8, batch 219, train_loss 3.46617126465\n",
      "epoch 8, batch 220, train_loss 3.49535346031\n",
      "epoch 8, batch 221, train_loss 3.64897656441\n",
      "epoch 8, batch 222, train_loss 2.36207795143\n",
      "epoch 8, batch 223, train_loss 2.36032986641\n",
      "epoch 8, batch 224, train_loss 2.71054601669\n",
      "epoch 8, batch 225, train_loss 2.84495544434\n",
      "epoch 8, batch 226, train_loss 3.38155555725\n",
      "epoch 8, batch 227, train_loss 3.13155055046\n",
      "epoch 8, batch 228, train_loss 3.295784235\n",
      "epoch 8, batch 229, train_loss 3.11594033241\n",
      "epoch 8, batch 230, train_loss 2.97845506668\n",
      "epoch 8, batch 231, train_loss 2.66679906845\n",
      "epoch 8, batch 232, train_loss 3.33329725266\n",
      "epoch 8, batch 233, train_loss 3.59209918976\n",
      "epoch 8, batch 234, train_loss 3.15099382401\n",
      "epoch 8, batch 235, train_loss 3.72664904594\n",
      "epoch 8, batch 236, train_loss 3.23699760437\n",
      "epoch 8, batch 237, train_loss 3.15213060379\n",
      "epoch 8, batch 238, train_loss 3.173153162\n",
      "epoch 8, batch 239, train_loss 2.98828363419\n",
      "epoch 8, batch 240, train_loss 3.44114613533\n",
      "epoch 8, batch 241, train_loss 3.35753941536\n",
      "epoch 8, batch 242, train_loss 3.18341970444\n",
      "epoch 8, batch 243, train_loss 3.37984108925\n",
      "epoch 8, batch 244, train_loss 3.51662349701\n",
      "epoch 8, batch 245, train_loss 3.27231740952\n",
      "epoch 8, batch 246, train_loss 3.19492268562\n",
      "epoch 8, batch 247, train_loss 3.38653612137\n",
      "epoch 8, batch 248, train_loss 3.12341189384\n",
      "epoch 8, batch 249, train_loss 3.83519101143\n",
      "epoch 8, batch 250, train_loss 3.59939074516\n",
      "epoch 8, batch 251, train_loss 3.95667004585\n",
      "epoch 8, batch 252, train_loss 2.72285056114\n",
      "epoch 8, batch 253, train_loss 2.25572490692\n",
      "epoch 8, batch 254, train_loss 2.90008449554\n",
      "epoch 8, batch 255, train_loss 2.24577713013\n",
      "epoch 8, batch 256, train_loss 2.6295208931\n",
      "epoch 8, batch 257, train_loss 3.10474944115\n",
      "epoch 8, batch 258, train_loss 2.46897912025\n",
      "epoch 8, batch 259, train_loss 2.40507102013\n",
      "epoch 8, batch 260, train_loss 2.15314102173\n",
      "epoch 8, batch 261, train_loss 3.52701044083\n",
      "epoch 8, batch 262, train_loss 4.13056182861\n",
      "epoch 8, batch 263, train_loss 3.20970630646\n",
      "epoch 8, batch 264, train_loss 2.91704535484\n",
      "epoch 8, batch 265, train_loss 3.57068872452\n",
      "epoch 8, batch 266, train_loss 3.61981868744\n",
      "epoch 8, batch 267, train_loss 2.84934544563\n",
      "epoch 8, batch 268, train_loss 2.38809728622\n",
      "epoch 8, batch 269, train_loss 2.45571231842\n",
      "epoch 8, batch 270, train_loss 2.76131057739\n",
      "epoch 8, batch 271, train_loss 3.11486554146\n",
      "epoch 8, batch 272, train_loss 3.13341712952\n",
      "epoch 8, batch 273, train_loss 3.25611352921\n",
      "epoch 8, batch 274, train_loss 3.05902266502\n",
      "epoch 8, batch 275, train_loss 3.07270884514\n",
      "epoch 8, batch 276, train_loss 2.84788584709\n",
      "epoch 8, batch 277, train_loss 3.68174004555\n",
      "epoch 8, batch 278, train_loss 2.81205129623\n",
      "epoch 8, batch 279, train_loss 3.36373281479\n",
      "epoch 8, batch 280, train_loss 3.34945726395\n",
      "epoch 8, batch 281, train_loss 3.14143753052\n",
      "epoch 8, batch 282, train_loss 2.76918506622\n",
      "epoch 8, batch 283, train_loss 4.39963197708\n",
      "epoch 8, batch 284, train_loss 3.2956635952\n",
      "epoch 8, batch 285, train_loss 2.02598404884\n",
      "epoch 8, batch 286, train_loss 2.27129530907\n",
      "epoch 8, batch 287, train_loss 2.37845730782\n",
      "epoch 8, batch 288, train_loss 3.03243327141\n",
      "epoch 8, batch 289, train_loss 3.08073163033\n",
      "epoch 8, batch 290, train_loss 2.9302752018\n",
      "epoch 8, batch 291, train_loss 2.78897595406\n",
      "epoch 8, batch 292, train_loss 2.28705716133\n",
      "epoch 8, batch 293, train_loss 3.39707303047\n",
      "epoch 8, batch 294, train_loss 2.96179413795\n",
      "epoch 8, batch 295, train_loss 2.67584300041\n",
      "epoch 8, batch 296, train_loss 2.97616505623\n",
      "epoch 8, batch 297, train_loss 2.63316607475\n",
      "epoch 8, batch 298, train_loss 2.87904810905\n",
      "epoch 8, batch 299, train_loss 3.44688010216\n",
      "epoch 8, batch 300, train_loss 3.16489386559\n",
      "epoch 8, batch 301, train_loss 4.15532493591\n",
      "epoch 8, batch 302, train_loss 4.17259693146\n",
      "epoch 8, batch 303, train_loss 3.83165550232\n",
      "epoch 8, batch 304, train_loss 4.51587486267\n",
      "epoch 8, batch 305, train_loss 4.46596956253\n",
      "epoch 8, batch 306, train_loss 3.08958268166\n",
      "epoch 8, batch 307, train_loss 2.55961108208\n",
      "epoch 8, batch 308, train_loss 3.00988698006\n",
      "epoch 8, batch 309, train_loss 3.17629480362\n",
      "epoch 8, batch 310, train_loss 2.97495365143\n",
      "epoch 8, batch 311, train_loss 3.09587454796\n",
      "epoch 8, batch 312, train_loss 3.27820587158\n",
      "epoch 8, batch 313, train_loss 2.98264193535\n",
      "epoch 8, batch 314, train_loss 3.33428478241\n",
      "epoch 8, batch 315, train_loss 2.79070234299\n",
      "epoch 8, batch 316, train_loss 2.95206642151\n",
      "epoch 8, batch 317, train_loss 3.44295501709\n",
      "epoch 8, batch 318, train_loss 2.55795669556\n",
      "epoch 8, batch 319, train_loss 4.31983995438\n",
      "epoch 8, batch 320, train_loss 3.16514992714\n",
      "epoch 8, batch 321, train_loss 4.41641998291\n",
      "epoch 8, batch 322, train_loss 2.44885277748\n",
      "epoch 8, batch 323, train_loss 2.31710028648\n",
      "epoch 8, batch 324, train_loss 3.46979379654\n",
      "epoch 8, batch 325, train_loss 3.12316656113\n",
      "epoch 8, batch 326, train_loss 3.1139550209\n",
      "epoch 8, batch 327, train_loss 3.09346961975\n",
      "epoch 8, batch 328, train_loss 2.97227883339\n",
      "epoch 8, batch 329, train_loss 2.63761138916\n",
      "epoch 8, batch 330, train_loss 3.994556427\n",
      "epoch 8, batch 331, train_loss 3.66251754761\n",
      "epoch 8, batch 332, train_loss 2.47563719749\n",
      "epoch 8, batch 333, train_loss 3.55546116829\n",
      "epoch 8, batch 334, train_loss 3.26039505005\n",
      "epoch 8, batch 335, train_loss 3.16337537766\n",
      "epoch 8, batch 336, train_loss 3.74093961716\n",
      "epoch 8, batch 337, train_loss 2.28644824028\n",
      "epoch 8, batch 338, train_loss 3.12543678284\n",
      "epoch 8, batch 339, train_loss 2.90798711777\n",
      "epoch 8, batch 340, train_loss 3.10351896286\n",
      "epoch 8, batch 341, train_loss 3.03774523735\n",
      "epoch 8, batch 342, train_loss 3.08709645271\n",
      "epoch 8, batch 343, train_loss 2.98277521133\n",
      "epoch 8, batch 344, train_loss 3.95778560638\n",
      "epoch 8, batch 345, train_loss 3.35755825043\n",
      "epoch 8, batch 346, train_loss 3.1164753437\n",
      "epoch 8, batch 347, train_loss 2.91320824623\n",
      "epoch 8, batch 348, train_loss 3.16868925095\n",
      "epoch 8, batch 349, train_loss 4.2003569603\n",
      "epoch 8, batch 350, train_loss 4.17587280273\n",
      "epoch 8, batch 351, train_loss 3.96394658089\n",
      "epoch 8, batch 352, train_loss 4.01502418518\n",
      "epoch 8, batch 353, train_loss 3.26718091965\n",
      "epoch 8, batch 354, train_loss 2.64745855331\n",
      "epoch 8, batch 355, train_loss 2.95363593102\n",
      "epoch 8, batch 356, train_loss 2.92469573021\n",
      "epoch 8, batch 357, train_loss 2.86768484116\n",
      "epoch 8, batch 358, train_loss 2.42788410187\n",
      "epoch 8, batch 359, train_loss 3.41380524635\n",
      "epoch 8, batch 360, train_loss 2.90564107895\n",
      "epoch 8, batch 361, train_loss 2.57238650322\n",
      "epoch 8, batch 362, train_loss 4.32302570343\n",
      "epoch 8, batch 363, train_loss 3.18060684204\n",
      "epoch 8, batch 364, train_loss 3.07905340195\n",
      "epoch 8, batch 365, train_loss 3.98773002625\n",
      "epoch 8, batch 366, train_loss 4.11466836929\n",
      "epoch 8, batch 367, train_loss 4.86596679688\n",
      "epoch 8, batch 368, train_loss 2.53595423698\n",
      "epoch 8, batch 369, train_loss 3.47628092766\n",
      "epoch 8, batch 370, train_loss 3.86662173271\n",
      "epoch 8, batch 371, train_loss 4.82881784439\n",
      "epoch 8, batch 372, train_loss 2.32683110237\n",
      "epoch 8, batch 373, train_loss 2.49815034866\n",
      "epoch 8, batch 374, train_loss 2.83013272285\n",
      "epoch 8, batch 375, train_loss 3.22605228424\n",
      "epoch 8, batch 376, train_loss 2.48120117188\n",
      "epoch 8, batch 377, train_loss 2.88744735718\n",
      "epoch 8, batch 378, train_loss 3.6028983593\n",
      "epoch 8, batch 379, train_loss 2.76609063148\n",
      "epoch 8, batch 380, train_loss 2.78870773315\n",
      "epoch 8, batch 381, train_loss 2.88670420647\n",
      "epoch 8, batch 382, train_loss 3.78085541725\n",
      "epoch 8, batch 383, train_loss 3.34003472328\n",
      "epoch 8, batch 384, train_loss 2.99987101555\n",
      "epoch 8, batch 385, train_loss 4.40883636475\n",
      "epoch 8, batch 386, train_loss 2.69962310791\n",
      "epoch 8, batch 387, train_loss 4.57316684723\n",
      "epoch 8, batch 388, train_loss 2.70483541489\n",
      "epoch 8, batch 389, train_loss 4.29139518738\n",
      "epoch 8, batch 390, train_loss 4.2778339386\n",
      "epoch 8, batch 391, train_loss 4.3076543808\n",
      "epoch 8, batch 392, train_loss 4.06514930725\n",
      "epoch 8, batch 393, train_loss 3.30325007439\n",
      "epoch 8, batch 394, train_loss 3.93614578247\n",
      "epoch 8, batch 395, train_loss 2.88425469398\n",
      "epoch 8, batch 396, train_loss 4.18583059311\n",
      "epoch 8, batch 397, train_loss 4.29711914062\n",
      "epoch 8, batch 398, train_loss 3.22187972069\n",
      "epoch 8, batch 399, train_loss 3.45813679695\n",
      "epoch 8, batch 400, train_loss 4.43896436691\n",
      "epoch 8, batch 401, train_loss 3.66867017746\n",
      "epoch 8, batch 402, train_loss 3.24330353737\n",
      "epoch 8, batch 403, train_loss 3.18303227425\n",
      "epoch 8, batch 404, train_loss 4.03231287003\n",
      "epoch 8, batch 405, train_loss 3.52762627602\n",
      "epoch 8, batch 406, train_loss 3.3784840107\n",
      "epoch 8, batch 407, train_loss 3.17515277863\n",
      "epoch 8, batch 408, train_loss 3.02991724014\n",
      "epoch 8, batch 409, train_loss 2.54005670547\n",
      "epoch 8, batch 410, train_loss 4.16675567627\n",
      "epoch 8, batch 411, train_loss 3.25745487213\n",
      "epoch 8, batch 412, train_loss 2.7624361515\n",
      "epoch 8, batch 413, train_loss 3.11146640778\n",
      "epoch 8, batch 414, train_loss 3.6419365406\n",
      "epoch 8, batch 415, train_loss 3.707447052\n",
      "epoch 8, batch 416, train_loss 3.45446324348\n",
      "epoch 8, batch 417, train_loss 2.99750471115\n",
      "epoch 8, batch 418, train_loss 3.40515828133\n",
      "epoch 8, batch 419, train_loss 3.02707695961\n",
      "epoch 8, batch 420, train_loss 2.45770311356\n",
      "epoch 8, batch 421, train_loss 4.28252887726\n",
      "epoch 8, batch 422, train_loss 3.00536727905\n",
      "epoch 8, batch 423, train_loss 3.12763476372\n",
      "epoch 8, batch 424, train_loss 4.22202253342\n",
      "epoch 8, batch 425, train_loss 3.52266335487\n",
      "epoch 8, batch 426, train_loss 4.69451761246\n",
      "epoch 8, batch 427, train_loss 3.81740570068\n",
      "epoch 8, batch 428, train_loss 2.61242794991\n",
      "epoch 8, batch 429, train_loss 3.62978982925\n",
      "epoch 8, batch 430, train_loss 3.68788075447\n",
      "epoch 8, batch 431, train_loss 3.2826833725\n",
      "epoch 8, batch 432, train_loss 3.48801970482\n",
      "epoch 8, batch 433, train_loss 2.88892006874\n",
      "epoch 8, batch 434, train_loss 3.40182900429\n",
      "epoch 8, batch 435, train_loss 3.20148086548\n",
      "epoch 8, batch 436, train_loss 3.55307626724\n",
      "epoch 8, batch 437, train_loss 4.00996780396\n",
      "epoch 8, batch 438, train_loss 3.65924692154\n",
      "epoch 8, batch 439, train_loss 3.41812968254\n",
      "epoch 8, batch 440, train_loss 4.72424793243\n",
      "epoch 8, batch 441, train_loss 4.84448337555\n",
      "epoch 8, batch 442, train_loss 4.01267719269\n",
      "epoch 8, batch 443, train_loss 2.459243536\n",
      "epoch 8, batch 444, train_loss 2.98469281197\n",
      "epoch 8, batch 445, train_loss 4.58441543579\n",
      "epoch 8, batch 446, train_loss 3.34205055237\n",
      "epoch 8, batch 447, train_loss 3.03457808495\n",
      "epoch 8, batch 448, train_loss 4.46702337265\n",
      "epoch 8, batch 449, train_loss 3.84033203125\n",
      "epoch 8, batch 450, train_loss 3.16794109344\n",
      "epoch 8, batch 451, train_loss 2.91460204124\n",
      "epoch 8, batch 452, train_loss 2.67035651207\n",
      "epoch 8, batch 453, train_loss 5.15714263916\n",
      "epoch 8, batch 454, train_loss 5.01506853104\n",
      "epoch 8, batch 455, train_loss 2.5509660244\n",
      "epoch 8, batch 456, train_loss 3.23670363426\n",
      "epoch 8, batch 457, train_loss 3.47530889511\n",
      "epoch 8, batch 458, train_loss 3.10431885719\n",
      "epoch 8, batch 459, train_loss 3.38771390915\n",
      "epoch 8, batch 460, train_loss 3.8837325573\n",
      "epoch 8, batch 461, train_loss 3.38224458694\n",
      "epoch 8, batch 462, train_loss 3.05257606506\n",
      "epoch 8, batch 463, train_loss 3.2055850029\n",
      "epoch 8, batch 464, train_loss 3.49089050293\n",
      "epoch 8, batch 465, train_loss 2.87788057327\n",
      "epoch 8, batch 466, train_loss 3.65831589699\n",
      "epoch 8, batch 467, train_loss 3.47742819786\n",
      "epoch 8, batch 468, train_loss 3.61987829208\n",
      "epoch 8, batch 469, train_loss 3.18160462379\n",
      "epoch 8, batch 470, train_loss 3.04793930054\n",
      "epoch 8, batch 471, train_loss 3.1253798008\n",
      "epoch 8, batch 472, train_loss 2.99140048027\n",
      "epoch 8, batch 473, train_loss 3.69360494614\n",
      "epoch 8, batch 474, train_loss 4.28312683105\n",
      "epoch 8, batch 475, train_loss 3.00654840469\n",
      "epoch 8, batch 476, train_loss 2.51916265488\n",
      "epoch 8, batch 477, train_loss 2.61699056625\n",
      "epoch 8, batch 478, train_loss 2.45222401619\n",
      "epoch 8, batch 479, train_loss 2.96055054665\n",
      "epoch 8, batch 480, train_loss 3.15957713127\n",
      "epoch 8, batch 481, train_loss 4.74098300934\n",
      "epoch 8, batch 482, train_loss 2.86973214149\n",
      "epoch 8, batch 483, train_loss 3.49649000168\n",
      "epoch 8, batch 484, train_loss 3.16214227676\n",
      "epoch 8, batch 485, train_loss 2.38735246658\n",
      "epoch 8, batch 486, train_loss 2.39404296875\n",
      "epoch 8, batch 487, train_loss 2.55543541908\n",
      "epoch 8, batch 488, train_loss 2.95884633064\n",
      "epoch 8, batch 489, train_loss 3.08018088341\n",
      "epoch 8, batch 490, train_loss 3.00180387497\n",
      "epoch 8, batch 491, train_loss 2.92395544052\n",
      "epoch 8, batch 492, train_loss 3.2653465271\n",
      "epoch 8, batch 493, train_loss 2.85388946533\n",
      "epoch 8, batch 494, train_loss 2.94808840752\n",
      "epoch 8, batch 495, train_loss 3.0229344368\n",
      "epoch 8, batch 496, train_loss 2.6527838707\n",
      "epoch 8, batch 497, train_loss 2.82964301109\n",
      "epoch 8, batch 498, train_loss 2.96770119667\n",
      "epoch 8, batch 499, train_loss 2.92460107803\n",
      "epoch 8, batch 500, train_loss 3.38230204582\n",
      "epoch 8, batch 501, train_loss 3.0320789814\n",
      "epoch 8, batch 502, train_loss 4.32352495193\n",
      "epoch 8, batch 503, train_loss 4.25732898712\n",
      "epoch 8, batch 504, train_loss 3.71224021912\n",
      "epoch 8, batch 505, train_loss 3.51417016983\n",
      "epoch 8, batch 506, train_loss 3.18543124199\n",
      "epoch 8, batch 507, train_loss 4.19443941116\n",
      "epoch 8, batch 508, train_loss 3.00395226479\n",
      "epoch 8, batch 509, train_loss 4.34909152985\n",
      "epoch 8, batch 510, train_loss 4.31778001785\n",
      "epoch 8, batch 511, train_loss 4.33026838303\n",
      "epoch 8, batch 512, train_loss 3.36870765686\n",
      "epoch 8, batch 513, train_loss 4.76457643509\n",
      "epoch 8, batch 514, train_loss 4.70120716095\n",
      "epoch 8, batch 515, train_loss 4.59584379196\n",
      "epoch 8, batch 516, train_loss 2.86151623726\n",
      "epoch 8, batch 517, train_loss 3.05703425407\n",
      "epoch 8, batch 518, train_loss 2.95968079567\n",
      "epoch 8, batch 519, train_loss 2.64184188843\n",
      "epoch 8, batch 520, train_loss 3.80267953873\n",
      "epoch 8, batch 521, train_loss 2.97220563889\n",
      "epoch 8, batch 522, train_loss 2.58870100975\n",
      "epoch 8, batch 523, train_loss 2.58256411552\n",
      "epoch 8, batch 524, train_loss 2.43350577354\n",
      "epoch 8, batch 525, train_loss 2.26293897629\n",
      "epoch 8, batch 526, train_loss 2.35717535019\n",
      "epoch 8, batch 527, train_loss 2.38809728622\n",
      "epoch 8, batch 528, train_loss 2.6303832531\n",
      "epoch 8, batch 529, train_loss 2.51758933067\n",
      "epoch 8, batch 530, train_loss 2.00992321968\n",
      "epoch 8, batch 531, train_loss 1.76314294338\n",
      "epoch 8, batch 532, train_loss 2.62387752533\n",
      "epoch 8, batch 533, train_loss 1.66677129269\n",
      "epoch 8, batch 534, train_loss 1.72625732422\n",
      "epoch 8, batch 535, train_loss 3.17295956612\n",
      "epoch 8, batch 536, train_loss 3.39887952805\n",
      "epoch 8, batch 537, train_loss 3.27078223228\n",
      "epoch 8, batch 538, train_loss 3.40352606773\n",
      "epoch 8, batch 539, train_loss 3.67789769173\n",
      "epoch 8, batch 540, train_loss 4.01288700104\n",
      "epoch 9, batch 0, train_loss 3.37797546387\n",
      "epoch 9, batch 1, train_loss 3.73796129227\n",
      "epoch 9, batch 2, train_loss 3.08373641968\n",
      "epoch 9, batch 3, train_loss 3.34584832191\n",
      "epoch 9, batch 4, train_loss 3.09719538689\n",
      "epoch 9, batch 5, train_loss 3.16698431969\n",
      "epoch 9, batch 6, train_loss 4.12934494019\n",
      "epoch 9, batch 7, train_loss 3.12142205238\n",
      "epoch 9, batch 8, train_loss 2.7737557888\n",
      "epoch 9, batch 9, train_loss 3.76505637169\n",
      "epoch 9, batch 10, train_loss 3.0084528923\n",
      "epoch 9, batch 11, train_loss 3.07582950592\n",
      "epoch 9, batch 12, train_loss 2.80833482742\n",
      "epoch 9, batch 13, train_loss 3.29976415634\n",
      "epoch 9, batch 14, train_loss 2.72497558594\n",
      "epoch 9, batch 15, train_loss 3.45171093941\n",
      "epoch 9, batch 16, train_loss 2.99227881432\n",
      "epoch 9, batch 17, train_loss 2.84447216988\n",
      "epoch 9, batch 18, train_loss 2.50666666031\n",
      "epoch 9, batch 19, train_loss 2.06890249252\n",
      "epoch 9, batch 20, train_loss 2.91489434242\n",
      "epoch 9, batch 21, train_loss 2.85056781769\n",
      "epoch 9, batch 22, train_loss 3.33425378799\n",
      "epoch 9, batch 23, train_loss 2.90450000763\n",
      "epoch 9, batch 24, train_loss 2.90288686752\n",
      "epoch 9, batch 25, train_loss 3.35011434555\n",
      "epoch 9, batch 26, train_loss 3.28705859184\n",
      "epoch 9, batch 27, train_loss 3.19535374641\n",
      "epoch 9, batch 28, train_loss 3.49124646187\n",
      "epoch 9, batch 29, train_loss 3.38874149323\n",
      "epoch 9, batch 30, train_loss 3.48259091377\n",
      "epoch 9, batch 31, train_loss 2.92584633827\n",
      "epoch 9, batch 32, train_loss 2.70027780533\n",
      "epoch 9, batch 33, train_loss 4.53577899933\n",
      "epoch 9, batch 34, train_loss 4.62523841858\n",
      "epoch 9, batch 35, train_loss 3.26851201057\n",
      "epoch 9, batch 36, train_loss 3.29487991333\n",
      "epoch 9, batch 37, train_loss 3.23613595963\n",
      "epoch 9, batch 38, train_loss 3.38603305817\n",
      "epoch 9, batch 39, train_loss 3.85471653938\n",
      "epoch 9, batch 40, train_loss 2.94067955017\n",
      "epoch 9, batch 41, train_loss 3.22581791878\n",
      "epoch 9, batch 42, train_loss 3.24008917809\n",
      "epoch 9, batch 43, train_loss 3.00811028481\n",
      "epoch 9, batch 44, train_loss 3.65398383141\n",
      "epoch 9, batch 45, train_loss 3.26975703239\n",
      "epoch 9, batch 46, train_loss 3.22579693794\n",
      "epoch 9, batch 47, train_loss 3.31881332397\n",
      "epoch 9, batch 48, train_loss 3.13828921318\n",
      "epoch 9, batch 49, train_loss 3.27957820892\n",
      "epoch 9, batch 50, train_loss 3.19158148766\n",
      "epoch 9, batch 51, train_loss 2.84953141212\n",
      "epoch 9, batch 52, train_loss 3.30225563049\n",
      "epoch 9, batch 53, train_loss 3.15856957436\n",
      "epoch 9, batch 54, train_loss 2.91516113281\n",
      "epoch 9, batch 55, train_loss 2.95410895348\n",
      "epoch 9, batch 56, train_loss 2.84441971779\n",
      "epoch 9, batch 57, train_loss 3.32431912422\n",
      "epoch 9, batch 58, train_loss 3.1312584877\n",
      "epoch 9, batch 59, train_loss 3.0899116993\n",
      "epoch 9, batch 60, train_loss 3.28540492058\n",
      "epoch 9, batch 61, train_loss 1.89210569859\n",
      "epoch 9, batch 62, train_loss 2.86091780663\n",
      "epoch 9, batch 63, train_loss 3.00504875183\n",
      "epoch 9, batch 64, train_loss 3.2459602356\n",
      "epoch 9, batch 65, train_loss 3.56240773201\n",
      "epoch 9, batch 66, train_loss 3.32742643356\n",
      "epoch 9, batch 67, train_loss 2.14833021164\n",
      "epoch 9, batch 68, train_loss 3.02364826202\n",
      "epoch 9, batch 69, train_loss 2.36901640892\n",
      "epoch 9, batch 70, train_loss 3.90667796135\n",
      "epoch 9, batch 71, train_loss 3.96429085732\n",
      "epoch 9, batch 72, train_loss 2.81387090683\n",
      "epoch 9, batch 73, train_loss 3.23165130615\n",
      "epoch 9, batch 74, train_loss 3.79216384888\n",
      "epoch 9, batch 75, train_loss 3.27177476883\n",
      "epoch 9, batch 76, train_loss 3.55834507942\n",
      "epoch 9, batch 77, train_loss 4.2437376976\n",
      "epoch 9, batch 78, train_loss 3.1038813591\n",
      "epoch 9, batch 79, train_loss 3.12569022179\n",
      "epoch 9, batch 80, train_loss 2.95656514168\n",
      "epoch 9, batch 81, train_loss 3.09000182152\n",
      "epoch 9, batch 82, train_loss 2.98638343811\n",
      "epoch 9, batch 83, train_loss 3.1238284111\n",
      "epoch 9, batch 84, train_loss 2.87178850174\n",
      "epoch 9, batch 85, train_loss 2.89436626434\n",
      "epoch 9, batch 86, train_loss 3.41152977943\n",
      "epoch 9, batch 87, train_loss 2.61478567123\n",
      "epoch 9, batch 88, train_loss 2.92412829399\n",
      "epoch 9, batch 89, train_loss 3.22320604324\n",
      "epoch 9, batch 90, train_loss 2.9257144928\n",
      "epoch 9, batch 91, train_loss 2.96569776535\n",
      "epoch 9, batch 92, train_loss 3.42195296288\n",
      "epoch 9, batch 93, train_loss 4.17021799088\n",
      "epoch 9, batch 94, train_loss 2.97827529907\n",
      "epoch 9, batch 95, train_loss 3.04750919342\n",
      "epoch 9, batch 96, train_loss 2.42488431931\n",
      "epoch 9, batch 97, train_loss 3.03493046761\n",
      "epoch 9, batch 98, train_loss 2.96933937073\n",
      "epoch 9, batch 99, train_loss 3.20350813866\n",
      "epoch 9, batch 100, train_loss 2.98742079735\n",
      "epoch 9, batch 101, train_loss 3.14116072655\n",
      "epoch 9, batch 102, train_loss 2.96080684662\n",
      "epoch 9, batch 103, train_loss 4.2137336731\n",
      "epoch 9, batch 104, train_loss 3.47666478157\n",
      "epoch 9, batch 105, train_loss 3.29071092606\n",
      "epoch 9, batch 106, train_loss 3.68610692024\n",
      "epoch 9, batch 107, train_loss 3.511033535\n",
      "epoch 9, batch 108, train_loss 3.12580180168\n",
      "epoch 9, batch 109, train_loss 3.39074110985\n",
      "epoch 9, batch 110, train_loss 3.20365571976\n",
      "epoch 9, batch 111, train_loss 3.612429142\n",
      "epoch 9, batch 112, train_loss 3.5638897419\n",
      "epoch 9, batch 113, train_loss 2.96963834763\n",
      "epoch 9, batch 114, train_loss 3.55242538452\n",
      "epoch 9, batch 115, train_loss 4.27178096771\n",
      "epoch 9, batch 116, train_loss 4.15644884109\n",
      "epoch 9, batch 117, train_loss 4.30461406708\n",
      "epoch 9, batch 118, train_loss 3.73150539398\n",
      "epoch 9, batch 119, train_loss 2.46885466576\n",
      "epoch 9, batch 120, train_loss 3.28866648674\n",
      "epoch 9, batch 121, train_loss 3.10997343063\n",
      "epoch 9, batch 122, train_loss 3.32559108734\n",
      "epoch 9, batch 123, train_loss 3.09752511978\n",
      "epoch 9, batch 124, train_loss 2.79556250572\n",
      "epoch 9, batch 125, train_loss 2.76620316505\n",
      "epoch 9, batch 126, train_loss 2.73947525024\n",
      "epoch 9, batch 127, train_loss 2.80690860748\n",
      "epoch 9, batch 128, train_loss 2.64945554733\n",
      "epoch 9, batch 129, train_loss 3.05041408539\n",
      "epoch 9, batch 130, train_loss 2.99055933952\n",
      "epoch 9, batch 131, train_loss 3.35277533531\n",
      "epoch 9, batch 132, train_loss 3.09650397301\n",
      "epoch 9, batch 133, train_loss 2.33693051338\n",
      "epoch 9, batch 134, train_loss 2.36608195305\n",
      "epoch 9, batch 135, train_loss 4.12168264389\n",
      "epoch 9, batch 136, train_loss 3.54338765144\n",
      "epoch 9, batch 137, train_loss 2.7228269577\n",
      "epoch 9, batch 138, train_loss 2.57752752304\n",
      "epoch 9, batch 139, train_loss 3.09635519981\n",
      "epoch 9, batch 140, train_loss 2.58458185196\n",
      "epoch 9, batch 141, train_loss 2.89895534515\n",
      "epoch 9, batch 142, train_loss 4.11124897003\n",
      "epoch 9, batch 143, train_loss 2.58866095543\n",
      "epoch 9, batch 144, train_loss 2.10685706139\n",
      "epoch 9, batch 145, train_loss 3.08844280243\n",
      "epoch 9, batch 146, train_loss 3.27065253258\n",
      "epoch 9, batch 147, train_loss 3.0026576519\n",
      "epoch 9, batch 148, train_loss 2.90319252014\n",
      "epoch 9, batch 149, train_loss 3.33116984367\n",
      "epoch 9, batch 150, train_loss 3.45635128021\n",
      "epoch 9, batch 151, train_loss 2.11989593506\n",
      "epoch 9, batch 152, train_loss 2.36006402969\n",
      "epoch 9, batch 153, train_loss 4.02179861069\n",
      "epoch 9, batch 154, train_loss 3.18287587166\n",
      "epoch 9, batch 155, train_loss 2.7748105526\n",
      "epoch 9, batch 156, train_loss 2.97462534904\n",
      "epoch 9, batch 157, train_loss 2.56547999382\n",
      "epoch 9, batch 158, train_loss 2.58056688309\n",
      "epoch 9, batch 159, train_loss 2.78539514542\n",
      "epoch 9, batch 160, train_loss 2.88678789139\n",
      "epoch 9, batch 161, train_loss 3.42122673988\n",
      "epoch 9, batch 162, train_loss 3.39329051971\n",
      "epoch 9, batch 163, train_loss 4.85174131393\n",
      "epoch 9, batch 164, train_loss 2.34011077881\n",
      "epoch 9, batch 165, train_loss 2.29094195366\n",
      "epoch 9, batch 166, train_loss 2.60916757584\n",
      "epoch 9, batch 167, train_loss 2.63905620575\n",
      "epoch 9, batch 168, train_loss 2.86172413826\n",
      "epoch 9, batch 169, train_loss 2.65846133232\n",
      "epoch 9, batch 170, train_loss 3.17992782593\n",
      "epoch 9, batch 171, train_loss 2.58953046799\n",
      "epoch 9, batch 172, train_loss 3.19394087791\n",
      "epoch 9, batch 173, train_loss 2.13861131668\n",
      "epoch 9, batch 174, train_loss 3.67771053314\n",
      "epoch 9, batch 175, train_loss 3.15559625626\n",
      "epoch 9, batch 176, train_loss 3.04929900169\n",
      "epoch 9, batch 177, train_loss 2.85581588745\n",
      "epoch 9, batch 178, train_loss 2.78741574287\n",
      "epoch 9, batch 179, train_loss 3.0399274826\n",
      "epoch 9, batch 180, train_loss 2.9105322361\n",
      "epoch 9, batch 181, train_loss 3.53547430038\n",
      "epoch 9, batch 182, train_loss 3.0310561657\n",
      "epoch 9, batch 183, train_loss 3.69035983086\n",
      "epoch 9, batch 184, train_loss 2.40258860588\n",
      "epoch 9, batch 185, train_loss 2.76355433464\n",
      "epoch 9, batch 186, train_loss 3.37560224533\n",
      "epoch 9, batch 187, train_loss 2.45968747139\n",
      "epoch 9, batch 188, train_loss 2.80289936066\n",
      "epoch 9, batch 189, train_loss 2.78328132629\n",
      "epoch 9, batch 190, train_loss 3.55137705803\n",
      "epoch 9, batch 191, train_loss 3.40430164337\n",
      "epoch 9, batch 192, train_loss 4.36316537857\n",
      "epoch 9, batch 193, train_loss 4.07664346695\n",
      "epoch 9, batch 194, train_loss 4.68762159348\n",
      "epoch 9, batch 195, train_loss 4.76951169968\n",
      "epoch 9, batch 196, train_loss 3.19198036194\n",
      "epoch 9, batch 197, train_loss 2.54394412041\n",
      "epoch 9, batch 198, train_loss 2.33713316917\n",
      "epoch 9, batch 199, train_loss 3.35348463058\n",
      "epoch 9, batch 200, train_loss 2.81692957878\n",
      "epoch 9, batch 201, train_loss 3.1225965023\n",
      "epoch 9, batch 202, train_loss 3.56378960609\n",
      "epoch 9, batch 203, train_loss 3.69887900352\n",
      "epoch 9, batch 204, train_loss 3.68490719795\n",
      "epoch 9, batch 205, train_loss 3.13787198067\n",
      "epoch 9, batch 206, train_loss 4.15935182571\n",
      "epoch 9, batch 207, train_loss 3.51354241371\n",
      "epoch 9, batch 208, train_loss 4.22476196289\n",
      "epoch 9, batch 209, train_loss 3.9608464241\n",
      "epoch 9, batch 210, train_loss 2.16330337524\n",
      "epoch 9, batch 211, train_loss 2.9251306057\n",
      "epoch 9, batch 212, train_loss 3.84703278542\n",
      "epoch 9, batch 213, train_loss 3.17349267006\n",
      "epoch 9, batch 214, train_loss 3.03316235542\n",
      "epoch 9, batch 215, train_loss 2.24457478523\n",
      "epoch 9, batch 216, train_loss 3.3937959671\n",
      "epoch 9, batch 217, train_loss 2.83575510979\n",
      "epoch 9, batch 218, train_loss 3.30741024017\n",
      "epoch 9, batch 219, train_loss 3.41994738579\n",
      "epoch 9, batch 220, train_loss 3.45318770409\n",
      "epoch 9, batch 221, train_loss 3.60110402107\n",
      "epoch 9, batch 222, train_loss 2.32788205147\n",
      "epoch 9, batch 223, train_loss 2.32676458359\n",
      "epoch 9, batch 224, train_loss 2.67783522606\n",
      "epoch 9, batch 225, train_loss 2.81345200539\n",
      "epoch 9, batch 226, train_loss 3.33645510674\n",
      "epoch 9, batch 227, train_loss 3.09171056747\n",
      "epoch 9, batch 228, train_loss 3.25034093857\n",
      "epoch 9, batch 229, train_loss 3.07130479813\n",
      "epoch 9, batch 230, train_loss 2.93811488152\n",
      "epoch 9, batch 231, train_loss 2.63131117821\n",
      "epoch 9, batch 232, train_loss 3.29013586044\n",
      "epoch 9, batch 233, train_loss 3.54567527771\n",
      "epoch 9, batch 234, train_loss 3.11401891708\n",
      "epoch 9, batch 235, train_loss 3.67799377441\n",
      "epoch 9, batch 236, train_loss 3.20254707336\n",
      "epoch 9, batch 237, train_loss 3.11131858826\n",
      "epoch 9, batch 238, train_loss 3.13773488998\n",
      "epoch 9, batch 239, train_loss 2.95416665077\n",
      "epoch 9, batch 240, train_loss 3.40019178391\n",
      "epoch 9, batch 241, train_loss 3.31873655319\n",
      "epoch 9, batch 242, train_loss 3.14260053635\n",
      "epoch 9, batch 243, train_loss 3.33939862251\n",
      "epoch 9, batch 244, train_loss 3.47741532326\n",
      "epoch 9, batch 245, train_loss 3.23736429214\n",
      "epoch 9, batch 246, train_loss 3.15680480003\n",
      "epoch 9, batch 247, train_loss 3.34781575203\n",
      "epoch 9, batch 248, train_loss 3.08555531502\n",
      "epoch 9, batch 249, train_loss 3.78747034073\n",
      "epoch 9, batch 250, train_loss 3.54717278481\n",
      "epoch 9, batch 251, train_loss 3.90410971642\n",
      "epoch 9, batch 252, train_loss 2.69123339653\n",
      "epoch 9, batch 253, train_loss 2.22805500031\n",
      "epoch 9, batch 254, train_loss 2.85731434822\n",
      "epoch 9, batch 255, train_loss 2.21654415131\n",
      "epoch 9, batch 256, train_loss 2.59638285637\n",
      "epoch 9, batch 257, train_loss 3.06426930428\n",
      "epoch 9, batch 258, train_loss 2.43648600578\n",
      "epoch 9, batch 259, train_loss 2.37476873398\n",
      "epoch 9, batch 260, train_loss 2.1264128685\n",
      "epoch 9, batch 261, train_loss 3.48161149025\n",
      "epoch 9, batch 262, train_loss 4.08167266846\n",
      "epoch 9, batch 263, train_loss 3.17443156242\n",
      "epoch 9, batch 264, train_loss 2.88466477394\n",
      "epoch 9, batch 265, train_loss 3.52073454857\n",
      "epoch 9, batch 266, train_loss 3.5715174675\n",
      "epoch 9, batch 267, train_loss 2.81343483925\n",
      "epoch 9, batch 268, train_loss 2.35448360443\n",
      "epoch 9, batch 269, train_loss 2.42717862129\n",
      "epoch 9, batch 270, train_loss 2.72851490974\n",
      "epoch 9, batch 271, train_loss 3.07642126083\n",
      "epoch 9, batch 272, train_loss 3.09663558006\n",
      "epoch 9, batch 273, train_loss 3.21470355988\n",
      "epoch 9, batch 274, train_loss 3.02390980721\n",
      "epoch 9, batch 275, train_loss 3.03538513184\n",
      "epoch 9, batch 276, train_loss 2.81634640694\n",
      "epoch 9, batch 277, train_loss 3.64325904846\n",
      "epoch 9, batch 278, train_loss 2.77678084373\n",
      "epoch 9, batch 279, train_loss 3.31882929802\n",
      "epoch 9, batch 280, train_loss 3.30293440819\n",
      "epoch 9, batch 281, train_loss 3.10627841949\n",
      "epoch 9, batch 282, train_loss 2.73411750793\n",
      "epoch 9, batch 283, train_loss 4.33470582962\n",
      "epoch 9, batch 284, train_loss 3.25099921227\n",
      "epoch 9, batch 285, train_loss 2.00074386597\n",
      "epoch 9, batch 286, train_loss 2.24419260025\n",
      "epoch 9, batch 287, train_loss 2.34934806824\n",
      "epoch 9, batch 288, train_loss 2.99802494049\n",
      "epoch 9, batch 289, train_loss 3.04569268227\n",
      "epoch 9, batch 290, train_loss 2.88889050484\n",
      "epoch 9, batch 291, train_loss 2.75643968582\n",
      "epoch 9, batch 292, train_loss 2.26314711571\n",
      "epoch 9, batch 293, train_loss 3.35108089447\n",
      "epoch 9, batch 294, train_loss 2.92668914795\n",
      "epoch 9, batch 295, train_loss 2.64396762848\n",
      "epoch 9, batch 296, train_loss 2.93796825409\n",
      "epoch 9, batch 297, train_loss 2.60226941109\n",
      "epoch 9, batch 298, train_loss 2.84072589874\n",
      "epoch 9, batch 299, train_loss 3.38967061043\n",
      "epoch 9, batch 300, train_loss 3.11949563026\n",
      "epoch 9, batch 301, train_loss 4.09212446213\n",
      "epoch 9, batch 302, train_loss 4.10650587082\n",
      "epoch 9, batch 303, train_loss 3.77533340454\n",
      "epoch 9, batch 304, train_loss 4.44485855103\n",
      "epoch 9, batch 305, train_loss 4.40050458908\n",
      "epoch 9, batch 306, train_loss 3.04136896133\n",
      "epoch 9, batch 307, train_loss 2.53026413918\n",
      "epoch 9, batch 308, train_loss 2.97855758667\n",
      "epoch 9, batch 309, train_loss 3.14058303833\n",
      "epoch 9, batch 310, train_loss 2.9465918541\n",
      "epoch 9, batch 311, train_loss 3.05924654007\n",
      "epoch 9, batch 312, train_loss 3.24049830437\n",
      "epoch 9, batch 313, train_loss 2.95232152939\n",
      "epoch 9, batch 314, train_loss 3.29428076744\n",
      "epoch 9, batch 315, train_loss 2.76015210152\n",
      "epoch 9, batch 316, train_loss 2.92040038109\n",
      "epoch 9, batch 317, train_loss 3.40576887131\n",
      "epoch 9, batch 318, train_loss 2.52705335617\n",
      "epoch 9, batch 319, train_loss 4.25850963593\n",
      "epoch 9, batch 320, train_loss 3.12294793129\n",
      "epoch 9, batch 321, train_loss 4.35201025009\n",
      "epoch 9, batch 322, train_loss 2.41841578484\n",
      "epoch 9, batch 323, train_loss 2.2900967598\n",
      "epoch 9, batch 324, train_loss 3.42192149162\n",
      "epoch 9, batch 325, train_loss 3.08707642555\n",
      "epoch 9, batch 326, train_loss 3.07724761963\n",
      "epoch 9, batch 327, train_loss 3.0556435585\n",
      "epoch 9, batch 328, train_loss 2.93376636505\n",
      "epoch 9, batch 329, train_loss 2.60605382919\n",
      "epoch 9, batch 330, train_loss 3.9506893158\n",
      "epoch 9, batch 331, train_loss 3.62569689751\n",
      "epoch 9, batch 332, train_loss 2.44970655441\n",
      "epoch 9, batch 333, train_loss 3.50985527039\n",
      "epoch 9, batch 334, train_loss 3.2167403698\n",
      "epoch 9, batch 335, train_loss 3.12475967407\n",
      "epoch 9, batch 336, train_loss 3.68934965134\n",
      "epoch 9, batch 337, train_loss 2.25919723511\n",
      "epoch 9, batch 338, train_loss 3.08710694313\n",
      "epoch 9, batch 339, train_loss 2.87259745598\n",
      "epoch 9, batch 340, train_loss 3.07017803192\n",
      "epoch 9, batch 341, train_loss 3.00728678703\n",
      "epoch 9, batch 342, train_loss 3.05235862732\n",
      "epoch 9, batch 343, train_loss 2.94948339462\n",
      "epoch 9, batch 344, train_loss 3.90022969246\n",
      "epoch 9, batch 345, train_loss 3.31183552742\n",
      "epoch 9, batch 346, train_loss 3.0757958889\n",
      "epoch 9, batch 347, train_loss 2.87533545494\n",
      "epoch 9, batch 348, train_loss 3.13260507584\n",
      "epoch 9, batch 349, train_loss 4.14428949356\n",
      "epoch 9, batch 350, train_loss 4.12017726898\n",
      "epoch 9, batch 351, train_loss 3.91778445244\n",
      "epoch 9, batch 352, train_loss 3.96500468254\n",
      "epoch 9, batch 353, train_loss 3.22570204735\n",
      "epoch 9, batch 354, train_loss 2.61217784882\n",
      "epoch 9, batch 355, train_loss 2.91947174072\n",
      "epoch 9, batch 356, train_loss 2.88514566422\n",
      "epoch 9, batch 357, train_loss 2.82977485657\n",
      "epoch 9, batch 358, train_loss 2.39650201797\n",
      "epoch 9, batch 359, train_loss 3.37183666229\n",
      "epoch 9, batch 360, train_loss 2.86701273918\n",
      "epoch 9, batch 361, train_loss 2.54092478752\n",
      "epoch 9, batch 362, train_loss 4.26588582993\n",
      "epoch 9, batch 363, train_loss 3.14417004585\n",
      "epoch 9, batch 364, train_loss 3.04249787331\n",
      "epoch 9, batch 365, train_loss 3.93698406219\n",
      "epoch 9, batch 366, train_loss 4.06385040283\n",
      "epoch 9, batch 367, train_loss 4.81133317947\n",
      "epoch 9, batch 368, train_loss 2.50689721107\n",
      "epoch 9, batch 369, train_loss 3.43559336662\n",
      "epoch 9, batch 370, train_loss 3.8284573555\n",
      "epoch 9, batch 371, train_loss 4.77436494827\n",
      "epoch 9, batch 372, train_loss 2.29852604866\n",
      "epoch 9, batch 373, train_loss 2.46774816513\n",
      "epoch 9, batch 374, train_loss 2.80211544037\n",
      "epoch 9, batch 375, train_loss 3.19202494621\n",
      "epoch 9, batch 376, train_loss 2.45232772827\n",
      "epoch 9, batch 377, train_loss 2.85593223572\n",
      "epoch 9, batch 378, train_loss 3.55826544762\n",
      "epoch 9, batch 379, train_loss 2.72976350784\n",
      "epoch 9, batch 380, train_loss 2.75518918037\n",
      "epoch 9, batch 381, train_loss 2.85174632072\n",
      "epoch 9, batch 382, train_loss 3.72452950478\n",
      "epoch 9, batch 383, train_loss 3.30132436752\n",
      "epoch 9, batch 384, train_loss 2.96452856064\n",
      "epoch 9, batch 385, train_loss 4.35279607773\n",
      "epoch 9, batch 386, train_loss 2.66188693047\n",
      "epoch 9, batch 387, train_loss 4.50359201431\n",
      "epoch 9, batch 388, train_loss 2.66933274269\n",
      "epoch 9, batch 389, train_loss 4.23455095291\n",
      "epoch 9, batch 390, train_loss 4.22704267502\n",
      "epoch 9, batch 391, train_loss 4.25888252258\n",
      "epoch 9, batch 392, train_loss 4.0180311203\n",
      "epoch 9, batch 393, train_loss 3.25911784172\n",
      "epoch 9, batch 394, train_loss 3.88801074028\n",
      "epoch 9, batch 395, train_loss 2.84747433662\n",
      "epoch 9, batch 396, train_loss 4.13059663773\n",
      "epoch 9, batch 397, train_loss 4.23342132568\n",
      "epoch 9, batch 398, train_loss 3.18510055542\n",
      "epoch 9, batch 399, train_loss 3.4129345417\n",
      "epoch 9, batch 400, train_loss 4.378074646\n",
      "epoch 9, batch 401, train_loss 3.6202378273\n",
      "epoch 9, batch 402, train_loss 3.20485115051\n",
      "epoch 9, batch 403, train_loss 3.14069080353\n",
      "epoch 9, batch 404, train_loss 3.97865629196\n",
      "epoch 9, batch 405, train_loss 3.48445701599\n",
      "epoch 9, batch 406, train_loss 3.33955574036\n",
      "epoch 9, batch 407, train_loss 3.13836741447\n",
      "epoch 9, batch 408, train_loss 2.99020147324\n",
      "epoch 9, batch 409, train_loss 2.50936055183\n",
      "epoch 9, batch 410, train_loss 4.12091732025\n",
      "epoch 9, batch 411, train_loss 3.21846079826\n",
      "epoch 9, batch 412, train_loss 2.72841668129\n",
      "epoch 9, batch 413, train_loss 3.07272291183\n",
      "epoch 9, batch 414, train_loss 3.59510588646\n",
      "epoch 9, batch 415, train_loss 3.66388773918\n",
      "epoch 9, batch 416, train_loss 3.41146111488\n",
      "epoch 9, batch 417, train_loss 2.96095776558\n",
      "epoch 9, batch 418, train_loss 3.36411547661\n",
      "epoch 9, batch 419, train_loss 2.9914059639\n",
      "epoch 9, batch 420, train_loss 2.43402934074\n",
      "epoch 9, batch 421, train_loss 4.232213974\n",
      "epoch 9, batch 422, train_loss 2.9705080986\n",
      "epoch 9, batch 423, train_loss 3.09004616737\n",
      "epoch 9, batch 424, train_loss 4.17329788208\n",
      "epoch 9, batch 425, train_loss 3.48393917084\n",
      "epoch 9, batch 426, train_loss 4.63708686829\n",
      "epoch 9, batch 427, train_loss 3.77474474907\n",
      "epoch 9, batch 428, train_loss 2.58002281189\n",
      "epoch 9, batch 429, train_loss 3.58281087875\n",
      "epoch 9, batch 430, train_loss 3.63464212418\n",
      "epoch 9, batch 431, train_loss 3.23790550232\n",
      "epoch 9, batch 432, train_loss 3.44207048416\n",
      "epoch 9, batch 433, train_loss 2.85309576988\n",
      "epoch 9, batch 434, train_loss 3.36215996742\n",
      "epoch 9, batch 435, train_loss 3.16728925705\n",
      "epoch 9, batch 436, train_loss 3.51305270195\n",
      "epoch 9, batch 437, train_loss 3.96356654167\n",
      "epoch 9, batch 438, train_loss 3.62093544006\n",
      "epoch 9, batch 439, train_loss 3.37877178192\n",
      "epoch 9, batch 440, train_loss 4.6648235321\n",
      "epoch 9, batch 441, train_loss 4.78228187561\n",
      "epoch 9, batch 442, train_loss 3.9648578167\n",
      "epoch 9, batch 443, train_loss 2.43018531799\n",
      "epoch 9, batch 444, train_loss 2.95070123672\n",
      "epoch 9, batch 445, train_loss 4.53656053543\n",
      "epoch 9, batch 446, train_loss 3.30615854263\n",
      "epoch 9, batch 447, train_loss 3.00226616859\n",
      "epoch 9, batch 448, train_loss 4.41712760925\n",
      "epoch 9, batch 449, train_loss 3.80497908592\n",
      "epoch 9, batch 450, train_loss 3.13030171394\n",
      "epoch 9, batch 451, train_loss 2.87599563599\n",
      "epoch 9, batch 452, train_loss 2.63571810722\n",
      "epoch 9, batch 453, train_loss 5.08656644821\n",
      "epoch 9, batch 454, train_loss 4.93600082397\n",
      "epoch 9, batch 455, train_loss 2.52264499664\n",
      "epoch 9, batch 456, train_loss 3.19707417488\n",
      "epoch 9, batch 457, train_loss 3.42546391487\n",
      "epoch 9, batch 458, train_loss 3.06862902641\n",
      "epoch 9, batch 459, train_loss 3.34383320808\n",
      "epoch 9, batch 460, train_loss 3.83118963242\n",
      "epoch 9, batch 461, train_loss 3.33747601509\n",
      "epoch 9, batch 462, train_loss 3.01213002205\n",
      "epoch 9, batch 463, train_loss 3.16162180901\n",
      "epoch 9, batch 464, train_loss 3.44355392456\n",
      "epoch 9, batch 465, train_loss 2.84377074242\n",
      "epoch 9, batch 466, train_loss 3.60864806175\n",
      "epoch 9, batch 467, train_loss 3.43082523346\n",
      "epoch 9, batch 468, train_loss 3.57186317444\n",
      "epoch 9, batch 469, train_loss 3.13918232918\n",
      "epoch 9, batch 470, train_loss 3.00642538071\n",
      "epoch 9, batch 471, train_loss 3.08546900749\n",
      "epoch 9, batch 472, train_loss 2.95251393318\n",
      "epoch 9, batch 473, train_loss 3.6449816227\n",
      "epoch 9, batch 474, train_loss 4.22705841064\n",
      "epoch 9, batch 475, train_loss 2.97197031975\n",
      "epoch 9, batch 476, train_loss 2.4896235466\n",
      "epoch 9, batch 477, train_loss 2.57818841934\n",
      "epoch 9, batch 478, train_loss 2.41678309441\n",
      "epoch 9, batch 479, train_loss 2.92276930809\n",
      "epoch 9, batch 480, train_loss 3.11472439766\n",
      "epoch 9, batch 481, train_loss 4.67458724976\n",
      "epoch 9, batch 482, train_loss 2.83053398132\n",
      "epoch 9, batch 483, train_loss 3.44852590561\n",
      "epoch 9, batch 484, train_loss 3.12145066261\n",
      "epoch 9, batch 485, train_loss 2.35392642021\n",
      "epoch 9, batch 486, train_loss 2.35995411873\n",
      "epoch 9, batch 487, train_loss 2.52300190926\n",
      "epoch 9, batch 488, train_loss 2.92496442795\n",
      "epoch 9, batch 489, train_loss 3.04860162735\n",
      "epoch 9, batch 490, train_loss 2.96659827232\n",
      "epoch 9, batch 491, train_loss 2.89144825935\n",
      "epoch 9, batch 492, train_loss 3.23299789429\n",
      "epoch 9, batch 493, train_loss 2.82738161087\n",
      "epoch 9, batch 494, train_loss 2.91848993301\n",
      "epoch 9, batch 495, train_loss 2.99465417862\n",
      "epoch 9, batch 496, train_loss 2.62448215485\n",
      "epoch 9, batch 497, train_loss 2.80066800117\n",
      "epoch 9, batch 498, train_loss 2.93506932259\n",
      "epoch 9, batch 499, train_loss 2.89233613014\n",
      "epoch 9, batch 500, train_loss 3.33905649185\n",
      "epoch 9, batch 501, train_loss 2.99676942825\n",
      "epoch 9, batch 502, train_loss 4.26853895187\n",
      "epoch 9, batch 503, train_loss 4.19697189331\n",
      "epoch 9, batch 504, train_loss 3.66197752953\n",
      "epoch 9, batch 505, train_loss 3.47269916534\n",
      "epoch 9, batch 506, train_loss 3.14823031425\n",
      "epoch 9, batch 507, train_loss 4.14630174637\n",
      "epoch 9, batch 508, train_loss 2.97481536865\n",
      "epoch 9, batch 509, train_loss 4.29647445679\n",
      "epoch 9, batch 510, train_loss 4.2623462677\n",
      "epoch 9, batch 511, train_loss 4.27660322189\n",
      "epoch 9, batch 512, train_loss 3.33313679695\n",
      "epoch 9, batch 513, train_loss 4.71420955658\n",
      "epoch 9, batch 514, train_loss 4.64717960358\n",
      "epoch 9, batch 515, train_loss 4.54302978516\n",
      "epoch 9, batch 516, train_loss 2.82632160187\n",
      "epoch 9, batch 517, train_loss 3.02438092232\n",
      "epoch 9, batch 518, train_loss 2.92920970917\n",
      "epoch 9, batch 519, train_loss 2.61261820793\n",
      "epoch 9, batch 520, train_loss 3.7548494339\n",
      "epoch 9, batch 521, train_loss 2.93654322624\n",
      "epoch 9, batch 522, train_loss 2.56193161011\n",
      "epoch 9, batch 523, train_loss 2.55365943909\n",
      "epoch 9, batch 524, train_loss 2.40541601181\n",
      "epoch 9, batch 525, train_loss 2.23686361313\n",
      "epoch 9, batch 526, train_loss 2.33073449135\n",
      "epoch 9, batch 527, train_loss 2.35815358162\n",
      "epoch 9, batch 528, train_loss 2.59822630882\n",
      "epoch 9, batch 529, train_loss 2.48700165749\n",
      "epoch 9, batch 530, train_loss 1.98417520523\n",
      "epoch 9, batch 531, train_loss 1.74696660042\n",
      "epoch 9, batch 532, train_loss 2.57586121559\n",
      "epoch 9, batch 533, train_loss 1.64304637909\n",
      "epoch 9, batch 534, train_loss 1.70821928978\n",
      "epoch 9, batch 535, train_loss 3.13060998917\n",
      "epoch 9, batch 536, train_loss 3.35908842087\n",
      "epoch 9, batch 537, train_loss 3.23222994804\n",
      "epoch 9, batch 538, train_loss 3.35796403885\n",
      "epoch 9, batch 539, train_loss 3.62439084053\n",
      "epoch 9, batch 540, train_loss 3.95487737656\n",
      "epoch 10, batch 0, train_loss 3.34524559975\n",
      "epoch 10, batch 1, train_loss 3.68888235092\n",
      "epoch 10, batch 2, train_loss 3.0223596096\n",
      "epoch 10, batch 3, train_loss 3.29685282707\n",
      "epoch 10, batch 4, train_loss 3.02530622482\n",
      "epoch 10, batch 5, train_loss 3.09505414963\n",
      "epoch 10, batch 6, train_loss 4.03477716446\n",
      "epoch 10, batch 7, train_loss 3.06039476395\n",
      "epoch 10, batch 8, train_loss 2.71561956406\n",
      "epoch 10, batch 9, train_loss 3.67874336243\n",
      "epoch 10, batch 10, train_loss 2.96201205254\n",
      "epoch 10, batch 11, train_loss 3.02516436577\n",
      "epoch 10, batch 12, train_loss 2.76180529594\n",
      "epoch 10, batch 13, train_loss 3.24466466904\n",
      "epoch 10, batch 14, train_loss 2.67619204521\n",
      "epoch 10, batch 15, train_loss 3.39347600937\n",
      "epoch 10, batch 16, train_loss 2.9409570694\n",
      "epoch 10, batch 17, train_loss 2.79907202721\n",
      "epoch 10, batch 18, train_loss 2.46802759171\n",
      "epoch 10, batch 19, train_loss 2.03690457344\n",
      "epoch 10, batch 20, train_loss 2.86947274208\n",
      "epoch 10, batch 21, train_loss 2.80649757385\n",
      "epoch 10, batch 22, train_loss 3.28741502762\n",
      "epoch 10, batch 23, train_loss 2.86982059479\n",
      "epoch 10, batch 24, train_loss 2.86800384521\n",
      "epoch 10, batch 25, train_loss 3.30231738091\n",
      "epoch 10, batch 26, train_loss 3.23902368546\n",
      "epoch 10, batch 27, train_loss 3.15401625633\n",
      "epoch 10, batch 28, train_loss 3.44854426384\n",
      "epoch 10, batch 29, train_loss 3.34208869934\n",
      "epoch 10, batch 30, train_loss 3.44279694557\n",
      "epoch 10, batch 31, train_loss 2.88419413567\n",
      "epoch 10, batch 32, train_loss 2.67217850685\n",
      "epoch 10, batch 33, train_loss 4.47491979599\n",
      "epoch 10, batch 34, train_loss 4.5649356842\n",
      "epoch 10, batch 35, train_loss 3.23066186905\n",
      "epoch 10, batch 36, train_loss 3.254809618\n",
      "epoch 10, batch 37, train_loss 3.19593286514\n",
      "epoch 10, batch 38, train_loss 3.34737920761\n",
      "epoch 10, batch 39, train_loss 3.81095314026\n",
      "epoch 10, batch 40, train_loss 2.90293383598\n",
      "epoch 10, batch 41, train_loss 3.19268012047\n",
      "epoch 10, batch 42, train_loss 3.20005488396\n",
      "epoch 10, batch 43, train_loss 2.96821761131\n",
      "epoch 10, batch 44, train_loss 3.61642003059\n",
      "epoch 10, batch 45, train_loss 3.23505020142\n",
      "epoch 10, batch 46, train_loss 3.19263577461\n",
      "epoch 10, batch 47, train_loss 3.27647280693\n",
      "epoch 10, batch 48, train_loss 3.09978652\n",
      "epoch 10, batch 49, train_loss 3.23617434502\n",
      "epoch 10, batch 50, train_loss 3.1539144516\n",
      "epoch 10, batch 51, train_loss 2.81041932106\n",
      "epoch 10, batch 52, train_loss 3.25897836685\n",
      "epoch 10, batch 53, train_loss 3.11241769791\n",
      "epoch 10, batch 54, train_loss 2.88040065765\n",
      "epoch 10, batch 55, train_loss 2.92060804367\n",
      "epoch 10, batch 56, train_loss 2.80902647972\n",
      "epoch 10, batch 57, train_loss 3.27972483635\n",
      "epoch 10, batch 58, train_loss 3.09384775162\n",
      "epoch 10, batch 59, train_loss 3.04822516441\n",
      "epoch 10, batch 60, train_loss 3.24041604996\n",
      "epoch 10, batch 61, train_loss 1.86814630032\n",
      "epoch 10, batch 62, train_loss 2.8211479187\n",
      "epoch 10, batch 63, train_loss 2.96960139275\n",
      "epoch 10, batch 64, train_loss 3.21595406532\n",
      "epoch 10, batch 65, train_loss 3.52731823921\n",
      "epoch 10, batch 66, train_loss 3.29068565369\n",
      "epoch 10, batch 67, train_loss 2.12639975548\n",
      "epoch 10, batch 68, train_loss 2.98467946053\n",
      "epoch 10, batch 69, train_loss 2.33780145645\n",
      "epoch 10, batch 70, train_loss 3.84962391853\n",
      "epoch 10, batch 71, train_loss 3.9143512249\n",
      "epoch 10, batch 72, train_loss 2.78118371964\n",
      "epoch 10, batch 73, train_loss 3.19243121147\n",
      "epoch 10, batch 74, train_loss 3.74495649338\n",
      "epoch 10, batch 75, train_loss 3.2334856987\n",
      "epoch 10, batch 76, train_loss 3.51455116272\n",
      "epoch 10, batch 77, train_loss 4.1844959259\n",
      "epoch 10, batch 78, train_loss 3.06762933731\n",
      "epoch 10, batch 79, train_loss 3.08874845505\n",
      "epoch 10, batch 80, train_loss 2.91743063927\n",
      "epoch 10, batch 81, train_loss 3.06110477448\n",
      "epoch 10, batch 82, train_loss 2.95644807816\n",
      "epoch 10, batch 83, train_loss 3.09534072876\n",
      "epoch 10, batch 84, train_loss 2.8413002491\n",
      "epoch 10, batch 85, train_loss 2.86108493805\n",
      "epoch 10, batch 86, train_loss 3.38184404373\n",
      "epoch 10, batch 87, train_loss 2.58537697792\n",
      "epoch 10, batch 88, train_loss 2.89613580704\n",
      "epoch 10, batch 89, train_loss 3.19368052483\n",
      "epoch 10, batch 90, train_loss 2.89727640152\n",
      "epoch 10, batch 91, train_loss 2.93066906929\n",
      "epoch 10, batch 92, train_loss 3.38065218925\n",
      "epoch 10, batch 93, train_loss 4.11568975449\n",
      "epoch 10, batch 94, train_loss 2.94262337685\n",
      "epoch 10, batch 95, train_loss 3.01140117645\n",
      "epoch 10, batch 96, train_loss 2.39775490761\n",
      "epoch 10, batch 97, train_loss 2.99466443062\n",
      "epoch 10, batch 98, train_loss 2.93553566933\n",
      "epoch 10, batch 99, train_loss 3.16230130196\n",
      "epoch 10, batch 100, train_loss 2.95045852661\n",
      "epoch 10, batch 101, train_loss 3.1021053791\n",
      "epoch 10, batch 102, train_loss 2.92488861084\n",
      "epoch 10, batch 103, train_loss 4.16220426559\n",
      "epoch 10, batch 104, train_loss 3.43837141991\n",
      "epoch 10, batch 105, train_loss 3.25591611862\n",
      "epoch 10, batch 106, train_loss 3.64401388168\n",
      "epoch 10, batch 107, train_loss 3.46750378609\n",
      "epoch 10, batch 108, train_loss 3.09075641632\n",
      "epoch 10, batch 109, train_loss 3.35834145546\n",
      "epoch 10, batch 110, train_loss 3.16994404793\n",
      "epoch 10, batch 111, train_loss 3.57618308067\n",
      "epoch 10, batch 112, train_loss 3.52492046356\n",
      "epoch 10, batch 113, train_loss 2.93280029297\n",
      "epoch 10, batch 114, train_loss 3.51258468628\n",
      "epoch 10, batch 115, train_loss 4.2103304863\n",
      "epoch 10, batch 116, train_loss 4.09455299377\n",
      "epoch 10, batch 117, train_loss 4.25381612778\n",
      "epoch 10, batch 118, train_loss 3.69150495529\n",
      "epoch 10, batch 119, train_loss 2.44006705284\n",
      "epoch 10, batch 120, train_loss 3.25463080406\n",
      "epoch 10, batch 121, train_loss 3.07145595551\n",
      "epoch 10, batch 122, train_loss 3.28390908241\n",
      "epoch 10, batch 123, train_loss 3.05860638618\n",
      "epoch 10, batch 124, train_loss 2.76161360741\n",
      "epoch 10, batch 125, train_loss 2.73337864876\n",
      "epoch 10, batch 126, train_loss 2.70750880241\n",
      "epoch 10, batch 127, train_loss 2.77394461632\n",
      "epoch 10, batch 128, train_loss 2.60704421997\n",
      "epoch 10, batch 129, train_loss 3.01348280907\n",
      "epoch 10, batch 130, train_loss 2.95346999168\n",
      "epoch 10, batch 131, train_loss 3.31746721268\n",
      "epoch 10, batch 132, train_loss 3.06230592728\n",
      "epoch 10, batch 133, train_loss 2.31197023392\n",
      "epoch 10, batch 134, train_loss 2.33845686913\n",
      "epoch 10, batch 135, train_loss 4.07300615311\n",
      "epoch 10, batch 136, train_loss 3.50740265846\n",
      "epoch 10, batch 137, train_loss 2.68995952606\n",
      "epoch 10, batch 138, train_loss 2.54848837852\n",
      "epoch 10, batch 139, train_loss 3.06126236916\n",
      "epoch 10, batch 140, train_loss 2.55448079109\n",
      "epoch 10, batch 141, train_loss 2.86122727394\n",
      "epoch 10, batch 142, train_loss 4.05549049377\n",
      "epoch 10, batch 143, train_loss 2.55755543709\n",
      "epoch 10, batch 144, train_loss 2.08358669281\n",
      "epoch 10, batch 145, train_loss 3.05212712288\n",
      "epoch 10, batch 146, train_loss 3.23854231834\n",
      "epoch 10, batch 147, train_loss 2.96932005882\n",
      "epoch 10, batch 148, train_loss 2.87051773071\n",
      "epoch 10, batch 149, train_loss 3.29132771492\n",
      "epoch 10, batch 150, train_loss 3.41905450821\n",
      "epoch 10, batch 151, train_loss 2.09850382805\n",
      "epoch 10, batch 152, train_loss 2.33234500885\n",
      "epoch 10, batch 153, train_loss 3.96994447708\n",
      "epoch 10, batch 154, train_loss 3.1478729248\n",
      "epoch 10, batch 155, train_loss 2.7437517643\n",
      "epoch 10, batch 156, train_loss 2.94125127792\n",
      "epoch 10, batch 157, train_loss 2.53956341743\n",
      "epoch 10, batch 158, train_loss 2.55164432526\n",
      "epoch 10, batch 159, train_loss 2.7566652298\n",
      "epoch 10, batch 160, train_loss 2.85358190536\n",
      "epoch 10, batch 161, train_loss 3.38411808014\n",
      "epoch 10, batch 162, train_loss 3.3625023365\n",
      "epoch 10, batch 163, train_loss 4.79750442505\n",
      "epoch 10, batch 164, train_loss 2.31727075577\n",
      "epoch 10, batch 165, train_loss 2.2725250721\n",
      "epoch 10, batch 166, train_loss 2.5764605999\n",
      "epoch 10, batch 167, train_loss 2.60696935654\n",
      "epoch 10, batch 168, train_loss 2.82265257835\n",
      "epoch 10, batch 169, train_loss 2.62804150581\n",
      "epoch 10, batch 170, train_loss 3.13905310631\n",
      "epoch 10, batch 171, train_loss 2.5547683239\n",
      "epoch 10, batch 172, train_loss 3.15311336517\n",
      "epoch 10, batch 173, train_loss 2.1160068512\n",
      "epoch 10, batch 174, train_loss 3.63180613518\n",
      "epoch 10, batch 175, train_loss 3.12003517151\n",
      "epoch 10, batch 176, train_loss 3.01099705696\n",
      "epoch 10, batch 177, train_loss 2.82114076614\n",
      "epoch 10, batch 178, train_loss 2.75346803665\n",
      "epoch 10, batch 179, train_loss 3.01177144051\n",
      "epoch 10, batch 180, train_loss 2.87859082222\n",
      "epoch 10, batch 181, train_loss 3.49449110031\n",
      "epoch 10, batch 182, train_loss 2.99966025352\n",
      "epoch 10, batch 183, train_loss 3.64928722382\n",
      "epoch 10, batch 184, train_loss 2.37913012505\n",
      "epoch 10, batch 185, train_loss 2.73543000221\n",
      "epoch 10, batch 186, train_loss 3.33423686028\n",
      "epoch 10, batch 187, train_loss 2.42858362198\n",
      "epoch 10, batch 188, train_loss 2.77249932289\n",
      "epoch 10, batch 189, train_loss 2.7525947094\n",
      "epoch 10, batch 190, train_loss 3.51162385941\n",
      "epoch 10, batch 191, train_loss 3.36413311958\n",
      "epoch 10, batch 192, train_loss 4.30430698395\n",
      "epoch 10, batch 193, train_loss 4.02569484711\n",
      "epoch 10, batch 194, train_loss 4.62795114517\n",
      "epoch 10, batch 195, train_loss 4.70970344543\n",
      "epoch 10, batch 196, train_loss 3.15569710732\n",
      "epoch 10, batch 197, train_loss 2.51554322243\n",
      "epoch 10, batch 198, train_loss 2.31391429901\n",
      "epoch 10, batch 199, train_loss 3.30998587608\n",
      "epoch 10, batch 200, train_loss 2.78736543655\n",
      "epoch 10, batch 201, train_loss 3.08550620079\n",
      "epoch 10, batch 202, train_loss 3.52444291115\n",
      "epoch 10, batch 203, train_loss 3.65992856026\n",
      "epoch 10, batch 204, train_loss 3.65119886398\n",
      "epoch 10, batch 205, train_loss 3.0971531868\n",
      "epoch 10, batch 206, train_loss 4.11403989792\n",
      "epoch 10, batch 207, train_loss 3.47065806389\n",
      "epoch 10, batch 208, train_loss 4.1770029068\n",
      "epoch 10, batch 209, train_loss 3.91189432144\n",
      "epoch 10, batch 210, train_loss 2.13961386681\n",
      "epoch 10, batch 211, train_loss 2.89049577713\n",
      "epoch 10, batch 212, train_loss 3.80987143517\n",
      "epoch 10, batch 213, train_loss 3.13704442978\n",
      "epoch 10, batch 214, train_loss 3.00331521034\n",
      "epoch 10, batch 215, train_loss 2.22299480438\n",
      "epoch 10, batch 216, train_loss 3.36277675629\n",
      "epoch 10, batch 217, train_loss 2.80913710594\n",
      "epoch 10, batch 218, train_loss 3.27168893814\n",
      "epoch 10, batch 219, train_loss 3.37600159645\n",
      "epoch 10, batch 220, train_loss 3.41706514359\n",
      "epoch 10, batch 221, train_loss 3.55830550194\n",
      "epoch 10, batch 222, train_loss 2.30258059502\n",
      "epoch 10, batch 223, train_loss 2.30172944069\n",
      "epoch 10, batch 224, train_loss 2.65513253212\n",
      "epoch 10, batch 225, train_loss 2.7829887867\n",
      "epoch 10, batch 226, train_loss 3.29831004143\n",
      "epoch 10, batch 227, train_loss 3.05957555771\n",
      "epoch 10, batch 228, train_loss 3.21494483948\n",
      "epoch 10, batch 229, train_loss 3.03452324867\n",
      "epoch 10, batch 230, train_loss 2.90774250031\n",
      "epoch 10, batch 231, train_loss 2.60511732101\n",
      "epoch 10, batch 232, train_loss 3.25466990471\n",
      "epoch 10, batch 233, train_loss 3.50624465942\n",
      "epoch 10, batch 234, train_loss 3.08246994019\n",
      "epoch 10, batch 235, train_loss 3.63889575005\n",
      "epoch 10, batch 236, train_loss 3.17361164093\n",
      "epoch 10, batch 237, train_loss 3.07749390602\n",
      "epoch 10, batch 238, train_loss 3.10648846626\n",
      "epoch 10, batch 239, train_loss 2.9231402874\n",
      "epoch 10, batch 240, train_loss 3.36567616463\n",
      "epoch 10, batch 241, train_loss 3.28760814667\n",
      "epoch 10, batch 242, train_loss 3.11021161079\n",
      "epoch 10, batch 243, train_loss 3.30746293068\n",
      "epoch 10, batch 244, train_loss 3.44131183624\n",
      "epoch 10, batch 245, train_loss 3.20572900772\n",
      "epoch 10, batch 246, train_loss 3.12386083603\n",
      "epoch 10, batch 247, train_loss 3.31226420403\n",
      "epoch 10, batch 248, train_loss 3.05276584625\n",
      "epoch 10, batch 249, train_loss 3.74473690987\n",
      "epoch 10, batch 250, train_loss 3.50229120255\n",
      "epoch 10, batch 251, train_loss 3.8581905365\n",
      "epoch 10, batch 252, train_loss 2.66165757179\n",
      "epoch 10, batch 253, train_loss 2.20229816437\n",
      "epoch 10, batch 254, train_loss 2.82082629204\n",
      "epoch 10, batch 255, train_loss 2.19037294388\n",
      "epoch 10, batch 256, train_loss 2.56946539879\n",
      "epoch 10, batch 257, train_loss 3.02737402916\n",
      "epoch 10, batch 258, train_loss 2.40745258331\n",
      "epoch 10, batch 259, train_loss 2.34621167183\n",
      "epoch 10, batch 260, train_loss 2.10484266281\n",
      "epoch 10, batch 261, train_loss 3.44310998917\n",
      "epoch 10, batch 262, train_loss 4.04121589661\n",
      "epoch 10, batch 263, train_loss 3.14391088486\n",
      "epoch 10, batch 264, train_loss 2.85454010963\n",
      "epoch 10, batch 265, train_loss 3.48220825195\n",
      "epoch 10, batch 266, train_loss 3.52714157104\n",
      "epoch 10, batch 267, train_loss 2.78388595581\n",
      "epoch 10, batch 268, train_loss 2.32376146317\n",
      "epoch 10, batch 269, train_loss 2.40147781372\n",
      "epoch 10, batch 270, train_loss 2.70054721832\n",
      "epoch 10, batch 271, train_loss 3.04149603844\n",
      "epoch 10, batch 272, train_loss 3.06251454353\n",
      "epoch 10, batch 273, train_loss 3.17740035057\n",
      "epoch 10, batch 274, train_loss 2.99195027351\n",
      "epoch 10, batch 275, train_loss 3.00266480446\n",
      "epoch 10, batch 276, train_loss 2.78928542137\n",
      "epoch 10, batch 277, train_loss 3.60883903503\n",
      "epoch 10, batch 278, train_loss 2.74250721931\n",
      "epoch 10, batch 279, train_loss 3.27862906456\n",
      "epoch 10, batch 280, train_loss 3.26266908646\n",
      "epoch 10, batch 281, train_loss 3.07575750351\n",
      "epoch 10, batch 282, train_loss 2.70418930054\n",
      "epoch 10, batch 283, train_loss 4.27272605896\n",
      "epoch 10, batch 284, train_loss 3.21043729782\n",
      "epoch 10, batch 285, train_loss 1.97929608822\n",
      "epoch 10, batch 286, train_loss 2.22090649605\n",
      "epoch 10, batch 287, train_loss 2.32451462746\n",
      "epoch 10, batch 288, train_loss 2.96861886978\n",
      "epoch 10, batch 289, train_loss 3.01759576797\n",
      "epoch 10, batch 290, train_loss 2.85328602791\n",
      "epoch 10, batch 291, train_loss 2.72764086723\n",
      "epoch 10, batch 292, train_loss 2.24291849136\n",
      "epoch 10, batch 293, train_loss 3.31122732162\n",
      "epoch 10, batch 294, train_loss 2.8974840641\n",
      "epoch 10, batch 295, train_loss 2.61812353134\n",
      "epoch 10, batch 296, train_loss 2.90582990646\n",
      "epoch 10, batch 297, train_loss 2.57400894165\n",
      "epoch 10, batch 298, train_loss 2.80701756477\n",
      "epoch 10, batch 299, train_loss 3.3394446373\n",
      "epoch 10, batch 300, train_loss 3.07943177223\n",
      "epoch 10, batch 301, train_loss 4.03600931168\n",
      "epoch 10, batch 302, train_loss 4.04887056351\n",
      "epoch 10, batch 303, train_loss 3.72714591026\n",
      "epoch 10, batch 304, train_loss 4.38046884537\n",
      "epoch 10, batch 305, train_loss 4.34429883957\n",
      "epoch 10, batch 306, train_loss 2.99895119667\n",
      "epoch 10, batch 307, train_loss 2.50361919403\n",
      "epoch 10, batch 308, train_loss 2.94808244705\n",
      "epoch 10, batch 309, train_loss 3.10650205612\n",
      "epoch 10, batch 310, train_loss 2.9202299118\n",
      "epoch 10, batch 311, train_loss 3.02675962448\n",
      "epoch 10, batch 312, train_loss 3.20632243156\n",
      "epoch 10, batch 313, train_loss 2.92015719414\n",
      "epoch 10, batch 314, train_loss 3.25338983536\n",
      "epoch 10, batch 315, train_loss 2.73003125191\n",
      "epoch 10, batch 316, train_loss 2.88916182518\n",
      "epoch 10, batch 317, train_loss 3.36754846573\n",
      "epoch 10, batch 318, train_loss 2.49826407433\n",
      "epoch 10, batch 319, train_loss 4.20713472366\n",
      "epoch 10, batch 320, train_loss 3.08186936378\n",
      "epoch 10, batch 321, train_loss 4.29873561859\n",
      "epoch 10, batch 322, train_loss 2.39176893234\n",
      "epoch 10, batch 323, train_loss 2.26661157608\n",
      "epoch 10, batch 324, train_loss 3.3799302578\n",
      "epoch 10, batch 325, train_loss 3.05659723282\n",
      "epoch 10, batch 326, train_loss 3.04539847374\n",
      "epoch 10, batch 327, train_loss 3.02377057076\n",
      "epoch 10, batch 328, train_loss 2.90060353279\n",
      "epoch 10, batch 329, train_loss 2.57977461815\n",
      "epoch 10, batch 330, train_loss 3.91042399406\n",
      "epoch 10, batch 331, train_loss 3.59415984154\n",
      "epoch 10, batch 332, train_loss 2.42764043808\n",
      "epoch 10, batch 333, train_loss 3.47183728218\n",
      "epoch 10, batch 334, train_loss 3.17869329453\n",
      "epoch 10, batch 335, train_loss 3.08603739738\n",
      "epoch 10, batch 336, train_loss 3.64461946487\n",
      "epoch 10, batch 337, train_loss 2.23514008522\n",
      "epoch 10, batch 338, train_loss 3.05340456963\n",
      "epoch 10, batch 339, train_loss 2.84282326698\n",
      "epoch 10, batch 340, train_loss 3.04297637939\n",
      "epoch 10, batch 341, train_loss 2.98197317123\n",
      "epoch 10, batch 342, train_loss 3.02174448967\n",
      "epoch 10, batch 343, train_loss 2.91995000839\n",
      "epoch 10, batch 344, train_loss 3.84615087509\n",
      "epoch 10, batch 345, train_loss 3.27010178566\n",
      "epoch 10, batch 346, train_loss 3.03968429565\n",
      "epoch 10, batch 347, train_loss 2.84163355827\n",
      "epoch 10, batch 348, train_loss 3.0993142128\n",
      "epoch 10, batch 349, train_loss 4.09423351288\n",
      "epoch 10, batch 350, train_loss 4.06999874115\n",
      "epoch 10, batch 351, train_loss 3.87405490875\n",
      "epoch 10, batch 352, train_loss 3.92096138\n",
      "epoch 10, batch 353, train_loss 3.19052386284\n",
      "epoch 10, batch 354, train_loss 2.58058309555\n",
      "epoch 10, batch 355, train_loss 2.89003276825\n",
      "epoch 10, batch 356, train_loss 2.85244059563\n",
      "epoch 10, batch 357, train_loss 2.79608726501\n",
      "epoch 10, batch 358, train_loss 2.36877417564\n",
      "epoch 10, batch 359, train_loss 3.33329677582\n",
      "epoch 10, batch 360, train_loss 2.83344817162\n",
      "epoch 10, batch 361, train_loss 2.51393771172\n",
      "epoch 10, batch 362, train_loss 4.21710968018\n",
      "epoch 10, batch 363, train_loss 3.11193776131\n",
      "epoch 10, batch 364, train_loss 3.01023244858\n",
      "epoch 10, batch 365, train_loss 3.88976597786\n",
      "epoch 10, batch 366, train_loss 4.01854896545\n",
      "epoch 10, batch 367, train_loss 4.75969982147\n",
      "epoch 10, batch 368, train_loss 2.4817070961\n",
      "epoch 10, batch 369, train_loss 3.39781546593\n",
      "epoch 10, batch 370, train_loss 3.79258227348\n",
      "epoch 10, batch 371, train_loss 4.72486162186\n",
      "epoch 10, batch 372, train_loss 2.2741496563\n",
      "epoch 10, batch 373, train_loss 2.44105267525\n",
      "epoch 10, batch 374, train_loss 2.77570033073\n",
      "epoch 10, batch 375, train_loss 3.16079950333\n",
      "epoch 10, batch 376, train_loss 2.42820549011\n",
      "epoch 10, batch 377, train_loss 2.82642054558\n",
      "epoch 10, batch 378, train_loss 3.52345204353\n",
      "epoch 10, batch 379, train_loss 2.69895768166\n",
      "epoch 10, batch 380, train_loss 2.72573328018\n",
      "epoch 10, batch 381, train_loss 2.82216143608\n",
      "epoch 10, batch 382, train_loss 3.67606902122\n",
      "epoch 10, batch 383, train_loss 3.26591110229\n",
      "epoch 10, batch 384, train_loss 2.93502688408\n",
      "epoch 10, batch 385, train_loss 4.30787992477\n",
      "epoch 10, batch 386, train_loss 2.63072252274\n",
      "epoch 10, batch 387, train_loss 4.42505979538\n",
      "epoch 10, batch 388, train_loss 2.6371922493\n",
      "epoch 10, batch 389, train_loss 4.18457555771\n",
      "epoch 10, batch 390, train_loss 4.18146562576\n",
      "epoch 10, batch 391, train_loss 4.21722602844\n",
      "epoch 10, batch 392, train_loss 3.98036694527\n",
      "epoch 10, batch 393, train_loss 3.22192955017\n",
      "epoch 10, batch 394, train_loss 3.84186124802\n",
      "epoch 10, batch 395, train_loss 2.81631040573\n",
      "epoch 10, batch 396, train_loss 4.08334064484\n",
      "epoch 10, batch 397, train_loss 4.18101549149\n",
      "epoch 10, batch 398, train_loss 3.15348291397\n",
      "epoch 10, batch 399, train_loss 3.37216091156\n",
      "epoch 10, batch 400, train_loss 4.31939697266\n",
      "epoch 10, batch 401, train_loss 3.57777738571\n",
      "epoch 10, batch 402, train_loss 3.17085576057\n",
      "epoch 10, batch 403, train_loss 3.10521221161\n",
      "epoch 10, batch 404, train_loss 3.93309545517\n",
      "epoch 10, batch 405, train_loss 3.44423532486\n",
      "epoch 10, batch 406, train_loss 3.30241250992\n",
      "epoch 10, batch 407, train_loss 3.10470032692\n",
      "epoch 10, batch 408, train_loss 2.95688986778\n",
      "epoch 10, batch 409, train_loss 2.48133397102\n",
      "epoch 10, batch 410, train_loss 4.08360767365\n",
      "epoch 10, batch 411, train_loss 3.18522596359\n",
      "epoch 10, batch 412, train_loss 2.69840979576\n",
      "epoch 10, batch 413, train_loss 3.03561210632\n",
      "epoch 10, batch 414, train_loss 3.55396652222\n",
      "epoch 10, batch 415, train_loss 3.62673711777\n",
      "epoch 10, batch 416, train_loss 3.37498688698\n",
      "epoch 10, batch 417, train_loss 2.92817950249\n",
      "epoch 10, batch 418, train_loss 3.32675933838\n",
      "epoch 10, batch 419, train_loss 2.96218037605\n",
      "epoch 10, batch 420, train_loss 2.41154098511\n",
      "epoch 10, batch 421, train_loss 4.18683385849\n",
      "epoch 10, batch 422, train_loss 2.93869018555\n",
      "epoch 10, batch 423, train_loss 3.05768656731\n",
      "epoch 10, batch 424, train_loss 4.12734937668\n",
      "epoch 10, batch 425, train_loss 3.44999098778\n",
      "epoch 10, batch 426, train_loss 4.58877468109\n",
      "epoch 10, batch 427, train_loss 3.73614740372\n",
      "epoch 10, batch 428, train_loss 2.55016875267\n",
      "epoch 10, batch 429, train_loss 3.54140853882\n",
      "epoch 10, batch 430, train_loss 3.58933186531\n",
      "epoch 10, batch 431, train_loss 3.20102715492\n",
      "epoch 10, batch 432, train_loss 3.40258455276\n",
      "epoch 10, batch 433, train_loss 2.82217741013\n",
      "epoch 10, batch 434, train_loss 3.32623100281\n",
      "epoch 10, batch 435, train_loss 3.13777470589\n",
      "epoch 10, batch 436, train_loss 3.47757029533\n",
      "epoch 10, batch 437, train_loss 3.91846585274\n",
      "epoch 10, batch 438, train_loss 3.58934855461\n",
      "epoch 10, batch 439, train_loss 3.34490394592\n",
      "epoch 10, batch 440, train_loss 4.61166906357\n",
      "epoch 10, batch 441, train_loss 4.72757482529\n",
      "epoch 10, batch 442, train_loss 3.92119216919\n",
      "epoch 10, batch 443, train_loss 2.40500020981\n",
      "epoch 10, batch 444, train_loss 2.92321538925\n",
      "epoch 10, batch 445, train_loss 4.49588441849\n",
      "epoch 10, batch 446, train_loss 3.27603554726\n",
      "epoch 10, batch 447, train_loss 2.97374153137\n",
      "epoch 10, batch 448, train_loss 4.37401723862\n",
      "epoch 10, batch 449, train_loss 3.77290058136\n",
      "epoch 10, batch 450, train_loss 3.09687495232\n",
      "epoch 10, batch 451, train_loss 2.84116864204\n",
      "epoch 10, batch 452, train_loss 2.60558915138\n",
      "epoch 10, batch 453, train_loss 5.02143478394\n",
      "epoch 10, batch 454, train_loss 4.86494159698\n",
      "epoch 10, batch 455, train_loss 2.49545431137\n",
      "epoch 10, batch 456, train_loss 3.16339063644\n",
      "epoch 10, batch 457, train_loss 3.37901759148\n",
      "epoch 10, batch 458, train_loss 3.03942656517\n",
      "epoch 10, batch 459, train_loss 3.30442810059\n",
      "epoch 10, batch 460, train_loss 3.78675627708\n",
      "epoch 10, batch 461, train_loss 3.29850888252\n",
      "epoch 10, batch 462, train_loss 2.97773504257\n",
      "epoch 10, batch 463, train_loss 3.12427353859\n",
      "epoch 10, batch 464, train_loss 3.40477156639\n",
      "epoch 10, batch 465, train_loss 2.81461524963\n",
      "epoch 10, batch 466, train_loss 3.56685471535\n",
      "epoch 10, batch 467, train_loss 3.39237356186\n",
      "epoch 10, batch 468, train_loss 3.53215074539\n",
      "epoch 10, batch 469, train_loss 3.10180711746\n",
      "epoch 10, batch 470, train_loss 2.97019028664\n",
      "epoch 10, batch 471, train_loss 3.05205798149\n",
      "epoch 10, batch 472, train_loss 2.91773462296\n",
      "epoch 10, batch 473, train_loss 3.60471916199\n",
      "epoch 10, batch 474, train_loss 4.17708015442\n",
      "epoch 10, batch 475, train_loss 2.94202566147\n",
      "epoch 10, batch 476, train_loss 2.45871138573\n",
      "epoch 10, batch 477, train_loss 2.54476642609\n",
      "epoch 10, batch 478, train_loss 2.38576769829\n",
      "epoch 10, batch 479, train_loss 2.89216828346\n",
      "epoch 10, batch 480, train_loss 3.07616496086\n",
      "epoch 10, batch 481, train_loss 4.61700582504\n",
      "epoch 10, batch 482, train_loss 2.79754948616\n",
      "epoch 10, batch 483, train_loss 3.40651845932\n",
      "epoch 10, batch 484, train_loss 3.08644628525\n",
      "epoch 10, batch 485, train_loss 2.32559347153\n",
      "epoch 10, batch 486, train_loss 2.33081102371\n",
      "epoch 10, batch 487, train_loss 2.49385166168\n",
      "epoch 10, batch 488, train_loss 2.89608597755\n",
      "epoch 10, batch 489, train_loss 3.02235889435\n",
      "epoch 10, batch 490, train_loss 2.93607068062\n",
      "epoch 10, batch 491, train_loss 2.86200332642\n",
      "epoch 10, batch 492, train_loss 3.20349359512\n",
      "epoch 10, batch 493, train_loss 2.80309438705\n",
      "epoch 10, batch 494, train_loss 2.89335799217\n",
      "epoch 10, batch 495, train_loss 2.96870422363\n",
      "epoch 10, batch 496, train_loss 2.60016489029\n",
      "epoch 10, batch 497, train_loss 2.77542901039\n",
      "epoch 10, batch 498, train_loss 2.90659141541\n",
      "epoch 10, batch 499, train_loss 2.86485219002\n",
      "epoch 10, batch 500, train_loss 3.30035471916\n",
      "epoch 10, batch 501, train_loss 2.96538853645\n",
      "epoch 10, batch 502, train_loss 4.21804761887\n",
      "epoch 10, batch 503, train_loss 4.1440577507\n",
      "epoch 10, batch 504, train_loss 3.61884355545\n",
      "epoch 10, batch 505, train_loss 3.43819594383\n",
      "epoch 10, batch 506, train_loss 3.11497235298\n",
      "epoch 10, batch 507, train_loss 4.1037735939\n",
      "epoch 10, batch 508, train_loss 2.94998049736\n",
      "epoch 10, batch 509, train_loss 4.24862527847\n",
      "epoch 10, batch 510, train_loss 4.21284341812\n",
      "epoch 10, batch 511, train_loss 4.23010015488\n",
      "epoch 10, batch 512, train_loss 3.3026714325\n",
      "epoch 10, batch 513, train_loss 4.66131210327\n",
      "epoch 10, batch 514, train_loss 4.59797954559\n",
      "epoch 10, batch 515, train_loss 4.49605321884\n",
      "epoch 10, batch 516, train_loss 2.79466223717\n",
      "epoch 10, batch 517, train_loss 2.99039459229\n",
      "epoch 10, batch 518, train_loss 2.89741277695\n",
      "epoch 10, batch 519, train_loss 2.58540725708\n",
      "epoch 10, batch 520, train_loss 3.71197557449\n",
      "epoch 10, batch 521, train_loss 2.90604019165\n",
      "epoch 10, batch 522, train_loss 2.53745651245\n",
      "epoch 10, batch 523, train_loss 2.52918434143\n",
      "epoch 10, batch 524, train_loss 2.37868499756\n",
      "epoch 10, batch 525, train_loss 2.21165108681\n",
      "epoch 10, batch 526, train_loss 2.30493378639\n",
      "epoch 10, batch 527, train_loss 2.32861828804\n",
      "epoch 10, batch 528, train_loss 2.5683863163\n",
      "epoch 10, batch 529, train_loss 2.46135663986\n",
      "epoch 10, batch 530, train_loss 1.96096730232\n",
      "epoch 10, batch 531, train_loss 1.73346030712\n",
      "epoch 10, batch 532, train_loss 2.53217983246\n",
      "epoch 10, batch 533, train_loss 1.62139606476\n",
      "epoch 10, batch 534, train_loss 1.69285750389\n",
      "epoch 10, batch 535, train_loss 3.08311700821\n",
      "epoch 10, batch 536, train_loss 3.33058953285\n",
      "epoch 10, batch 537, train_loss 3.19857668877\n",
      "epoch 10, batch 538, train_loss 3.31874155998\n",
      "epoch 10, batch 539, train_loss 3.57640838623\n",
      "epoch 10, batch 540, train_loss 3.90303683281\n",
      "epoch 11, batch 0, train_loss 3.31587266922\n",
      "epoch 11, batch 1, train_loss 3.64745974541\n",
      "epoch 11, batch 2, train_loss 2.97751808167\n",
      "epoch 11, batch 3, train_loss 3.26191997528\n",
      "epoch 11, batch 4, train_loss 2.96427488327\n",
      "epoch 11, batch 5, train_loss 3.03276968002\n",
      "epoch 11, batch 6, train_loss 3.94587779045\n",
      "epoch 11, batch 7, train_loss 3.00537204742\n",
      "epoch 11, batch 8, train_loss 2.67006874084\n",
      "epoch 11, batch 9, train_loss 3.6146914959\n",
      "epoch 11, batch 10, train_loss 2.91505360603\n",
      "epoch 11, batch 11, train_loss 2.98376584053\n",
      "epoch 11, batch 12, train_loss 2.72195386887\n",
      "epoch 11, batch 13, train_loss 3.20012521744\n",
      "epoch 11, batch 14, train_loss 2.63564944267\n",
      "epoch 11, batch 15, train_loss 3.3515689373\n",
      "epoch 11, batch 16, train_loss 2.89740419388\n",
      "epoch 11, batch 17, train_loss 2.75897121429\n",
      "epoch 11, batch 18, train_loss 2.43664932251\n",
      "epoch 11, batch 19, train_loss 2.01230192184\n",
      "epoch 11, batch 20, train_loss 2.83099412918\n",
      "epoch 11, batch 21, train_loss 2.77357983589\n",
      "epoch 11, batch 22, train_loss 3.24881720543\n",
      "epoch 11, batch 23, train_loss 2.84266829491\n",
      "epoch 11, batch 24, train_loss 2.84579968452\n",
      "epoch 11, batch 25, train_loss 3.26683330536\n",
      "epoch 11, batch 26, train_loss 3.20215249062\n",
      "epoch 11, batch 27, train_loss 3.11599159241\n",
      "epoch 11, batch 28, train_loss 3.40902328491\n",
      "epoch 11, batch 29, train_loss 3.30430459976\n",
      "epoch 11, batch 30, train_loss 3.40293550491\n",
      "epoch 11, batch 31, train_loss 2.84759759903\n",
      "epoch 11, batch 32, train_loss 2.6452293396\n",
      "epoch 11, batch 33, train_loss 4.42023515701\n",
      "epoch 11, batch 34, train_loss 4.50933885574\n",
      "epoch 11, batch 35, train_loss 3.19500184059\n",
      "epoch 11, batch 36, train_loss 3.2178170681\n",
      "epoch 11, batch 37, train_loss 3.15844559669\n",
      "epoch 11, batch 38, train_loss 3.31003284454\n",
      "epoch 11, batch 39, train_loss 3.77643609047\n",
      "epoch 11, batch 40, train_loss 2.87102723122\n",
      "epoch 11, batch 41, train_loss 3.16444420815\n",
      "epoch 11, batch 42, train_loss 3.16729807854\n",
      "epoch 11, batch 43, train_loss 2.93216013908\n",
      "epoch 11, batch 44, train_loss 3.58309316635\n",
      "epoch 11, batch 45, train_loss 3.20371961594\n",
      "epoch 11, batch 46, train_loss 3.16423130035\n",
      "epoch 11, batch 47, train_loss 3.24019289017\n",
      "epoch 11, batch 48, train_loss 3.06884121895\n",
      "epoch 11, batch 49, train_loss 3.19924616814\n",
      "epoch 11, batch 50, train_loss 3.12075853348\n",
      "epoch 11, batch 51, train_loss 2.77661156654\n",
      "epoch 11, batch 52, train_loss 3.22190690041\n",
      "epoch 11, batch 53, train_loss 3.07310986519\n",
      "epoch 11, batch 54, train_loss 2.84939146042\n",
      "epoch 11, batch 55, train_loss 2.8926680088\n",
      "epoch 11, batch 56, train_loss 2.77765679359\n",
      "epoch 11, batch 57, train_loss 3.23930692673\n",
      "epoch 11, batch 58, train_loss 3.05940318108\n",
      "epoch 11, batch 59, train_loss 3.01027083397\n",
      "epoch 11, batch 60, train_loss 3.20319151878\n",
      "epoch 11, batch 61, train_loss 1.84755396843\n",
      "epoch 11, batch 62, train_loss 2.78932237625\n",
      "epoch 11, batch 63, train_loss 2.94010806084\n",
      "epoch 11, batch 64, train_loss 3.18822431564\n",
      "epoch 11, batch 65, train_loss 3.49833631516\n",
      "epoch 11, batch 66, train_loss 3.25788497925\n",
      "epoch 11, batch 67, train_loss 2.10449457169\n",
      "epoch 11, batch 68, train_loss 2.95219302177\n",
      "epoch 11, batch 69, train_loss 2.31094551086\n",
      "epoch 11, batch 70, train_loss 3.79772543907\n",
      "epoch 11, batch 71, train_loss 3.8720510006\n",
      "epoch 11, batch 72, train_loss 2.75046634674\n",
      "epoch 11, batch 73, train_loss 3.15968561172\n",
      "epoch 11, batch 74, train_loss 3.70134305954\n",
      "epoch 11, batch 75, train_loss 3.19475197792\n",
      "epoch 11, batch 76, train_loss 3.47475290298\n",
      "epoch 11, batch 77, train_loss 4.13161182404\n",
      "epoch 11, batch 78, train_loss 3.03564763069\n",
      "epoch 11, batch 79, train_loss 3.0560631752\n",
      "epoch 11, batch 80, train_loss 2.88319325447\n",
      "epoch 11, batch 81, train_loss 3.03483843803\n",
      "epoch 11, batch 82, train_loss 2.93235135078\n",
      "epoch 11, batch 83, train_loss 3.06786394119\n",
      "epoch 11, batch 84, train_loss 2.81276917458\n",
      "epoch 11, batch 85, train_loss 2.8306388855\n",
      "epoch 11, batch 86, train_loss 3.35267043114\n",
      "epoch 11, batch 87, train_loss 2.55941724777\n",
      "epoch 11, batch 88, train_loss 2.87103962898\n",
      "epoch 11, batch 89, train_loss 3.16557884216\n",
      "epoch 11, batch 90, train_loss 2.87042927742\n",
      "epoch 11, batch 91, train_loss 2.90092349052\n",
      "epoch 11, batch 92, train_loss 3.34388852119\n",
      "epoch 11, batch 93, train_loss 4.0689330101\n",
      "epoch 11, batch 94, train_loss 2.91147446632\n",
      "epoch 11, batch 95, train_loss 2.9801094532\n",
      "epoch 11, batch 96, train_loss 2.37319731712\n",
      "epoch 11, batch 97, train_loss 2.95932602882\n",
      "epoch 11, batch 98, train_loss 2.90705060959\n",
      "epoch 11, batch 99, train_loss 3.12590146065\n",
      "epoch 11, batch 100, train_loss 2.91740751266\n",
      "epoch 11, batch 101, train_loss 3.06918263435\n",
      "epoch 11, batch 102, train_loss 2.89192152023\n",
      "epoch 11, batch 103, train_loss 4.12035417557\n",
      "epoch 11, batch 104, train_loss 3.40463280678\n",
      "epoch 11, batch 105, train_loss 3.22452783585\n",
      "epoch 11, batch 106, train_loss 3.60697722435\n",
      "epoch 11, batch 107, train_loss 3.42997550964\n",
      "epoch 11, batch 108, train_loss 3.05849504471\n",
      "epoch 11, batch 109, train_loss 3.33026528358\n",
      "epoch 11, batch 110, train_loss 3.14140844345\n",
      "epoch 11, batch 111, train_loss 3.54606580734\n",
      "epoch 11, batch 112, train_loss 3.49208855629\n",
      "epoch 11, batch 113, train_loss 2.90070939064\n",
      "epoch 11, batch 114, train_loss 3.47599315643\n",
      "epoch 11, batch 115, train_loss 4.15765285492\n",
      "epoch 11, batch 116, train_loss 4.04419898987\n",
      "epoch 11, batch 117, train_loss 4.2113494873\n",
      "epoch 11, batch 118, train_loss 3.65615534782\n",
      "epoch 11, batch 119, train_loss 2.41520261765\n",
      "epoch 11, batch 120, train_loss 3.22203063965\n",
      "epoch 11, batch 121, train_loss 3.03684854507\n",
      "epoch 11, batch 122, train_loss 3.24667024612\n",
      "epoch 11, batch 123, train_loss 3.0266418457\n",
      "epoch 11, batch 124, train_loss 2.73403811455\n",
      "epoch 11, batch 125, train_loss 2.70421504974\n",
      "epoch 11, batch 126, train_loss 2.68156838417\n",
      "epoch 11, batch 127, train_loss 2.74613046646\n",
      "epoch 11, batch 128, train_loss 2.57000660896\n",
      "epoch 11, batch 129, train_loss 2.98234558105\n",
      "epoch 11, batch 130, train_loss 2.92288160324\n",
      "epoch 11, batch 131, train_loss 3.2864882946\n",
      "epoch 11, batch 132, train_loss 3.03262996674\n",
      "epoch 11, batch 133, train_loss 2.29133486748\n",
      "epoch 11, batch 134, train_loss 2.31360602379\n",
      "epoch 11, batch 135, train_loss 4.02495527267\n",
      "epoch 11, batch 136, train_loss 3.47362709045\n",
      "epoch 11, batch 137, train_loss 2.65943241119\n",
      "epoch 11, batch 138, train_loss 2.52449321747\n",
      "epoch 11, batch 139, train_loss 3.03051519394\n",
      "epoch 11, batch 140, train_loss 2.52907085419\n",
      "epoch 11, batch 141, train_loss 2.82981801033\n",
      "epoch 11, batch 142, train_loss 4.00617027283\n",
      "epoch 11, batch 143, train_loss 2.52980589867\n",
      "epoch 11, batch 144, train_loss 2.06207370758\n",
      "epoch 11, batch 145, train_loss 3.02099895477\n",
      "epoch 11, batch 146, train_loss 3.20894694328\n",
      "epoch 11, batch 147, train_loss 2.9413356781\n",
      "epoch 11, batch 148, train_loss 2.84150981903\n",
      "epoch 11, batch 149, train_loss 3.25791883469\n",
      "epoch 11, batch 150, train_loss 3.38467764854\n",
      "epoch 11, batch 151, train_loss 2.07826137543\n",
      "epoch 11, batch 152, train_loss 2.30749440193\n",
      "epoch 11, batch 153, train_loss 3.92434263229\n",
      "epoch 11, batch 154, train_loss 3.11917114258\n",
      "epoch 11, batch 155, train_loss 2.71622371674\n",
      "epoch 11, batch 156, train_loss 2.91296172142\n",
      "epoch 11, batch 157, train_loss 2.51581311226\n",
      "epoch 11, batch 158, train_loss 2.52587604523\n",
      "epoch 11, batch 159, train_loss 2.73042201996\n",
      "epoch 11, batch 160, train_loss 2.82441306114\n",
      "epoch 11, batch 161, train_loss 3.35039901733\n",
      "epoch 11, batch 162, train_loss 3.33450698853\n",
      "epoch 11, batch 163, train_loss 4.75438261032\n",
      "epoch 11, batch 164, train_loss 2.29532384872\n",
      "epoch 11, batch 165, train_loss 2.25510525703\n",
      "epoch 11, batch 166, train_loss 2.54833316803\n",
      "epoch 11, batch 167, train_loss 2.57753801346\n",
      "epoch 11, batch 168, train_loss 2.78927993774\n",
      "epoch 11, batch 169, train_loss 2.60147833824\n",
      "epoch 11, batch 170, train_loss 3.1017780304\n",
      "epoch 11, batch 171, train_loss 2.52493572235\n",
      "epoch 11, batch 172, train_loss 3.11761689186\n",
      "epoch 11, batch 173, train_loss 2.09395313263\n",
      "epoch 11, batch 174, train_loss 3.59003424644\n",
      "epoch 11, batch 175, train_loss 3.08858275414\n",
      "epoch 11, batch 176, train_loss 2.97931027412\n",
      "epoch 11, batch 177, train_loss 2.79075622559\n",
      "epoch 11, batch 178, train_loss 2.72359418869\n",
      "epoch 11, batch 179, train_loss 2.98505568504\n",
      "epoch 11, batch 180, train_loss 2.85035037994\n",
      "epoch 11, batch 181, train_loss 3.45766687393\n",
      "epoch 11, batch 182, train_loss 2.97092199326\n",
      "epoch 11, batch 183, train_loss 3.61172842979\n",
      "epoch 11, batch 184, train_loss 2.35717582703\n",
      "epoch 11, batch 185, train_loss 2.71092748642\n",
      "epoch 11, batch 186, train_loss 3.29877996445\n",
      "epoch 11, batch 187, train_loss 2.40191268921\n",
      "epoch 11, batch 188, train_loss 2.74646258354\n",
      "epoch 11, batch 189, train_loss 2.72529602051\n",
      "epoch 11, batch 190, train_loss 3.47794318199\n",
      "epoch 11, batch 191, train_loss 3.32815694809\n",
      "epoch 11, batch 192, train_loss 4.25400686264\n",
      "epoch 11, batch 193, train_loss 3.98214626312\n",
      "epoch 11, batch 194, train_loss 4.5795249939\n",
      "epoch 11, batch 195, train_loss 4.65801858902\n",
      "epoch 11, batch 196, train_loss 3.1240105629\n",
      "epoch 11, batch 197, train_loss 2.49210524559\n",
      "epoch 11, batch 198, train_loss 2.29211997986\n",
      "epoch 11, batch 199, train_loss 3.27109217644\n",
      "epoch 11, batch 200, train_loss 2.76137399673\n",
      "epoch 11, batch 201, train_loss 3.05278968811\n",
      "epoch 11, batch 202, train_loss 3.48983597755\n",
      "epoch 11, batch 203, train_loss 3.6281235218\n",
      "epoch 11, batch 204, train_loss 3.62282180786\n",
      "epoch 11, batch 205, train_loss 3.06252336502\n",
      "epoch 11, batch 206, train_loss 4.06878805161\n",
      "epoch 11, batch 207, train_loss 3.43523311615\n",
      "epoch 11, batch 208, train_loss 4.13315916061\n",
      "epoch 11, batch 209, train_loss 3.86634111404\n",
      "epoch 11, batch 210, train_loss 2.11786437035\n",
      "epoch 11, batch 211, train_loss 2.86035585403\n",
      "epoch 11, batch 212, train_loss 3.77263212204\n",
      "epoch 11, batch 213, train_loss 3.10593390465\n",
      "epoch 11, batch 214, train_loss 2.97386240959\n",
      "epoch 11, batch 215, train_loss 2.20262861252\n",
      "epoch 11, batch 216, train_loss 3.33401322365\n",
      "epoch 11, batch 217, train_loss 2.78806829453\n",
      "epoch 11, batch 218, train_loss 3.24048852921\n",
      "epoch 11, batch 219, train_loss 3.33920073509\n",
      "epoch 11, batch 220, train_loss 3.38409638405\n",
      "epoch 11, batch 221, train_loss 3.51837325096\n",
      "epoch 11, batch 222, train_loss 2.28027176857\n",
      "epoch 11, batch 223, train_loss 2.27983021736\n",
      "epoch 11, batch 224, train_loss 2.63467812538\n",
      "epoch 11, batch 225, train_loss 2.75783419609\n",
      "epoch 11, batch 226, train_loss 3.2648460865\n",
      "epoch 11, batch 227, train_loss 3.0294008255\n",
      "epoch 11, batch 228, train_loss 3.18161725998\n",
      "epoch 11, batch 229, train_loss 3.00201702118\n",
      "epoch 11, batch 230, train_loss 2.88029932976\n",
      "epoch 11, batch 231, train_loss 2.58111906052\n",
      "epoch 11, batch 232, train_loss 3.2242846489\n",
      "epoch 11, batch 233, train_loss 3.47273302078\n",
      "epoch 11, batch 234, train_loss 3.05341935158\n",
      "epoch 11, batch 235, train_loss 3.60045385361\n",
      "epoch 11, batch 236, train_loss 3.14550256729\n",
      "epoch 11, batch 237, train_loss 3.04748010635\n",
      "epoch 11, batch 238, train_loss 3.07795834541\n",
      "epoch 11, batch 239, train_loss 2.89686393738\n",
      "epoch 11, batch 240, train_loss 3.33489012718\n",
      "epoch 11, batch 241, train_loss 3.2611618042\n",
      "epoch 11, batch 242, train_loss 3.07987117767\n",
      "epoch 11, batch 243, train_loss 3.27750492096\n",
      "epoch 11, batch 244, train_loss 3.4101331234\n",
      "epoch 11, batch 245, train_loss 3.17814707756\n",
      "epoch 11, batch 246, train_loss 3.09395456314\n",
      "epoch 11, batch 247, train_loss 3.27937936783\n",
      "epoch 11, batch 248, train_loss 3.02309203148\n",
      "epoch 11, batch 249, train_loss 3.70783925056\n",
      "epoch 11, batch 250, train_loss 3.46096515656\n",
      "epoch 11, batch 251, train_loss 3.8163690567\n",
      "epoch 11, batch 252, train_loss 2.63582396507\n",
      "epoch 11, batch 253, train_loss 2.17786765099\n",
      "epoch 11, batch 254, train_loss 2.78853297234\n",
      "epoch 11, batch 255, train_loss 2.16638731956\n",
      "epoch 11, batch 256, train_loss 2.54504561424\n",
      "epoch 11, batch 257, train_loss 2.99292683601\n",
      "epoch 11, batch 258, train_loss 2.38217568398\n",
      "epoch 11, batch 259, train_loss 2.32247543335\n",
      "epoch 11, batch 260, train_loss 2.08599495888\n",
      "epoch 11, batch 261, train_loss 3.40697789192\n",
      "epoch 11, batch 262, train_loss 4.00480461121\n",
      "epoch 11, batch 263, train_loss 3.1166613102\n",
      "epoch 11, batch 264, train_loss 2.82904791832\n",
      "epoch 11, batch 265, train_loss 3.44741725922\n",
      "epoch 11, batch 266, train_loss 3.4885571003\n",
      "epoch 11, batch 267, train_loss 2.75818562508\n",
      "epoch 11, batch 268, train_loss 2.29807949066\n",
      "epoch 11, batch 269, train_loss 2.37863063812\n",
      "epoch 11, batch 270, train_loss 2.67599272728\n",
      "epoch 11, batch 271, train_loss 3.01059651375\n",
      "epoch 11, batch 272, train_loss 3.0321931839\n",
      "epoch 11, batch 273, train_loss 3.14490294456\n",
      "epoch 11, batch 274, train_loss 2.96203541756\n",
      "epoch 11, batch 275, train_loss 2.97479963303\n",
      "epoch 11, batch 276, train_loss 2.76601600647\n",
      "epoch 11, batch 277, train_loss 3.57934069633\n",
      "epoch 11, batch 278, train_loss 2.71559000015\n",
      "epoch 11, batch 279, train_loss 3.24395751953\n",
      "epoch 11, batch 280, train_loss 3.22405052185\n",
      "epoch 11, batch 281, train_loss 3.04786086082\n",
      "epoch 11, batch 282, train_loss 2.67731404305\n",
      "epoch 11, batch 283, train_loss 4.21751594543\n",
      "epoch 11, batch 284, train_loss 3.17411494255\n",
      "epoch 11, batch 285, train_loss 1.95893442631\n",
      "epoch 11, batch 286, train_loss 2.20101547241\n",
      "epoch 11, batch 287, train_loss 2.3013074398\n",
      "epoch 11, batch 288, train_loss 2.94314670563\n",
      "epoch 11, batch 289, train_loss 2.99093914032\n",
      "epoch 11, batch 290, train_loss 2.82216572762\n",
      "epoch 11, batch 291, train_loss 2.70120167732\n",
      "epoch 11, batch 292, train_loss 2.22495532036\n",
      "epoch 11, batch 293, train_loss 3.27625751495\n",
      "epoch 11, batch 294, train_loss 2.87079358101\n",
      "epoch 11, batch 295, train_loss 2.59378886223\n",
      "epoch 11, batch 296, train_loss 2.87758851051\n",
      "epoch 11, batch 297, train_loss 2.55013895035\n",
      "epoch 11, batch 298, train_loss 2.77761936188\n",
      "epoch 11, batch 299, train_loss 3.2954158783\n",
      "epoch 11, batch 300, train_loss 3.04334878922\n",
      "epoch 11, batch 301, train_loss 3.98516559601\n",
      "epoch 11, batch 302, train_loss 3.99723529816\n",
      "epoch 11, batch 303, train_loss 3.68545031548\n",
      "epoch 11, batch 304, train_loss 4.32454919815\n",
      "epoch 11, batch 305, train_loss 4.29307413101\n",
      "epoch 11, batch 306, train_loss 2.96321630478\n",
      "epoch 11, batch 307, train_loss 2.47859168053\n",
      "epoch 11, batch 308, train_loss 2.91847491264\n",
      "epoch 11, batch 309, train_loss 3.07449388504\n",
      "epoch 11, batch 310, train_loss 2.89658427238\n",
      "epoch 11, batch 311, train_loss 2.99789977074\n",
      "epoch 11, batch 312, train_loss 3.17409992218\n",
      "epoch 11, batch 313, train_loss 2.89134478569\n",
      "epoch 11, batch 314, train_loss 3.21907114983\n",
      "epoch 11, batch 315, train_loss 2.7060983181\n",
      "epoch 11, batch 316, train_loss 2.86518645287\n",
      "epoch 11, batch 317, train_loss 3.33740758896\n",
      "epoch 11, batch 318, train_loss 2.47377085686\n",
      "epoch 11, batch 319, train_loss 4.1585855484\n",
      "epoch 11, batch 320, train_loss 3.04883527756\n",
      "epoch 11, batch 321, train_loss 4.24949741364\n",
      "epoch 11, batch 322, train_loss 2.3691072464\n",
      "epoch 11, batch 323, train_loss 2.24468827248\n",
      "epoch 11, batch 324, train_loss 3.34388923645\n",
      "epoch 11, batch 325, train_loss 3.0305416584\n",
      "epoch 11, batch 326, train_loss 3.01793003082\n",
      "epoch 11, batch 327, train_loss 2.99457144737\n",
      "epoch 11, batch 328, train_loss 2.87076258659\n",
      "epoch 11, batch 329, train_loss 2.55701351166\n",
      "epoch 11, batch 330, train_loss 3.87287735939\n",
      "epoch 11, batch 331, train_loss 3.56562280655\n",
      "epoch 11, batch 332, train_loss 2.40840673447\n",
      "epoch 11, batch 333, train_loss 3.43797755241\n",
      "epoch 11, batch 334, train_loss 3.14641427994\n",
      "epoch 11, batch 335, train_loss 3.05714130402\n",
      "epoch 11, batch 336, train_loss 3.60476279259\n",
      "epoch 11, batch 337, train_loss 2.21370506287\n",
      "epoch 11, batch 338, train_loss 3.02500343323\n",
      "epoch 11, batch 339, train_loss 2.81667423248\n",
      "epoch 11, batch 340, train_loss 3.0165412426\n",
      "epoch 11, batch 341, train_loss 2.9588997364\n",
      "epoch 11, batch 342, train_loss 2.9944331646\n",
      "epoch 11, batch 343, train_loss 2.89426064491\n",
      "epoch 11, batch 344, train_loss 3.80282640457\n",
      "epoch 11, batch 345, train_loss 3.23321318626\n",
      "epoch 11, batch 346, train_loss 3.00787949562\n",
      "epoch 11, batch 347, train_loss 2.81149530411\n",
      "epoch 11, batch 348, train_loss 3.06875491142\n",
      "epoch 11, batch 349, train_loss 4.05285978317\n",
      "epoch 11, batch 350, train_loss 4.02729558945\n",
      "epoch 11, batch 351, train_loss 3.83533716202\n",
      "epoch 11, batch 352, train_loss 3.88093805313\n",
      "epoch 11, batch 353, train_loss 3.16002178192\n",
      "epoch 11, batch 354, train_loss 2.5519888401\n",
      "epoch 11, batch 355, train_loss 2.86553478241\n",
      "epoch 11, batch 356, train_loss 2.82365465164\n",
      "epoch 11, batch 357, train_loss 2.76580905914\n",
      "epoch 11, batch 358, train_loss 2.34369802475\n",
      "epoch 11, batch 359, train_loss 3.29846811295\n",
      "epoch 11, batch 360, train_loss 2.80345082283\n",
      "epoch 11, batch 361, train_loss 2.48899030685\n",
      "epoch 11, batch 362, train_loss 4.1772851944\n",
      "epoch 11, batch 363, train_loss 3.08357954025\n",
      "epoch 11, batch 364, train_loss 2.98203969002\n",
      "epoch 11, batch 365, train_loss 3.84463906288\n",
      "epoch 11, batch 366, train_loss 3.97701454163\n",
      "epoch 11, batch 367, train_loss 4.71163415909\n",
      "epoch 11, batch 368, train_loss 2.45880198479\n",
      "epoch 11, batch 369, train_loss 3.3666908741\n",
      "epoch 11, batch 370, train_loss 3.75970125198\n",
      "epoch 11, batch 371, train_loss 4.68002986908\n",
      "epoch 11, batch 372, train_loss 2.25345087051\n",
      "epoch 11, batch 373, train_loss 2.41604733467\n",
      "epoch 11, batch 374, train_loss 2.75378608704\n",
      "epoch 11, batch 375, train_loss 3.13334226608\n",
      "epoch 11, batch 376, train_loss 2.40716457367\n",
      "epoch 11, batch 377, train_loss 2.80018591881\n",
      "epoch 11, batch 378, train_loss 3.48799705505\n",
      "epoch 11, batch 379, train_loss 2.67336511612\n",
      "epoch 11, batch 380, train_loss 2.7011153698\n",
      "epoch 11, batch 381, train_loss 2.79730510712\n",
      "epoch 11, batch 382, train_loss 3.63502812386\n",
      "epoch 11, batch 383, train_loss 3.23501634598\n",
      "epoch 11, batch 384, train_loss 2.90814876556\n",
      "epoch 11, batch 385, train_loss 4.26823949814\n",
      "epoch 11, batch 386, train_loss 2.6023914814\n",
      "epoch 11, batch 387, train_loss 4.36188697815\n",
      "epoch 11, batch 388, train_loss 2.60862231255\n",
      "epoch 11, batch 389, train_loss 4.13817834854\n",
      "epoch 11, batch 390, train_loss 4.13843345642\n",
      "epoch 11, batch 391, train_loss 4.17525815964\n",
      "epoch 11, batch 392, train_loss 3.94436812401\n",
      "epoch 11, batch 393, train_loss 3.18988656998\n",
      "epoch 11, batch 394, train_loss 3.80341458321\n",
      "epoch 11, batch 395, train_loss 2.78758716583\n",
      "epoch 11, batch 396, train_loss 4.03648805618\n",
      "epoch 11, batch 397, train_loss 4.13262557983\n",
      "epoch 11, batch 398, train_loss 3.12297725677\n",
      "epoch 11, batch 399, train_loss 3.33505797386\n",
      "epoch 11, batch 400, train_loss 4.26850891113\n",
      "epoch 11, batch 401, train_loss 3.53867959976\n",
      "epoch 11, batch 402, train_loss 3.13766813278\n",
      "epoch 11, batch 403, train_loss 3.07245063782\n",
      "epoch 11, batch 404, train_loss 3.89297485352\n",
      "epoch 11, batch 405, train_loss 3.41121006012\n",
      "epoch 11, batch 406, train_loss 3.2703025341\n",
      "epoch 11, batch 407, train_loss 3.074457407\n",
      "epoch 11, batch 408, train_loss 2.92414999008\n",
      "epoch 11, batch 409, train_loss 2.45553922653\n",
      "epoch 11, batch 410, train_loss 4.04852867126\n",
      "epoch 11, batch 411, train_loss 3.15539741516\n",
      "epoch 11, batch 412, train_loss 2.67367601395\n",
      "epoch 11, batch 413, train_loss 3.00541853905\n",
      "epoch 11, batch 414, train_loss 3.514742136\n",
      "epoch 11, batch 415, train_loss 3.59295248985\n",
      "epoch 11, batch 416, train_loss 3.34256505966\n",
      "epoch 11, batch 417, train_loss 2.89892649651\n",
      "epoch 11, batch 418, train_loss 3.29322052002\n",
      "epoch 11, batch 419, train_loss 2.93432641029\n",
      "epoch 11, batch 420, train_loss 2.39187335968\n",
      "epoch 11, batch 421, train_loss 4.1468963623\n",
      "epoch 11, batch 422, train_loss 2.90970873833\n",
      "epoch 11, batch 423, train_loss 3.02802252769\n",
      "epoch 11, batch 424, train_loss 4.08517026901\n",
      "epoch 11, batch 425, train_loss 3.42137718201\n",
      "epoch 11, batch 426, train_loss 4.54497480392\n",
      "epoch 11, batch 427, train_loss 3.70182323456\n",
      "epoch 11, batch 428, train_loss 2.52590203285\n",
      "epoch 11, batch 429, train_loss 3.50628423691\n",
      "epoch 11, batch 430, train_loss 3.55014324188\n",
      "epoch 11, batch 431, train_loss 3.16714787483\n",
      "epoch 11, batch 432, train_loss 3.36691331863\n",
      "epoch 11, batch 433, train_loss 2.79307770729\n",
      "epoch 11, batch 434, train_loss 3.29618883133\n",
      "epoch 11, batch 435, train_loss 3.11189460754\n",
      "epoch 11, batch 436, train_loss 3.44501900673\n",
      "epoch 11, batch 437, train_loss 3.88068819046\n",
      "epoch 11, batch 438, train_loss 3.56040334702\n",
      "epoch 11, batch 439, train_loss 3.3141515255\n",
      "epoch 11, batch 440, train_loss 4.56155872345\n",
      "epoch 11, batch 441, train_loss 4.68127155304\n",
      "epoch 11, batch 442, train_loss 3.88264513016\n",
      "epoch 11, batch 443, train_loss 2.38162136078\n",
      "epoch 11, batch 444, train_loss 2.89752388\n",
      "epoch 11, batch 445, train_loss 4.4561662674\n",
      "epoch 11, batch 446, train_loss 3.24859499931\n",
      "epoch 11, batch 447, train_loss 2.94871783257\n",
      "epoch 11, batch 448, train_loss 4.33148384094\n",
      "epoch 11, batch 449, train_loss 3.73924875259\n",
      "epoch 11, batch 450, train_loss 3.06713080406\n",
      "epoch 11, batch 451, train_loss 2.81078100204\n",
      "epoch 11, batch 452, train_loss 2.58038020134\n",
      "epoch 11, batch 453, train_loss 4.95706129074\n",
      "epoch 11, batch 454, train_loss 4.79772186279\n",
      "epoch 11, batch 455, train_loss 2.4712138176\n",
      "epoch 11, batch 456, train_loss 3.13225364685\n",
      "epoch 11, batch 457, train_loss 3.33548784256\n",
      "epoch 11, batch 458, train_loss 3.00975394249\n",
      "epoch 11, batch 459, train_loss 3.26902723312\n",
      "epoch 11, batch 460, train_loss 3.74785685539\n",
      "epoch 11, batch 461, train_loss 3.26242852211\n",
      "epoch 11, batch 462, train_loss 2.94785881042\n",
      "epoch 11, batch 463, train_loss 3.092192173\n",
      "epoch 11, batch 464, train_loss 3.36936879158\n",
      "epoch 11, batch 465, train_loss 2.78522992134\n",
      "epoch 11, batch 466, train_loss 3.52862048149\n",
      "epoch 11, batch 467, train_loss 3.356112957\n",
      "epoch 11, batch 468, train_loss 3.49403452873\n",
      "epoch 11, batch 469, train_loss 3.06800532341\n",
      "epoch 11, batch 470, train_loss 2.93674492836\n",
      "epoch 11, batch 471, train_loss 3.02095103264\n",
      "epoch 11, batch 472, train_loss 2.88706374168\n",
      "epoch 11, batch 473, train_loss 3.56962394714\n",
      "epoch 11, batch 474, train_loss 4.13462543488\n",
      "epoch 11, batch 475, train_loss 2.91553783417\n",
      "epoch 11, batch 476, train_loss 2.43551516533\n",
      "epoch 11, batch 477, train_loss 2.51536726952\n",
      "epoch 11, batch 478, train_loss 2.35687875748\n",
      "epoch 11, batch 479, train_loss 2.86512470245\n",
      "epoch 11, batch 480, train_loss 3.03910684586\n",
      "epoch 11, batch 481, train_loss 4.56680059433\n",
      "epoch 11, batch 482, train_loss 2.76792693138\n",
      "epoch 11, batch 483, train_loss 3.37063860893\n",
      "epoch 11, batch 484, train_loss 3.0549902916\n",
      "epoch 11, batch 485, train_loss 2.3012444973\n",
      "epoch 11, batch 486, train_loss 2.30488729477\n",
      "epoch 11, batch 487, train_loss 2.46709251404\n",
      "epoch 11, batch 488, train_loss 2.86973047256\n",
      "epoch 11, batch 489, train_loss 2.99575352669\n",
      "epoch 11, batch 490, train_loss 2.90731859207\n",
      "epoch 11, batch 491, train_loss 2.8366112709\n",
      "epoch 11, batch 492, train_loss 3.18030738831\n",
      "epoch 11, batch 493, train_loss 2.78353953362\n",
      "epoch 11, batch 494, train_loss 2.87244558334\n",
      "epoch 11, batch 495, train_loss 2.94821119308\n",
      "epoch 11, batch 496, train_loss 2.57906246185\n",
      "epoch 11, batch 497, train_loss 2.75364255905\n",
      "epoch 11, batch 498, train_loss 2.88182759285\n",
      "epoch 11, batch 499, train_loss 2.84072470665\n",
      "epoch 11, batch 500, train_loss 3.26254272461\n",
      "epoch 11, batch 501, train_loss 2.93907546997\n",
      "epoch 11, batch 502, train_loss 4.17572975159\n",
      "epoch 11, batch 503, train_loss 4.09855604172\n",
      "epoch 11, batch 504, train_loss 3.58299636841\n",
      "epoch 11, batch 505, train_loss 3.40754437447\n",
      "epoch 11, batch 506, train_loss 3.08693766594\n",
      "epoch 11, batch 507, train_loss 4.06731748581\n",
      "epoch 11, batch 508, train_loss 2.92925548553\n",
      "epoch 11, batch 509, train_loss 4.20545053482\n",
      "epoch 11, batch 510, train_loss 4.16804790497\n",
      "epoch 11, batch 511, train_loss 4.18990278244\n",
      "epoch 11, batch 512, train_loss 3.27684473991\n",
      "epoch 11, batch 513, train_loss 4.61908531189\n",
      "epoch 11, batch 514, train_loss 4.55410242081\n",
      "epoch 11, batch 515, train_loss 4.45501661301\n",
      "epoch 11, batch 516, train_loss 2.76554894447\n",
      "epoch 11, batch 517, train_loss 2.96474814415\n",
      "epoch 11, batch 518, train_loss 2.87127447128\n",
      "epoch 11, batch 519, train_loss 2.56279277802\n",
      "epoch 11, batch 520, train_loss 3.67187690735\n",
      "epoch 11, batch 521, train_loss 2.87717938423\n",
      "epoch 11, batch 522, train_loss 2.51569199562\n",
      "epoch 11, batch 523, train_loss 2.5078701973\n",
      "epoch 11, batch 524, train_loss 2.35574436188\n",
      "epoch 11, batch 525, train_loss 2.19073629379\n",
      "epoch 11, batch 526, train_loss 2.28406119347\n",
      "epoch 11, batch 527, train_loss 2.30563521385\n",
      "epoch 11, batch 528, train_loss 2.54240846634\n",
      "epoch 11, batch 529, train_loss 2.43403172493\n",
      "epoch 11, batch 530, train_loss 1.93794047832\n",
      "epoch 11, batch 531, train_loss 1.72051334381\n",
      "epoch 11, batch 532, train_loss 2.49504780769\n",
      "epoch 11, batch 533, train_loss 1.59929001331\n",
      "epoch 11, batch 534, train_loss 1.67827057838\n",
      "epoch 11, batch 535, train_loss 3.04434132576\n",
      "epoch 11, batch 536, train_loss 3.2815220356\n",
      "epoch 11, batch 537, train_loss 3.17425990105\n",
      "epoch 11, batch 538, train_loss 3.28312325478\n",
      "epoch 11, batch 539, train_loss 3.53316688538\n",
      "epoch 11, batch 540, train_loss 3.85518884659\n",
      "epoch 12, batch 0, train_loss 3.28627228737\n",
      "epoch 12, batch 1, train_loss 3.61095190048\n",
      "epoch 12, batch 2, train_loss 2.93554520607\n",
      "epoch 12, batch 3, train_loss 3.22545456886\n",
      "epoch 12, batch 4, train_loss 2.91510748863\n",
      "epoch 12, batch 5, train_loss 2.98560166359\n",
      "epoch 12, batch 6, train_loss 3.86369562149\n",
      "epoch 12, batch 7, train_loss 2.95026016235\n",
      "epoch 12, batch 8, train_loss 2.63221549988\n",
      "epoch 12, batch 9, train_loss 3.5697247982\n",
      "epoch 12, batch 10, train_loss 2.88188481331\n",
      "epoch 12, batch 11, train_loss 2.94506311417\n",
      "epoch 12, batch 12, train_loss 2.68695068359\n",
      "epoch 12, batch 13, train_loss 3.15802526474\n",
      "epoch 12, batch 14, train_loss 2.59798693657\n",
      "epoch 12, batch 15, train_loss 3.30386734009\n",
      "epoch 12, batch 16, train_loss 2.85313749313\n",
      "epoch 12, batch 17, train_loss 2.72057747841\n",
      "epoch 12, batch 18, train_loss 2.4074215889\n",
      "epoch 12, batch 19, train_loss 1.99178206921\n",
      "epoch 12, batch 20, train_loss 2.79367828369\n",
      "epoch 12, batch 21, train_loss 2.73761439323\n",
      "epoch 12, batch 22, train_loss 3.20647501945\n",
      "epoch 12, batch 23, train_loss 2.81429672241\n",
      "epoch 12, batch 24, train_loss 2.82328414917\n",
      "epoch 12, batch 25, train_loss 3.23401165009\n",
      "epoch 12, batch 26, train_loss 3.16780400276\n",
      "epoch 12, batch 27, train_loss 3.0847659111\n",
      "epoch 12, batch 28, train_loss 3.37706637383\n",
      "epoch 12, batch 29, train_loss 3.27081465721\n",
      "epoch 12, batch 30, train_loss 3.36865758896\n",
      "epoch 12, batch 31, train_loss 2.81630110741\n",
      "epoch 12, batch 32, train_loss 2.62331795692\n",
      "epoch 12, batch 33, train_loss 4.37311935425\n",
      "epoch 12, batch 34, train_loss 4.45839262009\n",
      "epoch 12, batch 35, train_loss 3.16272997856\n",
      "epoch 12, batch 36, train_loss 3.18248414993\n",
      "epoch 12, batch 37, train_loss 3.12498092651\n",
      "epoch 12, batch 38, train_loss 3.27368354797\n",
      "epoch 12, batch 39, train_loss 3.74381446838\n",
      "epoch 12, batch 40, train_loss 2.84109711647\n",
      "epoch 12, batch 41, train_loss 3.13879847527\n",
      "epoch 12, batch 42, train_loss 3.13564777374\n",
      "epoch 12, batch 43, train_loss 2.89920043945\n",
      "epoch 12, batch 44, train_loss 3.55326151848\n",
      "epoch 12, batch 45, train_loss 3.17361998558\n",
      "epoch 12, batch 46, train_loss 3.13889551163\n",
      "epoch 12, batch 47, train_loss 3.20729255676\n",
      "epoch 12, batch 48, train_loss 3.03882598877\n",
      "epoch 12, batch 49, train_loss 3.16510748863\n",
      "epoch 12, batch 50, train_loss 3.0901389122\n",
      "epoch 12, batch 51, train_loss 2.74504637718\n",
      "epoch 12, batch 52, train_loss 3.18898963928\n",
      "epoch 12, batch 53, train_loss 3.03703522682\n",
      "epoch 12, batch 54, train_loss 2.82057833672\n",
      "epoch 12, batch 55, train_loss 2.86809301376\n",
      "epoch 12, batch 56, train_loss 2.75229668617\n",
      "epoch 12, batch 57, train_loss 3.2049279213\n",
      "epoch 12, batch 58, train_loss 3.02700304985\n",
      "epoch 12, batch 59, train_loss 2.97571754456\n",
      "epoch 12, batch 60, train_loss 3.17014694214\n",
      "epoch 12, batch 61, train_loss 1.82871448994\n",
      "epoch 12, batch 62, train_loss 2.7587518692\n",
      "epoch 12, batch 63, train_loss 2.91260886192\n",
      "epoch 12, batch 64, train_loss 3.16375398636\n",
      "epoch 12, batch 65, train_loss 3.4717130661\n",
      "epoch 12, batch 66, train_loss 3.22954964638\n",
      "epoch 12, batch 67, train_loss 2.0848338604\n",
      "epoch 12, batch 68, train_loss 2.92184686661\n",
      "epoch 12, batch 69, train_loss 2.28555417061\n",
      "epoch 12, batch 70, train_loss 3.74855184555\n",
      "epoch 12, batch 71, train_loss 3.83021736145\n",
      "epoch 12, batch 72, train_loss 2.72359085083\n",
      "epoch 12, batch 73, train_loss 3.12694644928\n",
      "epoch 12, batch 74, train_loss 3.66389894485\n",
      "epoch 12, batch 75, train_loss 3.16102695465\n",
      "epoch 12, batch 76, train_loss 3.4405734539\n",
      "epoch 12, batch 77, train_loss 4.08585262299\n",
      "epoch 12, batch 78, train_loss 3.00818133354\n",
      "epoch 12, batch 79, train_loss 3.02617502213\n",
      "epoch 12, batch 80, train_loss 2.85304141045\n",
      "epoch 12, batch 81, train_loss 3.00862431526\n",
      "epoch 12, batch 82, train_loss 2.91029667854\n",
      "epoch 12, batch 83, train_loss 3.04111385345\n",
      "epoch 12, batch 84, train_loss 2.78833532333\n",
      "epoch 12, batch 85, train_loss 2.80394506454\n",
      "epoch 12, batch 86, train_loss 3.32739543915\n",
      "epoch 12, batch 87, train_loss 2.53467917442\n",
      "epoch 12, batch 88, train_loss 2.84797334671\n",
      "epoch 12, batch 89, train_loss 3.14077210426\n",
      "epoch 12, batch 90, train_loss 2.84669566154\n",
      "epoch 12, batch 91, train_loss 2.87485671043\n",
      "epoch 12, batch 92, train_loss 3.31229662895\n",
      "epoch 12, batch 93, train_loss 4.02640914917\n",
      "epoch 12, batch 94, train_loss 2.88223361969\n",
      "epoch 12, batch 95, train_loss 2.95126271248\n",
      "epoch 12, batch 96, train_loss 2.35092544556\n",
      "epoch 12, batch 97, train_loss 2.92729115486\n",
      "epoch 12, batch 98, train_loss 2.88306117058\n",
      "epoch 12, batch 99, train_loss 3.09343910217\n",
      "epoch 12, batch 100, train_loss 2.89121866226\n",
      "epoch 12, batch 101, train_loss 3.03977799416\n",
      "epoch 12, batch 102, train_loss 2.86252880096\n",
      "epoch 12, batch 103, train_loss 4.08198308945\n",
      "epoch 12, batch 104, train_loss 3.3744161129\n",
      "epoch 12, batch 105, train_loss 3.19575214386\n",
      "epoch 12, batch 106, train_loss 3.57419991493\n",
      "epoch 12, batch 107, train_loss 3.39856886864\n",
      "epoch 12, batch 108, train_loss 3.03016424179\n",
      "epoch 12, batch 109, train_loss 3.30463504791\n",
      "epoch 12, batch 110, train_loss 3.11647891998\n",
      "epoch 12, batch 111, train_loss 3.51823091507\n",
      "epoch 12, batch 112, train_loss 3.46300125122\n",
      "epoch 12, batch 113, train_loss 2.8738899231\n",
      "epoch 12, batch 114, train_loss 3.44345808029\n",
      "epoch 12, batch 115, train_loss 4.1114821434\n",
      "epoch 12, batch 116, train_loss 3.9978017807\n",
      "epoch 12, batch 117, train_loss 4.17250823975\n",
      "epoch 12, batch 118, train_loss 3.62506961823\n",
      "epoch 12, batch 119, train_loss 2.39269304276\n",
      "epoch 12, batch 120, train_loss 3.19483661652\n",
      "epoch 12, batch 121, train_loss 3.00734996796\n",
      "epoch 12, batch 122, train_loss 3.21258497238\n",
      "epoch 12, batch 123, train_loss 2.99919104576\n",
      "epoch 12, batch 124, train_loss 2.70854282379\n",
      "epoch 12, batch 125, train_loss 2.67802214622\n",
      "epoch 12, batch 126, train_loss 2.6593310833\n",
      "epoch 12, batch 127, train_loss 2.72157478333\n",
      "epoch 12, batch 128, train_loss 2.53696751595\n",
      "epoch 12, batch 129, train_loss 2.95421051979\n",
      "epoch 12, batch 130, train_loss 2.8934943676\n",
      "epoch 12, batch 131, train_loss 3.25641989708\n",
      "epoch 12, batch 132, train_loss 3.00683426857\n",
      "epoch 12, batch 133, train_loss 2.27253556252\n",
      "epoch 12, batch 134, train_loss 2.29270553589\n",
      "epoch 12, batch 135, train_loss 3.984375\n",
      "epoch 12, batch 136, train_loss 3.44369697571\n",
      "epoch 12, batch 137, train_loss 2.63082933426\n",
      "epoch 12, batch 138, train_loss 2.50173544884\n",
      "epoch 12, batch 139, train_loss 3.00176548958\n",
      "epoch 12, batch 140, train_loss 2.50467085838\n",
      "epoch 12, batch 141, train_loss 2.80234932899\n",
      "epoch 12, batch 142, train_loss 3.96385622025\n",
      "epoch 12, batch 143, train_loss 2.50548171997\n",
      "epoch 12, batch 144, train_loss 2.04309821129\n",
      "epoch 12, batch 145, train_loss 2.99420595169\n",
      "epoch 12, batch 146, train_loss 3.17920398712\n",
      "epoch 12, batch 147, train_loss 2.91453385353\n",
      "epoch 12, batch 148, train_loss 2.81508588791\n",
      "epoch 12, batch 149, train_loss 3.22715306282\n",
      "epoch 12, batch 150, train_loss 3.35359859467\n",
      "epoch 12, batch 151, train_loss 2.05897569656\n",
      "epoch 12, batch 152, train_loss 2.28466272354\n",
      "epoch 12, batch 153, train_loss 3.88302922249\n",
      "epoch 12, batch 154, train_loss 3.09131407738\n",
      "epoch 12, batch 155, train_loss 2.69108390808\n",
      "epoch 12, batch 156, train_loss 2.88746213913\n",
      "epoch 12, batch 157, train_loss 2.49362421036\n",
      "epoch 12, batch 158, train_loss 2.50216794014\n",
      "epoch 12, batch 159, train_loss 2.70575261116\n",
      "epoch 12, batch 160, train_loss 2.7958612442\n",
      "epoch 12, batch 161, train_loss 3.32206201553\n",
      "epoch 12, batch 162, train_loss 3.30793404579\n",
      "epoch 12, batch 163, train_loss 4.71677732468\n",
      "epoch 12, batch 164, train_loss 2.27632212639\n",
      "epoch 12, batch 165, train_loss 2.24052333832\n",
      "epoch 12, batch 166, train_loss 2.52217364311\n",
      "epoch 12, batch 167, train_loss 2.55080485344\n",
      "epoch 12, batch 168, train_loss 2.75730228424\n",
      "epoch 12, batch 169, train_loss 2.57714796066\n",
      "epoch 12, batch 170, train_loss 3.06854367256\n",
      "epoch 12, batch 171, train_loss 2.49898433685\n",
      "epoch 12, batch 172, train_loss 3.08528184891\n",
      "epoch 12, batch 173, train_loss 2.0753929615\n",
      "epoch 12, batch 174, train_loss 3.5527009964\n",
      "epoch 12, batch 175, train_loss 3.05839276314\n",
      "epoch 12, batch 176, train_loss 2.94984388351\n",
      "epoch 12, batch 177, train_loss 2.76159191132\n",
      "epoch 12, batch 178, train_loss 2.69499254227\n",
      "epoch 12, batch 179, train_loss 2.96147346497\n",
      "epoch 12, batch 180, train_loss 2.82549500465\n",
      "epoch 12, batch 181, train_loss 3.4257979393\n",
      "epoch 12, batch 182, train_loss 2.94415783882\n",
      "epoch 12, batch 183, train_loss 3.57440900803\n",
      "epoch 12, batch 184, train_loss 2.33564352989\n",
      "epoch 12, batch 185, train_loss 2.68943786621\n",
      "epoch 12, batch 186, train_loss 3.26654052734\n",
      "epoch 12, batch 187, train_loss 2.37696790695\n",
      "epoch 12, batch 188, train_loss 2.72216629982\n",
      "epoch 12, batch 189, train_loss 2.70205760002\n",
      "epoch 12, batch 190, train_loss 3.44487071037\n",
      "epoch 12, batch 191, train_loss 3.29614973068\n",
      "epoch 12, batch 192, train_loss 4.20929670334\n",
      "epoch 12, batch 193, train_loss 3.94102239609\n",
      "epoch 12, batch 194, train_loss 4.53627252579\n",
      "epoch 12, batch 195, train_loss 4.61473989487\n",
      "epoch 12, batch 196, train_loss 3.09522557259\n",
      "epoch 12, batch 197, train_loss 2.47050189972\n",
      "epoch 12, batch 198, train_loss 2.27312302589\n",
      "epoch 12, batch 199, train_loss 3.23549842834\n",
      "epoch 12, batch 200, train_loss 2.73859262466\n",
      "epoch 12, batch 201, train_loss 3.02240729332\n",
      "epoch 12, batch 202, train_loss 3.45728993416\n",
      "epoch 12, batch 203, train_loss 3.59947490692\n",
      "epoch 12, batch 204, train_loss 3.59335899353\n",
      "epoch 12, batch 205, train_loss 3.03020310402\n",
      "epoch 12, batch 206, train_loss 4.02828550339\n",
      "epoch 12, batch 207, train_loss 3.40329146385\n",
      "epoch 12, batch 208, train_loss 4.09502983093\n",
      "epoch 12, batch 209, train_loss 3.82835078239\n",
      "epoch 12, batch 210, train_loss 2.09818673134\n",
      "epoch 12, batch 211, train_loss 2.83257174492\n",
      "epoch 12, batch 212, train_loss 3.74197173119\n",
      "epoch 12, batch 213, train_loss 3.0762784481\n",
      "epoch 12, batch 214, train_loss 2.94707083702\n",
      "epoch 12, batch 215, train_loss 2.18254971504\n",
      "epoch 12, batch 216, train_loss 3.30320215225\n",
      "epoch 12, batch 217, train_loss 2.76710677147\n",
      "epoch 12, batch 218, train_loss 3.21131062508\n",
      "epoch 12, batch 219, train_loss 3.30703997612\n",
      "epoch 12, batch 220, train_loss 3.3542971611\n",
      "epoch 12, batch 221, train_loss 3.48217272758\n",
      "epoch 12, batch 222, train_loss 2.26027035713\n",
      "epoch 12, batch 223, train_loss 2.25925302505\n",
      "epoch 12, batch 224, train_loss 2.61157798767\n",
      "epoch 12, batch 225, train_loss 2.73332476616\n",
      "epoch 12, batch 226, train_loss 3.23531293869\n",
      "epoch 12, batch 227, train_loss 3.00250530243\n",
      "epoch 12, batch 228, train_loss 3.15238165855\n",
      "epoch 12, batch 229, train_loss 2.97213101387\n",
      "epoch 12, batch 230, train_loss 2.85462260246\n",
      "epoch 12, batch 231, train_loss 2.55818200111\n",
      "epoch 12, batch 232, train_loss 3.19659352303\n",
      "epoch 12, batch 233, train_loss 3.4421479702\n",
      "epoch 12, batch 234, train_loss 3.02779364586\n",
      "epoch 12, batch 235, train_loss 3.56525039673\n",
      "epoch 12, batch 236, train_loss 3.12084126472\n",
      "epoch 12, batch 237, train_loss 3.01783370972\n",
      "epoch 12, batch 238, train_loss 3.05036091805\n",
      "epoch 12, batch 239, train_loss 2.87376475334\n",
      "epoch 12, batch 240, train_loss 3.30702710152\n",
      "epoch 12, batch 241, train_loss 3.23505592346\n",
      "epoch 12, batch 242, train_loss 3.0532720089\n",
      "epoch 12, batch 243, train_loss 3.25017309189\n",
      "epoch 12, batch 244, train_loss 3.38288378716\n",
      "epoch 12, batch 245, train_loss 3.15168356895\n",
      "epoch 12, batch 246, train_loss 3.06656289101\n",
      "epoch 12, batch 247, train_loss 3.25051403046\n",
      "epoch 12, batch 248, train_loss 2.99566149712\n",
      "epoch 12, batch 249, train_loss 3.67457413673\n",
      "epoch 12, batch 250, train_loss 3.42637443542\n",
      "epoch 12, batch 251, train_loss 3.77836084366\n",
      "epoch 12, batch 252, train_loss 2.61360359192\n",
      "epoch 12, batch 253, train_loss 2.15553545952\n",
      "epoch 12, batch 254, train_loss 2.76001381874\n",
      "epoch 12, batch 255, train_loss 2.14472389221\n",
      "epoch 12, batch 256, train_loss 2.52321982384\n",
      "epoch 12, batch 257, train_loss 2.96313261986\n",
      "epoch 12, batch 258, train_loss 2.35973286629\n",
      "epoch 12, batch 259, train_loss 2.30211043358\n",
      "epoch 12, batch 260, train_loss 2.06946063042\n",
      "epoch 12, batch 261, train_loss 3.37516880035\n",
      "epoch 12, batch 262, train_loss 3.97112464905\n",
      "epoch 12, batch 263, train_loss 3.09422922134\n",
      "epoch 12, batch 264, train_loss 2.80668711662\n",
      "epoch 12, batch 265, train_loss 3.41514086723\n",
      "epoch 12, batch 266, train_loss 3.45583605766\n",
      "epoch 12, batch 267, train_loss 2.73552489281\n",
      "epoch 12, batch 268, train_loss 2.27426481247\n",
      "epoch 12, batch 269, train_loss 2.35686039925\n",
      "epoch 12, batch 270, train_loss 2.65316367149\n",
      "epoch 12, batch 271, train_loss 2.98245692253\n",
      "epoch 12, batch 272, train_loss 3.00483846664\n",
      "epoch 12, batch 273, train_loss 3.11556482315\n",
      "epoch 12, batch 274, train_loss 2.93319249153\n",
      "epoch 12, batch 275, train_loss 2.9488928318\n",
      "epoch 12, batch 276, train_loss 2.74522089958\n",
      "epoch 12, batch 277, train_loss 3.55183267593\n",
      "epoch 12, batch 278, train_loss 2.69206237793\n",
      "epoch 12, batch 279, train_loss 3.21341013908\n",
      "epoch 12, batch 280, train_loss 3.18972611427\n",
      "epoch 12, batch 281, train_loss 3.02291941643\n",
      "epoch 12, batch 282, train_loss 2.65316152573\n",
      "epoch 12, batch 283, train_loss 4.17645740509\n",
      "epoch 12, batch 284, train_loss 3.14150190353\n",
      "epoch 12, batch 285, train_loss 1.94117224216\n",
      "epoch 12, batch 286, train_loss 2.18321323395\n",
      "epoch 12, batch 287, train_loss 2.28221845627\n",
      "epoch 12, batch 288, train_loss 2.92077755928\n",
      "epoch 12, batch 289, train_loss 2.96590209007\n",
      "epoch 12, batch 290, train_loss 2.79506349564\n",
      "epoch 12, batch 291, train_loss 2.67753243446\n",
      "epoch 12, batch 292, train_loss 2.20821022987\n",
      "epoch 12, batch 293, train_loss 3.24678254128\n",
      "epoch 12, batch 294, train_loss 2.84661865234\n",
      "epoch 12, batch 295, train_loss 2.57390093803\n",
      "epoch 12, batch 296, train_loss 2.85454297066\n",
      "epoch 12, batch 297, train_loss 2.52804994583\n",
      "epoch 12, batch 298, train_loss 2.75135803223\n",
      "epoch 12, batch 299, train_loss 3.25570654869\n",
      "epoch 12, batch 300, train_loss 3.01315259933\n",
      "epoch 12, batch 301, train_loss 3.9393594265\n",
      "epoch 12, batch 302, train_loss 3.95294070244\n",
      "epoch 12, batch 303, train_loss 3.64890527725\n",
      "epoch 12, batch 304, train_loss 4.2741189003\n",
      "epoch 12, batch 305, train_loss 4.24797582626\n",
      "epoch 12, batch 306, train_loss 2.93227601051\n",
      "epoch 12, batch 307, train_loss 2.45714426041\n",
      "epoch 12, batch 308, train_loss 2.89252448082\n",
      "epoch 12, batch 309, train_loss 3.0466170311\n",
      "epoch 12, batch 310, train_loss 2.87501478195\n",
      "epoch 12, batch 311, train_loss 2.97199082375\n",
      "epoch 12, batch 312, train_loss 3.14615941048\n",
      "epoch 12, batch 313, train_loss 2.86609697342\n",
      "epoch 12, batch 314, train_loss 3.18860125542\n",
      "epoch 12, batch 315, train_loss 2.68499922752\n",
      "epoch 12, batch 316, train_loss 2.84395337105\n",
      "epoch 12, batch 317, train_loss 3.30970573425\n",
      "epoch 12, batch 318, train_loss 2.45358705521\n",
      "epoch 12, batch 319, train_loss 4.11457061768\n",
      "epoch 12, batch 320, train_loss 3.01677060127\n",
      "epoch 12, batch 321, train_loss 4.20746421814\n",
      "epoch 12, batch 322, train_loss 2.34887051582\n",
      "epoch 12, batch 323, train_loss 2.22571587563\n",
      "epoch 12, batch 324, train_loss 3.31236600876\n",
      "epoch 12, batch 325, train_loss 3.00639414787\n",
      "epoch 12, batch 326, train_loss 2.99285793304\n",
      "epoch 12, batch 327, train_loss 2.96790003777\n",
      "epoch 12, batch 328, train_loss 2.84395909309\n",
      "epoch 12, batch 329, train_loss 2.5367205143\n",
      "epoch 12, batch 330, train_loss 3.83987593651\n",
      "epoch 12, batch 331, train_loss 3.53973269463\n",
      "epoch 12, batch 332, train_loss 2.39003777504\n",
      "epoch 12, batch 333, train_loss 3.40802383423\n",
      "epoch 12, batch 334, train_loss 3.11664032936\n",
      "epoch 12, batch 335, train_loss 3.02994585037\n",
      "epoch 12, batch 336, train_loss 3.56746315956\n",
      "epoch 12, batch 337, train_loss 2.1946849823\n",
      "epoch 12, batch 338, train_loss 2.99924898148\n",
      "epoch 12, batch 339, train_loss 2.79397797585\n",
      "epoch 12, batch 340, train_loss 2.99289798737\n",
      "epoch 12, batch 341, train_loss 2.93778014183\n",
      "epoch 12, batch 342, train_loss 2.96888327599\n",
      "epoch 12, batch 343, train_loss 2.87092065811\n",
      "epoch 12, batch 344, train_loss 3.76104235649\n",
      "epoch 12, batch 345, train_loss 3.19908261299\n",
      "epoch 12, batch 346, train_loss 2.9785425663\n",
      "epoch 12, batch 347, train_loss 2.78516435623\n",
      "epoch 12, batch 348, train_loss 3.03973817825\n",
      "epoch 12, batch 349, train_loss 4.01082992554\n",
      "epoch 12, batch 350, train_loss 3.98559117317\n",
      "epoch 12, batch 351, train_loss 3.80387568474\n",
      "epoch 12, batch 352, train_loss 3.84495043755\n",
      "epoch 12, batch 353, train_loss 3.13236832619\n",
      "epoch 12, batch 354, train_loss 2.52506637573\n",
      "epoch 12, batch 355, train_loss 2.84399485588\n",
      "epoch 12, batch 356, train_loss 2.79837346077\n",
      "epoch 12, batch 357, train_loss 2.73897957802\n",
      "epoch 12, batch 358, train_loss 2.32243275642\n",
      "epoch 12, batch 359, train_loss 3.2661716938\n",
      "epoch 12, batch 360, train_loss 2.77499175072\n",
      "epoch 12, batch 361, train_loss 2.46491622925\n",
      "epoch 12, batch 362, train_loss 4.13965940475\n",
      "epoch 12, batch 363, train_loss 3.05748009682\n",
      "epoch 12, batch 364, train_loss 2.95732831955\n",
      "epoch 12, batch 365, train_loss 3.79952740669\n",
      "epoch 12, batch 366, train_loss 3.94117283821\n",
      "epoch 12, batch 367, train_loss 4.66949319839\n",
      "epoch 12, batch 368, train_loss 2.43734908104\n",
      "epoch 12, batch 369, train_loss 3.33478236198\n",
      "epoch 12, batch 370, train_loss 3.72852563858\n",
      "epoch 12, batch 371, train_loss 4.63875818253\n",
      "epoch 12, batch 372, train_loss 2.23559856415\n",
      "epoch 12, batch 373, train_loss 2.39388847351\n",
      "epoch 12, batch 374, train_loss 2.73295402527\n",
      "epoch 12, batch 375, train_loss 3.10750198364\n",
      "epoch 12, batch 376, train_loss 2.38925004005\n",
      "epoch 12, batch 377, train_loss 2.77774381638\n",
      "epoch 12, batch 378, train_loss 3.4571211338\n",
      "epoch 12, batch 379, train_loss 2.65163350105\n",
      "epoch 12, batch 380, train_loss 2.67777729034\n",
      "epoch 12, batch 381, train_loss 2.77588534355\n",
      "epoch 12, batch 382, train_loss 3.59929513931\n",
      "epoch 12, batch 383, train_loss 3.20676064491\n",
      "epoch 12, batch 384, train_loss 2.88463044167\n",
      "epoch 12, batch 385, train_loss 4.2318687439\n",
      "epoch 12, batch 386, train_loss 2.57748556137\n",
      "epoch 12, batch 387, train_loss 4.30456972122\n",
      "epoch 12, batch 388, train_loss 2.58404636383\n",
      "epoch 12, batch 389, train_loss 4.09809494019\n",
      "epoch 12, batch 390, train_loss 4.09988212585\n",
      "epoch 12, batch 391, train_loss 4.13825273514\n",
      "epoch 12, batch 392, train_loss 3.91232061386\n",
      "epoch 12, batch 393, train_loss 3.16072583199\n",
      "epoch 12, batch 394, train_loss 3.7695069313\n",
      "epoch 12, batch 395, train_loss 2.76330685616\n",
      "epoch 12, batch 396, train_loss 3.99432659149\n",
      "epoch 12, batch 397, train_loss 4.08967542648\n",
      "epoch 12, batch 398, train_loss 3.09389734268\n",
      "epoch 12, batch 399, train_loss 3.30155134201\n",
      "epoch 12, batch 400, train_loss 4.22290086746\n",
      "epoch 12, batch 401, train_loss 3.5052382946\n",
      "epoch 12, batch 402, train_loss 3.10834479332\n",
      "epoch 12, batch 403, train_loss 3.04347085953\n",
      "epoch 12, batch 404, train_loss 3.85625481606\n",
      "epoch 12, batch 405, train_loss 3.37896490097\n",
      "epoch 12, batch 406, train_loss 3.24082541466\n",
      "epoch 12, batch 407, train_loss 3.0489180088\n",
      "epoch 12, batch 408, train_loss 2.89793801308\n",
      "epoch 12, batch 409, train_loss 2.43230867386\n",
      "epoch 12, batch 410, train_loss 4.0130944252\n",
      "epoch 12, batch 411, train_loss 3.12659358978\n",
      "epoch 12, batch 412, train_loss 2.64986419678\n",
      "epoch 12, batch 413, train_loss 2.97902464867\n",
      "epoch 12, batch 414, train_loss 3.47891974449\n",
      "epoch 12, batch 415, train_loss 3.56013536453\n",
      "epoch 12, batch 416, train_loss 3.31090044975\n",
      "epoch 12, batch 417, train_loss 2.87383055687\n",
      "epoch 12, batch 418, train_loss 3.2623898983\n",
      "epoch 12, batch 419, train_loss 2.90943813324\n",
      "epoch 12, batch 420, train_loss 2.37292027473\n",
      "epoch 12, batch 421, train_loss 4.11141729355\n",
      "epoch 12, batch 422, train_loss 2.88416481018\n",
      "epoch 12, batch 423, train_loss 3.00120735168\n",
      "epoch 12, batch 424, train_loss 4.05419540405\n",
      "epoch 12, batch 425, train_loss 3.39410567284\n",
      "epoch 12, batch 426, train_loss 4.50545501709\n",
      "epoch 12, batch 427, train_loss 3.66952896118\n",
      "epoch 12, batch 428, train_loss 2.50348925591\n",
      "epoch 12, batch 429, train_loss 3.47549939156\n",
      "epoch 12, batch 430, train_loss 3.51398062706\n",
      "epoch 12, batch 431, train_loss 3.13792920113\n",
      "epoch 12, batch 432, train_loss 3.33413386345\n",
      "epoch 12, batch 433, train_loss 2.76672840118\n",
      "epoch 12, batch 434, train_loss 3.26775193214\n",
      "epoch 12, batch 435, train_loss 3.08812308311\n",
      "epoch 12, batch 436, train_loss 3.41745138168\n",
      "epoch 12, batch 437, train_loss 3.84852695465\n",
      "epoch 12, batch 438, train_loss 3.53442597389\n",
      "epoch 12, batch 439, train_loss 3.28496527672\n",
      "epoch 12, batch 440, train_loss 4.51619720459\n",
      "epoch 12, batch 441, train_loss 4.63961982727\n",
      "epoch 12, batch 442, train_loss 3.8466989994\n",
      "epoch 12, batch 443, train_loss 2.36115241051\n",
      "epoch 12, batch 444, train_loss 2.87399196625\n",
      "epoch 12, batch 445, train_loss 4.41827630997\n",
      "epoch 12, batch 446, train_loss 3.22376298904\n",
      "epoch 12, batch 447, train_loss 2.92542815208\n",
      "epoch 12, batch 448, train_loss 4.29257392883\n",
      "epoch 12, batch 449, train_loss 3.7102560997\n",
      "epoch 12, batch 450, train_loss 3.04178524017\n",
      "epoch 12, batch 451, train_loss 2.78549575806\n",
      "epoch 12, batch 452, train_loss 2.5581882\n",
      "epoch 12, batch 453, train_loss 4.89846372604\n",
      "epoch 12, batch 454, train_loss 4.73391199112\n",
      "epoch 12, batch 455, train_loss 2.44736266136\n",
      "epoch 12, batch 456, train_loss 3.10493206978\n",
      "epoch 12, batch 457, train_loss 3.29560756683\n",
      "epoch 12, batch 458, train_loss 2.98313736916\n",
      "epoch 12, batch 459, train_loss 3.23729801178\n",
      "epoch 12, batch 460, train_loss 3.71106982231\n",
      "epoch 12, batch 461, train_loss 3.2286491394\n",
      "epoch 12, batch 462, train_loss 2.92163777351\n",
      "epoch 12, batch 463, train_loss 3.0646314621\n",
      "epoch 12, batch 464, train_loss 3.34024810791\n",
      "epoch 12, batch 465, train_loss 2.75962400436\n",
      "epoch 12, batch 466, train_loss 3.49435305595\n",
      "epoch 12, batch 467, train_loss 3.32543253899\n",
      "epoch 12, batch 468, train_loss 3.46216487885\n",
      "epoch 12, batch 469, train_loss 3.03774619102\n",
      "epoch 12, batch 470, train_loss 2.90613222122\n",
      "epoch 12, batch 471, train_loss 2.99287819862\n",
      "epoch 12, batch 472, train_loss 2.85874867439\n",
      "epoch 12, batch 473, train_loss 3.53809833527\n",
      "epoch 12, batch 474, train_loss 4.09397125244\n",
      "epoch 12, batch 475, train_loss 2.89193797112\n",
      "epoch 12, batch 476, train_loss 2.4153444767\n",
      "epoch 12, batch 477, train_loss 2.48941159248\n",
      "epoch 12, batch 478, train_loss 2.33257913589\n",
      "epoch 12, batch 479, train_loss 2.84266901016\n",
      "epoch 12, batch 480, train_loss 3.0073595047\n",
      "epoch 12, batch 481, train_loss 4.51828193665\n",
      "epoch 12, batch 482, train_loss 2.73936128616\n",
      "epoch 12, batch 483, train_loss 3.33552932739\n",
      "epoch 12, batch 484, train_loss 3.02431869507\n",
      "epoch 12, batch 485, train_loss 2.2775015831\n",
      "epoch 12, batch 486, train_loss 2.28118634224\n",
      "epoch 12, batch 487, train_loss 2.44462633133\n",
      "epoch 12, batch 488, train_loss 2.84598231316\n",
      "epoch 12, batch 489, train_loss 2.97154068947\n",
      "epoch 12, batch 490, train_loss 2.87959337234\n",
      "epoch 12, batch 491, train_loss 2.81152963638\n",
      "epoch 12, batch 492, train_loss 3.1539785862\n",
      "epoch 12, batch 493, train_loss 2.76501178741\n",
      "epoch 12, batch 494, train_loss 2.85084486008\n",
      "epoch 12, batch 495, train_loss 2.92623829842\n",
      "epoch 12, batch 496, train_loss 2.5604057312\n",
      "epoch 12, batch 497, train_loss 2.73210859299\n",
      "epoch 12, batch 498, train_loss 2.85915613174\n",
      "epoch 12, batch 499, train_loss 2.81936097145\n",
      "epoch 12, batch 500, train_loss 3.22999882698\n",
      "epoch 12, batch 501, train_loss 2.91491174698\n",
      "epoch 12, batch 502, train_loss 4.13484287262\n",
      "epoch 12, batch 503, train_loss 4.05605697632\n",
      "epoch 12, batch 504, train_loss 3.54806232452\n",
      "epoch 12, batch 505, train_loss 3.37887954712\n",
      "epoch 12, batch 506, train_loss 3.06017279625\n",
      "epoch 12, batch 507, train_loss 4.03379487991\n",
      "epoch 12, batch 508, train_loss 2.91003036499\n",
      "epoch 12, batch 509, train_loss 4.16865158081\n",
      "epoch 12, batch 510, train_loss 4.12979650497\n",
      "epoch 12, batch 511, train_loss 4.15208387375\n",
      "epoch 12, batch 512, train_loss 3.25279474258\n",
      "epoch 12, batch 513, train_loss 4.57351875305\n",
      "epoch 12, batch 514, train_loss 4.51075553894\n",
      "epoch 12, batch 515, train_loss 4.41587972641\n",
      "epoch 12, batch 516, train_loss 2.74116420746\n",
      "epoch 12, batch 517, train_loss 2.9424123764\n",
      "epoch 12, batch 518, train_loss 2.84871602058\n",
      "epoch 12, batch 519, train_loss 2.542324543\n",
      "epoch 12, batch 520, train_loss 3.63793802261\n",
      "epoch 12, batch 521, train_loss 2.85251832008\n",
      "epoch 12, batch 522, train_loss 2.49649119377\n",
      "epoch 12, batch 523, train_loss 2.48866081238\n",
      "epoch 12, batch 524, train_loss 2.33688139915\n",
      "epoch 12, batch 525, train_loss 2.17172503471\n",
      "epoch 12, batch 526, train_loss 2.26628732681\n",
      "epoch 12, batch 527, train_loss 2.28299355507\n",
      "epoch 12, batch 528, train_loss 2.51754260063\n",
      "epoch 12, batch 529, train_loss 2.4086689949\n",
      "epoch 12, batch 530, train_loss 1.91766905785\n",
      "epoch 12, batch 531, train_loss 1.71019220352\n",
      "epoch 12, batch 532, train_loss 2.4591884613\n",
      "epoch 12, batch 533, train_loss 1.58259427547\n",
      "epoch 12, batch 534, train_loss 1.66463601589\n",
      "epoch 12, batch 535, train_loss 3.01102209091\n",
      "epoch 12, batch 536, train_loss 3.24490666389\n",
      "epoch 12, batch 537, train_loss 3.13612675667\n",
      "epoch 12, batch 538, train_loss 3.25387048721\n",
      "epoch 12, batch 539, train_loss 3.50163531303\n",
      "epoch 12, batch 540, train_loss 3.8241417408\n",
      "epoch 13, batch 0, train_loss 3.26817178726\n",
      "epoch 13, batch 1, train_loss 3.57881784439\n",
      "epoch 13, batch 2, train_loss 2.89564180374\n",
      "epoch 13, batch 3, train_loss 3.19239497185\n",
      "epoch 13, batch 4, train_loss 2.86392307281\n",
      "epoch 13, batch 5, train_loss 2.93505811691\n",
      "epoch 13, batch 6, train_loss 3.79694247246\n",
      "epoch 13, batch 7, train_loss 2.90435886383\n",
      "epoch 13, batch 8, train_loss 2.58517980576\n",
      "epoch 13, batch 9, train_loss 3.50552892685\n",
      "epoch 13, batch 10, train_loss 2.84147644043\n",
      "epoch 13, batch 11, train_loss 2.90478491783\n",
      "epoch 13, batch 12, train_loss 2.65548658371\n",
      "epoch 13, batch 13, train_loss 3.12208676338\n",
      "epoch 13, batch 14, train_loss 2.56923747063\n",
      "epoch 13, batch 15, train_loss 3.27227163315\n",
      "epoch 13, batch 16, train_loss 2.81541013718\n",
      "epoch 13, batch 17, train_loss 2.68863677979\n",
      "epoch 13, batch 18, train_loss 2.38051605225\n",
      "epoch 13, batch 19, train_loss 1.96917152405\n",
      "epoch 13, batch 20, train_loss 2.75915622711\n",
      "epoch 13, batch 21, train_loss 2.69692754745\n",
      "epoch 13, batch 22, train_loss 3.16992759705\n",
      "epoch 13, batch 23, train_loss 2.78725361824\n",
      "epoch 13, batch 24, train_loss 2.80018854141\n",
      "epoch 13, batch 25, train_loss 3.2069683075\n",
      "epoch 13, batch 26, train_loss 3.13333249092\n",
      "epoch 13, batch 27, train_loss 3.05538630486\n",
      "epoch 13, batch 28, train_loss 3.34699487686\n",
      "epoch 13, batch 29, train_loss 3.24126672745\n",
      "epoch 13, batch 30, train_loss 3.33552694321\n",
      "epoch 13, batch 31, train_loss 2.78834486008\n",
      "epoch 13, batch 32, train_loss 2.60406422615\n",
      "epoch 13, batch 33, train_loss 4.33202314377\n",
      "epoch 13, batch 34, train_loss 4.4114985466\n",
      "epoch 13, batch 35, train_loss 3.13234043121\n",
      "epoch 13, batch 36, train_loss 3.14991116524\n",
      "epoch 13, batch 37, train_loss 3.09660387039\n",
      "epoch 13, batch 38, train_loss 3.24092888832\n",
      "epoch 13, batch 39, train_loss 3.71665668488\n",
      "epoch 13, batch 40, train_loss 2.81475996971\n",
      "epoch 13, batch 41, train_loss 3.11504483223\n",
      "epoch 13, batch 42, train_loss 3.10897159576\n",
      "epoch 13, batch 43, train_loss 2.87002825737\n",
      "epoch 13, batch 44, train_loss 3.52985191345\n",
      "epoch 13, batch 45, train_loss 3.15082240105\n",
      "epoch 13, batch 46, train_loss 3.11491274834\n",
      "epoch 13, batch 47, train_loss 3.17878460884\n",
      "epoch 13, batch 48, train_loss 3.01081061363\n",
      "epoch 13, batch 49, train_loss 3.13425946236\n",
      "epoch 13, batch 50, train_loss 3.06349563599\n",
      "epoch 13, batch 51, train_loss 2.71833062172\n",
      "epoch 13, batch 52, train_loss 3.15751218796\n",
      "epoch 13, batch 53, train_loss 3.00215601921\n",
      "epoch 13, batch 54, train_loss 2.79628825188\n",
      "epoch 13, batch 55, train_loss 2.84426760674\n",
      "epoch 13, batch 56, train_loss 2.7289083004\n",
      "epoch 13, batch 57, train_loss 3.17497992516\n",
      "epoch 13, batch 58, train_loss 2.99702000618\n",
      "epoch 13, batch 59, train_loss 2.94474601746\n",
      "epoch 13, batch 60, train_loss 3.1409239769\n",
      "epoch 13, batch 61, train_loss 1.81144118309\n",
      "epoch 13, batch 62, train_loss 2.73135185242\n",
      "epoch 13, batch 63, train_loss 2.88608241081\n",
      "epoch 13, batch 64, train_loss 3.13777446747\n",
      "epoch 13, batch 65, train_loss 3.44429731369\n",
      "epoch 13, batch 66, train_loss 3.20160627365\n",
      "epoch 13, batch 67, train_loss 2.06747817993\n",
      "epoch 13, batch 68, train_loss 2.89471507072\n",
      "epoch 13, batch 69, train_loss 2.26404190063\n",
      "epoch 13, batch 70, train_loss 3.71333622932\n",
      "epoch 13, batch 71, train_loss 3.79789805412\n",
      "epoch 13, batch 72, train_loss 2.70010137558\n",
      "epoch 13, batch 73, train_loss 3.10015511513\n",
      "epoch 13, batch 74, train_loss 3.62989187241\n",
      "epoch 13, batch 75, train_loss 3.13130736351\n",
      "epoch 13, batch 76, train_loss 3.40991830826\n",
      "epoch 13, batch 77, train_loss 4.04187297821\n",
      "epoch 13, batch 78, train_loss 2.98282074928\n",
      "epoch 13, batch 79, train_loss 3.00124311447\n",
      "epoch 13, batch 80, train_loss 2.82765841484\n",
      "epoch 13, batch 81, train_loss 2.98420882225\n",
      "epoch 13, batch 82, train_loss 2.88607597351\n",
      "epoch 13, batch 83, train_loss 3.01498246193\n",
      "epoch 13, batch 84, train_loss 2.76516652107\n",
      "epoch 13, batch 85, train_loss 2.78049397469\n",
      "epoch 13, batch 86, train_loss 3.30384206772\n",
      "epoch 13, batch 87, train_loss 2.51343607903\n",
      "epoch 13, batch 88, train_loss 2.82705450058\n",
      "epoch 13, batch 89, train_loss 3.11797475815\n",
      "epoch 13, batch 90, train_loss 2.82314443588\n",
      "epoch 13, batch 91, train_loss 2.85037946701\n",
      "epoch 13, batch 92, train_loss 3.28457856178\n",
      "epoch 13, batch 93, train_loss 3.98595762253\n",
      "epoch 13, batch 94, train_loss 2.85778069496\n",
      "epoch 13, batch 95, train_loss 2.92689013481\n",
      "epoch 13, batch 96, train_loss 2.32979106903\n",
      "epoch 13, batch 97, train_loss 2.89677500725\n",
      "epoch 13, batch 98, train_loss 2.86013627052\n",
      "epoch 13, batch 99, train_loss 3.0652050972\n",
      "epoch 13, batch 100, train_loss 2.86791658401\n",
      "epoch 13, batch 101, train_loss 3.01144337654\n",
      "epoch 13, batch 102, train_loss 2.83611083031\n",
      "epoch 13, batch 103, train_loss 4.04655122757\n",
      "epoch 13, batch 104, train_loss 3.34435629845\n",
      "epoch 13, batch 105, train_loss 3.16813135147\n",
      "epoch 13, batch 106, train_loss 3.54426288605\n",
      "epoch 13, batch 107, train_loss 3.37088608742\n",
      "epoch 13, batch 108, train_loss 3.00456261635\n",
      "epoch 13, batch 109, train_loss 3.27972960472\n",
      "epoch 13, batch 110, train_loss 3.08901500702\n",
      "epoch 13, batch 111, train_loss 3.4911904335\n",
      "epoch 13, batch 112, train_loss 3.43577599525\n",
      "epoch 13, batch 113, train_loss 2.84883999825\n",
      "epoch 13, batch 114, train_loss 3.41395926476\n",
      "epoch 13, batch 115, train_loss 4.07396650314\n",
      "epoch 13, batch 116, train_loss 3.96196341515\n",
      "epoch 13, batch 117, train_loss 4.13176584244\n",
      "epoch 13, batch 118, train_loss 3.59413528442\n",
      "epoch 13, batch 119, train_loss 2.37092971802\n",
      "epoch 13, batch 120, train_loss 3.17114496231\n",
      "epoch 13, batch 121, train_loss 2.98131155968\n",
      "epoch 13, batch 122, train_loss 3.18179011345\n",
      "epoch 13, batch 123, train_loss 2.97630548477\n",
      "epoch 13, batch 124, train_loss 2.68712759018\n",
      "epoch 13, batch 125, train_loss 2.65371632576\n",
      "epoch 13, batch 126, train_loss 2.63928389549\n",
      "epoch 13, batch 127, train_loss 2.70060420036\n",
      "epoch 13, batch 128, train_loss 2.50686192513\n",
      "epoch 13, batch 129, train_loss 2.93025898933\n",
      "epoch 13, batch 130, train_loss 2.86725878716\n",
      "epoch 13, batch 131, train_loss 3.23012733459\n",
      "epoch 13, batch 132, train_loss 2.98409867287\n",
      "epoch 13, batch 133, train_loss 2.25388169289\n",
      "epoch 13, batch 134, train_loss 2.27520227432\n",
      "epoch 13, batch 135, train_loss 3.94806528091\n",
      "epoch 13, batch 136, train_loss 3.41819381714\n",
      "epoch 13, batch 137, train_loss 2.60575008392\n",
      "epoch 13, batch 138, train_loss 2.4831237793\n",
      "epoch 13, batch 139, train_loss 2.97482824326\n",
      "epoch 13, batch 140, train_loss 2.48351693153\n",
      "epoch 13, batch 141, train_loss 2.77797102928\n",
      "epoch 13, batch 142, train_loss 3.92417168617\n",
      "epoch 13, batch 143, train_loss 2.48377037048\n",
      "epoch 13, batch 144, train_loss 2.02613997459\n",
      "epoch 13, batch 145, train_loss 2.96804046631\n",
      "epoch 13, batch 146, train_loss 3.14991807938\n",
      "epoch 13, batch 147, train_loss 2.89079284668\n",
      "epoch 13, batch 148, train_loss 2.79166102409\n",
      "epoch 13, batch 149, train_loss 3.19737935066\n",
      "epoch 13, batch 150, train_loss 3.32658553123\n",
      "epoch 13, batch 151, train_loss 2.04195547104\n",
      "epoch 13, batch 152, train_loss 2.26409316063\n",
      "epoch 13, batch 153, train_loss 3.84649038315\n",
      "epoch 13, batch 154, train_loss 3.06491136551\n",
      "epoch 13, batch 155, train_loss 2.6678712368\n",
      "epoch 13, batch 156, train_loss 2.86371302605\n",
      "epoch 13, batch 157, train_loss 2.47276711464\n",
      "epoch 13, batch 158, train_loss 2.48128843307\n",
      "epoch 13, batch 159, train_loss 2.68541884422\n",
      "epoch 13, batch 160, train_loss 2.77127194405\n",
      "epoch 13, batch 161, train_loss 3.29347872734\n",
      "epoch 13, batch 162, train_loss 3.28438544273\n",
      "epoch 13, batch 163, train_loss 4.68268823624\n",
      "epoch 13, batch 164, train_loss 2.25822162628\n",
      "epoch 13, batch 165, train_loss 2.22545170784\n",
      "epoch 13, batch 166, train_loss 2.50085234642\n",
      "epoch 13, batch 167, train_loss 2.52733540535\n",
      "epoch 13, batch 168, train_loss 2.72825860977\n",
      "epoch 13, batch 169, train_loss 2.55355882645\n",
      "epoch 13, batch 170, train_loss 3.03569102287\n",
      "epoch 13, batch 171, train_loss 2.47569656372\n",
      "epoch 13, batch 172, train_loss 3.0560631752\n",
      "epoch 13, batch 173, train_loss 2.05707621574\n",
      "epoch 13, batch 174, train_loss 3.5167722702\n",
      "epoch 13, batch 175, train_loss 3.02993249893\n",
      "epoch 13, batch 176, train_loss 2.92518424988\n",
      "epoch 13, batch 177, train_loss 2.73531365395\n",
      "epoch 13, batch 178, train_loss 2.66895246506\n",
      "epoch 13, batch 179, train_loss 2.93786859512\n",
      "epoch 13, batch 180, train_loss 2.79951024055\n",
      "epoch 13, batch 181, train_loss 3.39733219147\n",
      "epoch 13, batch 182, train_loss 2.91996741295\n",
      "epoch 13, batch 183, train_loss 3.54042053223\n",
      "epoch 13, batch 184, train_loss 2.31631207466\n",
      "epoch 13, batch 185, train_loss 2.66867494583\n",
      "epoch 13, batch 186, train_loss 3.23650431633\n",
      "epoch 13, batch 187, train_loss 2.35442733765\n",
      "epoch 13, batch 188, train_loss 2.69869351387\n",
      "epoch 13, batch 189, train_loss 2.67846012115\n",
      "epoch 13, batch 190, train_loss 3.41520261765\n",
      "epoch 13, batch 191, train_loss 3.26744818687\n",
      "epoch 13, batch 192, train_loss 4.17263174057\n",
      "epoch 13, batch 193, train_loss 3.90065383911\n",
      "epoch 13, batch 194, train_loss 4.48963689804\n",
      "epoch 13, batch 195, train_loss 4.57136678696\n",
      "epoch 13, batch 196, train_loss 3.06697010994\n",
      "epoch 13, batch 197, train_loss 2.45267057419\n",
      "epoch 13, batch 198, train_loss 2.25716018677\n",
      "epoch 13, batch 199, train_loss 3.20338225365\n",
      "epoch 13, batch 200, train_loss 2.71777963638\n",
      "epoch 13, batch 201, train_loss 2.99243712425\n",
      "epoch 13, batch 202, train_loss 3.42803430557\n",
      "epoch 13, batch 203, train_loss 3.57043838501\n",
      "epoch 13, batch 204, train_loss 3.56438136101\n",
      "epoch 13, batch 205, train_loss 3.00156021118\n",
      "epoch 13, batch 206, train_loss 3.99395871162\n",
      "epoch 13, batch 207, train_loss 3.37615275383\n",
      "epoch 13, batch 208, train_loss 4.06026887894\n",
      "epoch 13, batch 209, train_loss 3.79799246788\n",
      "epoch 13, batch 210, train_loss 2.08118963242\n",
      "epoch 13, batch 211, train_loss 2.80847120285\n",
      "epoch 13, batch 212, train_loss 3.71191906929\n",
      "epoch 13, batch 213, train_loss 3.04956650734\n",
      "epoch 13, batch 214, train_loss 2.92427372932\n",
      "epoch 13, batch 215, train_loss 2.16664004326\n",
      "epoch 13, batch 216, train_loss 3.27919793129\n",
      "epoch 13, batch 217, train_loss 2.74833512306\n",
      "epoch 13, batch 218, train_loss 3.18587517738\n",
      "epoch 13, batch 219, train_loss 3.28044915199\n",
      "epoch 13, batch 220, train_loss 3.3303873539\n",
      "epoch 13, batch 221, train_loss 3.45002865791\n",
      "epoch 13, batch 222, train_loss 2.24197149277\n",
      "epoch 13, batch 223, train_loss 2.23944473267\n",
      "epoch 13, batch 224, train_loss 2.59120702744\n",
      "epoch 13, batch 225, train_loss 2.71092891693\n",
      "epoch 13, batch 226, train_loss 3.20792913437\n",
      "epoch 13, batch 227, train_loss 2.97913169861\n",
      "epoch 13, batch 228, train_loss 3.12644600868\n",
      "epoch 13, batch 229, train_loss 2.94611334801\n",
      "epoch 13, batch 230, train_loss 2.82792210579\n",
      "epoch 13, batch 231, train_loss 2.53708052635\n",
      "epoch 13, batch 232, train_loss 3.17032194138\n",
      "epoch 13, batch 233, train_loss 3.41503000259\n",
      "epoch 13, batch 234, train_loss 3.00370693207\n",
      "epoch 13, batch 235, train_loss 3.5350227356\n",
      "epoch 13, batch 236, train_loss 3.09949469566\n",
      "epoch 13, batch 237, train_loss 2.9903690815\n",
      "epoch 13, batch 238, train_loss 3.02721214294\n",
      "epoch 13, batch 239, train_loss 2.85396313667\n",
      "epoch 13, batch 240, train_loss 3.28079891205\n",
      "epoch 13, batch 241, train_loss 3.20949077606\n",
      "epoch 13, batch 242, train_loss 3.02697587013\n",
      "epoch 13, batch 243, train_loss 3.22440171242\n",
      "epoch 13, batch 244, train_loss 3.35921096802\n",
      "epoch 13, batch 245, train_loss 3.12814736366\n",
      "epoch 13, batch 246, train_loss 3.04089808464\n",
      "epoch 13, batch 247, train_loss 3.22480940819\n",
      "epoch 13, batch 248, train_loss 2.97166347504\n",
      "epoch 13, batch 249, train_loss 3.64318728447\n",
      "epoch 13, batch 250, train_loss 3.39262890816\n",
      "epoch 13, batch 251, train_loss 3.74504208565\n",
      "epoch 13, batch 252, train_loss 2.59461045265\n",
      "epoch 13, batch 253, train_loss 2.13570928574\n",
      "epoch 13, batch 254, train_loss 2.73416423798\n",
      "epoch 13, batch 255, train_loss 2.12696146965\n",
      "epoch 13, batch 256, train_loss 2.50261378288\n",
      "epoch 13, batch 257, train_loss 2.93842053413\n",
      "epoch 13, batch 258, train_loss 2.33973622322\n",
      "epoch 13, batch 259, train_loss 2.28447318077\n",
      "epoch 13, batch 260, train_loss 2.05532097816\n",
      "epoch 13, batch 261, train_loss 3.34562492371\n",
      "epoch 13, batch 262, train_loss 3.93906641006\n",
      "epoch 13, batch 263, train_loss 3.0738530159\n",
      "epoch 13, batch 264, train_loss 2.78645157814\n",
      "epoch 13, batch 265, train_loss 3.38549828529\n",
      "epoch 13, batch 266, train_loss 3.42693066597\n",
      "epoch 13, batch 267, train_loss 2.71320819855\n",
      "epoch 13, batch 268, train_loss 2.25375509262\n",
      "epoch 13, batch 269, train_loss 2.33812594414\n",
      "epoch 13, batch 270, train_loss 2.63228201866\n",
      "epoch 13, batch 271, train_loss 2.95837330818\n",
      "epoch 13, batch 272, train_loss 2.98045015335\n",
      "epoch 13, batch 273, train_loss 3.09050559998\n",
      "epoch 13, batch 274, train_loss 2.90702438354\n",
      "epoch 13, batch 275, train_loss 2.92688345909\n",
      "epoch 13, batch 276, train_loss 2.72500014305\n",
      "epoch 13, batch 277, train_loss 3.52590894699\n",
      "epoch 13, batch 278, train_loss 2.66808176041\n",
      "epoch 13, batch 279, train_loss 3.18388152122\n",
      "epoch 13, batch 280, train_loss 3.16017842293\n",
      "epoch 13, batch 281, train_loss 2.99954032898\n",
      "epoch 13, batch 282, train_loss 2.63130497932\n",
      "epoch 13, batch 283, train_loss 4.13075685501\n",
      "epoch 13, batch 284, train_loss 3.11099553108\n",
      "epoch 13, batch 285, train_loss 1.92472326756\n",
      "epoch 13, batch 286, train_loss 2.16709923744\n",
      "epoch 13, batch 287, train_loss 2.26323771477\n",
      "epoch 13, batch 288, train_loss 2.89980959892\n",
      "epoch 13, batch 289, train_loss 2.94230937958\n",
      "epoch 13, batch 290, train_loss 2.7685611248\n",
      "epoch 13, batch 291, train_loss 2.65403652191\n",
      "epoch 13, batch 292, train_loss 2.19247436523\n",
      "epoch 13, batch 293, train_loss 3.21787023544\n",
      "epoch 13, batch 294, train_loss 2.82382512093\n",
      "epoch 13, batch 295, train_loss 2.55436205864\n",
      "epoch 13, batch 296, train_loss 2.83322358131\n",
      "epoch 13, batch 297, train_loss 2.50938916206\n",
      "epoch 13, batch 298, train_loss 2.72903251648\n",
      "epoch 13, batch 299, train_loss 3.2186820507\n",
      "epoch 13, batch 300, train_loss 2.98440623283\n",
      "epoch 13, batch 301, train_loss 3.89716124535\n",
      "epoch 13, batch 302, train_loss 3.91187000275\n",
      "epoch 13, batch 303, train_loss 3.61642503738\n",
      "epoch 13, batch 304, train_loss 4.22842359543\n",
      "epoch 13, batch 305, train_loss 4.20837020874\n",
      "epoch 13, batch 306, train_loss 2.90381503105\n",
      "epoch 13, batch 307, train_loss 2.43867111206\n",
      "epoch 13, batch 308, train_loss 2.86943221092\n",
      "epoch 13, batch 309, train_loss 3.02159905434\n",
      "epoch 13, batch 310, train_loss 2.85463833809\n",
      "epoch 13, batch 311, train_loss 2.94831156731\n",
      "epoch 13, batch 312, train_loss 3.12114834785\n",
      "epoch 13, batch 313, train_loss 2.84393811226\n",
      "epoch 13, batch 314, train_loss 3.16182279587\n",
      "epoch 13, batch 315, train_loss 2.66617655754\n",
      "epoch 13, batch 316, train_loss 2.82483744621\n",
      "epoch 13, batch 317, train_loss 3.28311538696\n",
      "epoch 13, batch 318, train_loss 2.43438506126\n",
      "epoch 13, batch 319, train_loss 4.07487916946\n",
      "epoch 13, batch 320, train_loss 2.98880457878\n",
      "epoch 13, batch 321, train_loss 4.16961431503\n",
      "epoch 13, batch 322, train_loss 2.33156347275\n",
      "epoch 13, batch 323, train_loss 2.20832371712\n",
      "epoch 13, batch 324, train_loss 3.2847237587\n",
      "epoch 13, batch 325, train_loss 2.98634266853\n",
      "epoch 13, batch 326, train_loss 2.97120475769\n",
      "epoch 13, batch 327, train_loss 2.94504022598\n",
      "epoch 13, batch 328, train_loss 2.81980967522\n",
      "epoch 13, batch 329, train_loss 2.51571083069\n",
      "epoch 13, batch 330, train_loss 3.81048536301\n",
      "epoch 13, batch 331, train_loss 3.51484966278\n",
      "epoch 13, batch 332, train_loss 2.37308096886\n",
      "epoch 13, batch 333, train_loss 3.38071990013\n",
      "epoch 13, batch 334, train_loss 3.09211802483\n",
      "epoch 13, batch 335, train_loss 3.00494217873\n",
      "epoch 13, batch 336, train_loss 3.53365707397\n",
      "epoch 13, batch 337, train_loss 2.17679262161\n",
      "epoch 13, batch 338, train_loss 2.97585344315\n",
      "epoch 13, batch 339, train_loss 2.77345919609\n",
      "epoch 13, batch 340, train_loss 2.97027802467\n",
      "epoch 13, batch 341, train_loss 2.91794109344\n",
      "epoch 13, batch 342, train_loss 2.94526433945\n",
      "epoch 13, batch 343, train_loss 2.84859752655\n",
      "epoch 13, batch 344, train_loss 3.72538805008\n",
      "epoch 13, batch 345, train_loss 3.16917181015\n",
      "epoch 13, batch 346, train_loss 2.95127177238\n",
      "epoch 13, batch 347, train_loss 2.76206088066\n",
      "epoch 13, batch 348, train_loss 3.01568865776\n",
      "epoch 13, batch 349, train_loss 3.97678303719\n",
      "epoch 13, batch 350, train_loss 3.95014452934\n",
      "epoch 13, batch 351, train_loss 3.77281856537\n",
      "epoch 13, batch 352, train_loss 3.81298923492\n",
      "epoch 13, batch 353, train_loss 3.10691833496\n",
      "epoch 13, batch 354, train_loss 2.50068211555\n",
      "epoch 13, batch 355, train_loss 2.82199478149\n",
      "epoch 13, batch 356, train_loss 2.77623438835\n",
      "epoch 13, batch 357, train_loss 2.71371650696\n",
      "epoch 13, batch 358, train_loss 2.30245518684\n",
      "epoch 13, batch 359, train_loss 3.23741412163\n",
      "epoch 13, batch 360, train_loss 2.75037837029\n",
      "epoch 13, batch 361, train_loss 2.44338154793\n",
      "epoch 13, batch 362, train_loss 4.10302686691\n",
      "epoch 13, batch 363, train_loss 3.03404164314\n",
      "epoch 13, batch 364, train_loss 2.93389105797\n",
      "epoch 13, batch 365, train_loss 3.75979804993\n",
      "epoch 13, batch 366, train_loss 3.90564203262\n",
      "epoch 13, batch 367, train_loss 4.63049030304\n",
      "epoch 13, batch 368, train_loss 2.4173605442\n",
      "epoch 13, batch 369, train_loss 3.30714178085\n",
      "epoch 13, batch 370, train_loss 3.69837117195\n",
      "epoch 13, batch 371, train_loss 4.59943962097\n",
      "epoch 13, batch 372, train_loss 2.21861767769\n",
      "epoch 13, batch 373, train_loss 2.37638831139\n",
      "epoch 13, batch 374, train_loss 2.71360945702\n",
      "epoch 13, batch 375, train_loss 3.08394360542\n",
      "epoch 13, batch 376, train_loss 2.37392354012\n",
      "epoch 13, batch 377, train_loss 2.75740861893\n",
      "epoch 13, batch 378, train_loss 3.42953228951\n",
      "epoch 13, batch 379, train_loss 2.63233757019\n",
      "epoch 13, batch 380, train_loss 2.65676164627\n",
      "epoch 13, batch 381, train_loss 2.757376194\n",
      "epoch 13, batch 382, train_loss 3.56779003143\n",
      "epoch 13, batch 383, train_loss 3.18108320236\n",
      "epoch 13, batch 384, train_loss 2.86303186417\n",
      "epoch 13, batch 385, train_loss 4.1987118721\n",
      "epoch 13, batch 386, train_loss 2.55643033981\n",
      "epoch 13, batch 387, train_loss 4.25170707703\n",
      "epoch 13, batch 388, train_loss 2.56221795082\n",
      "epoch 13, batch 389, train_loss 4.05801582336\n",
      "epoch 13, batch 390, train_loss 4.06560850143\n",
      "epoch 13, batch 391, train_loss 4.10675954819\n",
      "epoch 13, batch 392, train_loss 3.88398122787\n",
      "epoch 13, batch 393, train_loss 3.13486909866\n",
      "epoch 13, batch 394, train_loss 3.74111056328\n",
      "epoch 13, batch 395, train_loss 2.74162459373\n",
      "epoch 13, batch 396, train_loss 3.95685458183\n",
      "epoch 13, batch 397, train_loss 4.05180549622\n",
      "epoch 13, batch 398, train_loss 3.06736588478\n",
      "epoch 13, batch 399, train_loss 3.27154874802\n",
      "epoch 13, batch 400, train_loss 4.18425035477\n",
      "epoch 13, batch 401, train_loss 3.47622323036\n",
      "epoch 13, batch 402, train_loss 3.08313179016\n",
      "epoch 13, batch 403, train_loss 3.01852965355\n",
      "epoch 13, batch 404, train_loss 3.82363295555\n",
      "epoch 13, batch 405, train_loss 3.34980869293\n",
      "epoch 13, batch 406, train_loss 3.21422982216\n",
      "epoch 13, batch 407, train_loss 3.02467679977\n",
      "epoch 13, batch 408, train_loss 2.87307476997\n",
      "epoch 13, batch 409, train_loss 2.41224908829\n",
      "epoch 13, batch 410, train_loss 3.98263502121\n",
      "epoch 13, batch 411, train_loss 3.09992146492\n",
      "epoch 13, batch 412, train_loss 2.62949895859\n",
      "epoch 13, batch 413, train_loss 2.9552834034\n",
      "epoch 13, batch 414, train_loss 3.44711947441\n",
      "epoch 13, batch 415, train_loss 3.53092575073\n",
      "epoch 13, batch 416, train_loss 3.28265237808\n",
      "epoch 13, batch 417, train_loss 2.85099387169\n",
      "epoch 13, batch 418, train_loss 3.23580741882\n",
      "epoch 13, batch 419, train_loss 2.88833022118\n",
      "epoch 13, batch 420, train_loss 2.35651826859\n",
      "epoch 13, batch 421, train_loss 4.07887458801\n",
      "epoch 13, batch 422, train_loss 2.86041378975\n",
      "epoch 13, batch 423, train_loss 2.9765958786\n",
      "epoch 13, batch 424, train_loss 4.02009868622\n",
      "epoch 13, batch 425, train_loss 3.36821889877\n",
      "epoch 13, batch 426, train_loss 4.47045516968\n",
      "epoch 13, batch 427, train_loss 3.63972878456\n",
      "epoch 13, batch 428, train_loss 2.48290348053\n",
      "epoch 13, batch 429, train_loss 3.44767284393\n",
      "epoch 13, batch 430, train_loss 3.4816403389\n",
      "epoch 13, batch 431, train_loss 3.11289548874\n",
      "epoch 13, batch 432, train_loss 3.3037917614\n",
      "epoch 13, batch 433, train_loss 2.74340415001\n",
      "epoch 13, batch 434, train_loss 3.23976778984\n",
      "epoch 13, batch 435, train_loss 3.06585979462\n",
      "epoch 13, batch 436, train_loss 3.39125156403\n",
      "epoch 13, batch 437, train_loss 3.81800580025\n",
      "epoch 13, batch 438, train_loss 3.51084899902\n",
      "epoch 13, batch 439, train_loss 3.25894093513\n",
      "epoch 13, batch 440, train_loss 4.4746594429\n",
      "epoch 13, batch 441, train_loss 4.60076808929\n",
      "epoch 13, batch 442, train_loss 3.81428480148\n",
      "epoch 13, batch 443, train_loss 2.34174084663\n",
      "epoch 13, batch 444, train_loss 2.85211396217\n",
      "epoch 13, batch 445, train_loss 4.3870472908\n",
      "epoch 13, batch 446, train_loss 3.20007324219\n",
      "epoch 13, batch 447, train_loss 2.9049975872\n",
      "epoch 13, batch 448, train_loss 4.2582192421\n",
      "epoch 13, batch 449, train_loss 3.68251252174\n",
      "epoch 13, batch 450, train_loss 3.01869630814\n",
      "epoch 13, batch 451, train_loss 2.76265263557\n",
      "epoch 13, batch 452, train_loss 2.53879094124\n",
      "epoch 13, batch 453, train_loss 4.83993625641\n",
      "epoch 13, batch 454, train_loss 4.67472743988\n",
      "epoch 13, batch 455, train_loss 2.42661738396\n",
      "epoch 13, batch 456, train_loss 3.08025836945\n",
      "epoch 13, batch 457, train_loss 3.25834846497\n",
      "epoch 13, batch 458, train_loss 2.9575779438\n",
      "epoch 13, batch 459, train_loss 3.20876336098\n",
      "epoch 13, batch 460, train_loss 3.67810201645\n",
      "epoch 13, batch 461, train_loss 3.19677233696\n",
      "epoch 13, batch 462, train_loss 2.89798021317\n",
      "epoch 13, batch 463, train_loss 3.03974080086\n",
      "epoch 13, batch 464, train_loss 3.3159122467\n",
      "epoch 13, batch 465, train_loss 2.73749780655\n",
      "epoch 13, batch 466, train_loss 3.46605062485\n",
      "epoch 13, batch 467, train_loss 3.29808735847\n",
      "epoch 13, batch 468, train_loss 3.43487811089\n",
      "epoch 13, batch 469, train_loss 3.0100069046\n",
      "epoch 13, batch 470, train_loss 2.87754821777\n",
      "epoch 13, batch 471, train_loss 2.96759128571\n",
      "epoch 13, batch 472, train_loss 2.83267688751\n",
      "epoch 13, batch 473, train_loss 3.50799274445\n",
      "epoch 13, batch 474, train_loss 4.05685138702\n",
      "epoch 13, batch 475, train_loss 2.86896753311\n",
      "epoch 13, batch 476, train_loss 2.39591813087\n",
      "epoch 13, batch 477, train_loss 2.46410751343\n",
      "epoch 13, batch 478, train_loss 2.30987477303\n",
      "epoch 13, batch 479, train_loss 2.82211756706\n",
      "epoch 13, batch 480, train_loss 2.97781682014\n",
      "epoch 13, batch 481, train_loss 4.47742128372\n",
      "epoch 13, batch 482, train_loss 2.71564435959\n",
      "epoch 13, batch 483, train_loss 3.3055036068\n",
      "epoch 13, batch 484, train_loss 2.99749231339\n",
      "epoch 13, batch 485, train_loss 2.25810360909\n",
      "epoch 13, batch 486, train_loss 2.25991773605\n",
      "epoch 13, batch 487, train_loss 2.42521190643\n",
      "epoch 13, batch 488, train_loss 2.824208498\n",
      "epoch 13, batch 489, train_loss 2.94998550415\n",
      "epoch 13, batch 490, train_loss 2.85335254669\n",
      "epoch 13, batch 491, train_loss 2.78933382034\n",
      "epoch 13, batch 492, train_loss 3.12900781631\n",
      "epoch 13, batch 493, train_loss 2.74769663811\n",
      "epoch 13, batch 494, train_loss 2.83296489716\n",
      "epoch 13, batch 495, train_loss 2.90719556808\n",
      "epoch 13, batch 496, train_loss 2.54338765144\n",
      "epoch 13, batch 497, train_loss 2.71347522736\n",
      "epoch 13, batch 498, train_loss 2.83636546135\n",
      "epoch 13, batch 499, train_loss 2.7968173027\n",
      "epoch 13, batch 500, train_loss 3.19760894775\n",
      "epoch 13, batch 501, train_loss 2.89092850685\n",
      "epoch 13, batch 502, train_loss 4.09564495087\n",
      "epoch 13, batch 503, train_loss 4.01491880417\n",
      "epoch 13, batch 504, train_loss 3.51558661461\n",
      "epoch 13, batch 505, train_loss 3.35419225693\n",
      "epoch 13, batch 506, train_loss 3.03356480598\n",
      "epoch 13, batch 507, train_loss 4.00172281265\n",
      "epoch 13, batch 508, train_loss 2.89185881615\n",
      "epoch 13, batch 509, train_loss 4.13127851486\n",
      "epoch 13, batch 510, train_loss 4.09412956238\n",
      "epoch 13, batch 511, train_loss 4.1201543808\n",
      "epoch 13, batch 512, train_loss 3.23047995567\n",
      "epoch 13, batch 513, train_loss 4.53913259506\n",
      "epoch 13, batch 514, train_loss 4.47529649734\n",
      "epoch 13, batch 515, train_loss 4.38315629959\n",
      "epoch 13, batch 516, train_loss 2.71603178978\n",
      "epoch 13, batch 517, train_loss 2.9200925827\n",
      "epoch 13, batch 518, train_loss 2.82684493065\n",
      "epoch 13, batch 519, train_loss 2.5225944519\n",
      "epoch 13, batch 520, train_loss 3.60131406784\n",
      "epoch 13, batch 521, train_loss 2.82813477516\n",
      "epoch 13, batch 522, train_loss 2.47832155228\n",
      "epoch 13, batch 523, train_loss 2.47287344933\n",
      "epoch 13, batch 524, train_loss 2.32001805305\n",
      "epoch 13, batch 525, train_loss 2.15582537651\n",
      "epoch 13, batch 526, train_loss 2.24943065643\n",
      "epoch 13, batch 527, train_loss 2.2620370388\n",
      "epoch 13, batch 528, train_loss 2.49288249016\n",
      "epoch 13, batch 529, train_loss 2.38535451889\n",
      "epoch 13, batch 530, train_loss 1.89863812923\n",
      "epoch 13, batch 531, train_loss 1.69949102402\n",
      "epoch 13, batch 532, train_loss 2.41749548912\n",
      "epoch 13, batch 533, train_loss 1.56803226471\n",
      "epoch 13, batch 534, train_loss 1.65067958832\n",
      "epoch 13, batch 535, train_loss 2.98570370674\n",
      "epoch 13, batch 536, train_loss 3.21692633629\n",
      "epoch 13, batch 537, train_loss 3.1017768383\n",
      "epoch 13, batch 538, train_loss 3.2133910656\n",
      "epoch 13, batch 539, train_loss 3.4595580101\n",
      "epoch 13, batch 540, train_loss 3.7819776535\n",
      "epoch 14, batch 0, train_loss 3.24251699448\n",
      "epoch 14, batch 1, train_loss 3.5504026413\n",
      "epoch 14, batch 2, train_loss 2.85946321487\n",
      "epoch 14, batch 3, train_loss 3.16126179695\n",
      "epoch 14, batch 4, train_loss 2.8137588501\n",
      "epoch 14, batch 5, train_loss 2.88631391525\n",
      "epoch 14, batch 6, train_loss 3.72543954849\n",
      "epoch 14, batch 7, train_loss 2.86251330376\n",
      "epoch 14, batch 8, train_loss 2.54537153244\n",
      "epoch 14, batch 9, train_loss 3.44016838074\n",
      "epoch 14, batch 10, train_loss 2.79826879501\n",
      "epoch 14, batch 11, train_loss 2.86365461349\n",
      "epoch 14, batch 12, train_loss 2.61927390099\n",
      "epoch 14, batch 13, train_loss 3.08008241653\n",
      "epoch 14, batch 14, train_loss 2.5356733799\n",
      "epoch 14, batch 15, train_loss 3.23163986206\n",
      "epoch 14, batch 16, train_loss 2.77560472488\n",
      "epoch 14, batch 17, train_loss 2.65483093262\n",
      "epoch 14, batch 18, train_loss 2.35538625717\n",
      "epoch 14, batch 19, train_loss 1.94969177246\n",
      "epoch 14, batch 20, train_loss 2.72610855103\n",
      "epoch 14, batch 21, train_loss 2.6592297554\n",
      "epoch 14, batch 22, train_loss 3.13338756561\n",
      "epoch 14, batch 23, train_loss 2.76200056076\n",
      "epoch 14, batch 24, train_loss 2.77905488014\n",
      "epoch 14, batch 25, train_loss 3.18082761765\n",
      "epoch 14, batch 26, train_loss 3.10039591789\n",
      "epoch 14, batch 27, train_loss 3.02427387238\n",
      "epoch 14, batch 28, train_loss 3.3168463707\n",
      "epoch 14, batch 29, train_loss 3.20935153961\n",
      "epoch 14, batch 30, train_loss 3.30445289612\n",
      "epoch 14, batch 31, train_loss 2.76209187508\n",
      "epoch 14, batch 32, train_loss 2.58598327637\n",
      "epoch 14, batch 33, train_loss 4.28892230988\n",
      "epoch 14, batch 34, train_loss 4.3659696579\n",
      "epoch 14, batch 35, train_loss 3.10593724251\n",
      "epoch 14, batch 36, train_loss 3.11781835556\n",
      "epoch 14, batch 37, train_loss 3.06944966316\n",
      "epoch 14, batch 38, train_loss 3.20905661583\n",
      "epoch 14, batch 39, train_loss 3.68734335899\n",
      "epoch 14, batch 40, train_loss 2.78884601593\n",
      "epoch 14, batch 41, train_loss 3.09295201302\n",
      "epoch 14, batch 42, train_loss 3.0820953846\n",
      "epoch 14, batch 43, train_loss 2.84103965759\n",
      "epoch 14, batch 44, train_loss 3.50284028053\n",
      "epoch 14, batch 45, train_loss 3.1269390583\n",
      "epoch 14, batch 46, train_loss 3.09228467941\n",
      "epoch 14, batch 47, train_loss 3.1521832943\n",
      "epoch 14, batch 48, train_loss 2.98206472397\n",
      "epoch 14, batch 49, train_loss 3.10625743866\n",
      "epoch 14, batch 50, train_loss 3.03734016418\n",
      "epoch 14, batch 51, train_loss 2.69098734856\n",
      "epoch 14, batch 52, train_loss 3.12819671631\n",
      "epoch 14, batch 53, train_loss 2.97209906578\n",
      "epoch 14, batch 54, train_loss 2.77582478523\n",
      "epoch 14, batch 55, train_loss 2.82205367088\n",
      "epoch 14, batch 56, train_loss 2.70761156082\n",
      "epoch 14, batch 57, train_loss 3.14641165733\n",
      "epoch 14, batch 58, train_loss 2.96903562546\n",
      "epoch 14, batch 59, train_loss 2.91702961922\n",
      "epoch 14, batch 60, train_loss 3.11452102661\n",
      "epoch 14, batch 61, train_loss 1.79574918747\n",
      "epoch 14, batch 62, train_loss 2.7056312561\n",
      "epoch 14, batch 63, train_loss 2.86015367508\n",
      "epoch 14, batch 64, train_loss 3.11519670486\n",
      "epoch 14, batch 65, train_loss 3.41965699196\n",
      "epoch 14, batch 66, train_loss 3.17746424675\n",
      "epoch 14, batch 67, train_loss 2.05114293098\n",
      "epoch 14, batch 68, train_loss 2.86973524094\n",
      "epoch 14, batch 69, train_loss 2.24567914009\n",
      "epoch 14, batch 70, train_loss 3.67430090904\n",
      "epoch 14, batch 71, train_loss 3.76454591751\n",
      "epoch 14, batch 72, train_loss 2.67914271355\n",
      "epoch 14, batch 73, train_loss 3.07349705696\n",
      "epoch 14, batch 74, train_loss 3.60035395622\n",
      "epoch 14, batch 75, train_loss 3.1022040844\n",
      "epoch 14, batch 76, train_loss 3.37882828712\n",
      "epoch 14, batch 77, train_loss 4.00318384171\n",
      "epoch 14, batch 78, train_loss 2.95854926109\n",
      "epoch 14, batch 79, train_loss 2.97579503059\n",
      "epoch 14, batch 80, train_loss 2.80242013931\n",
      "epoch 14, batch 81, train_loss 2.96387577057\n",
      "epoch 14, batch 82, train_loss 2.8662750721\n",
      "epoch 14, batch 83, train_loss 2.99214100838\n",
      "epoch 14, batch 84, train_loss 2.74544382095\n",
      "epoch 14, batch 85, train_loss 2.75950288773\n",
      "epoch 14, batch 86, train_loss 3.28143930435\n",
      "epoch 14, batch 87, train_loss 2.49297976494\n",
      "epoch 14, batch 88, train_loss 2.80766415596\n",
      "epoch 14, batch 89, train_loss 3.09729170799\n",
      "epoch 14, batch 90, train_loss 2.79915714264\n",
      "epoch 14, batch 91, train_loss 2.82687973976\n",
      "epoch 14, batch 92, train_loss 3.25863099098\n",
      "epoch 14, batch 93, train_loss 3.94754528999\n",
      "epoch 14, batch 94, train_loss 2.83443570137\n",
      "epoch 14, batch 95, train_loss 2.90208220482\n",
      "epoch 14, batch 96, train_loss 2.31053733826\n",
      "epoch 14, batch 97, train_loss 2.86707043648\n",
      "epoch 14, batch 98, train_loss 2.83892655373\n",
      "epoch 14, batch 99, train_loss 3.04020428658\n",
      "epoch 14, batch 100, train_loss 2.84610581398\n",
      "epoch 14, batch 101, train_loss 2.98624777794\n",
      "epoch 14, batch 102, train_loss 2.81266379356\n",
      "epoch 14, batch 103, train_loss 4.01449680328\n",
      "epoch 14, batch 104, train_loss 3.31703352928\n",
      "epoch 14, batch 105, train_loss 3.1435418129\n",
      "epoch 14, batch 106, train_loss 3.51645374298\n",
      "epoch 14, batch 107, train_loss 3.34676742554\n",
      "epoch 14, batch 108, train_loss 2.98284816742\n",
      "epoch 14, batch 109, train_loss 3.2566485405\n",
      "epoch 14, batch 110, train_loss 3.06442928314\n",
      "epoch 14, batch 111, train_loss 3.46563220024\n",
      "epoch 14, batch 112, train_loss 3.41233778\n",
      "epoch 14, batch 113, train_loss 2.82474422455\n",
      "epoch 14, batch 114, train_loss 3.38763237\n",
      "epoch 14, batch 115, train_loss 4.03609800339\n",
      "epoch 14, batch 116, train_loss 3.92662715912\n",
      "epoch 14, batch 117, train_loss 4.09542894363\n",
      "epoch 14, batch 118, train_loss 3.56660938263\n",
      "epoch 14, batch 119, train_loss 2.34753036499\n",
      "epoch 14, batch 120, train_loss 3.14869785309\n",
      "epoch 14, batch 121, train_loss 2.95845222473\n",
      "epoch 14, batch 122, train_loss 3.15451669693\n",
      "epoch 14, batch 123, train_loss 2.95320725441\n",
      "epoch 14, batch 124, train_loss 2.66747140884\n",
      "epoch 14, batch 125, train_loss 2.63294053078\n",
      "epoch 14, batch 126, train_loss 2.62053489685\n",
      "epoch 14, batch 127, train_loss 2.68124365807\n",
      "epoch 14, batch 128, train_loss 2.47895812988\n",
      "epoch 14, batch 129, train_loss 2.90782618523\n",
      "epoch 14, batch 130, train_loss 2.84237861633\n",
      "epoch 14, batch 131, train_loss 3.20444417\n",
      "epoch 14, batch 132, train_loss 2.96154379845\n",
      "epoch 14, batch 133, train_loss 2.23616719246\n",
      "epoch 14, batch 134, train_loss 2.25770998001\n",
      "epoch 14, batch 135, train_loss 3.91399359703\n",
      "epoch 14, batch 136, train_loss 3.39447140694\n",
      "epoch 14, batch 137, train_loss 2.5827589035\n",
      "epoch 14, batch 138, train_loss 2.46561050415\n",
      "epoch 14, batch 139, train_loss 2.95115089417\n",
      "epoch 14, batch 140, train_loss 2.46443390846\n",
      "epoch 14, batch 141, train_loss 2.75488615036\n",
      "epoch 14, batch 142, train_loss 3.88609981537\n",
      "epoch 14, batch 143, train_loss 2.46258163452\n",
      "epoch 14, batch 144, train_loss 2.01211357117\n",
      "epoch 14, batch 145, train_loss 2.9449968338\n",
      "epoch 14, batch 146, train_loss 3.12437033653\n",
      "epoch 14, batch 147, train_loss 2.8696372509\n",
      "epoch 14, batch 148, train_loss 2.77128529549\n",
      "epoch 14, batch 149, train_loss 3.17087841034\n",
      "epoch 14, batch 150, train_loss 3.30207228661\n",
      "epoch 14, batch 151, train_loss 2.0274682045\n",
      "epoch 14, batch 152, train_loss 2.24461960793\n",
      "epoch 14, batch 153, train_loss 3.81261110306\n",
      "epoch 14, batch 154, train_loss 3.04091215134\n",
      "epoch 14, batch 155, train_loss 2.64480352402\n",
      "epoch 14, batch 156, train_loss 2.84180688858\n",
      "epoch 14, batch 157, train_loss 2.45438361168\n",
      "epoch 14, batch 158, train_loss 2.46416711807\n",
      "epoch 14, batch 159, train_loss 2.66571116447\n",
      "epoch 14, batch 160, train_loss 2.74978399277\n",
      "epoch 14, batch 161, train_loss 3.26858019829\n",
      "epoch 14, batch 162, train_loss 3.26337361336\n",
      "epoch 14, batch 163, train_loss 4.65076684952\n",
      "epoch 14, batch 164, train_loss 2.23999595642\n",
      "epoch 14, batch 165, train_loss 2.2115380764\n",
      "epoch 14, batch 166, train_loss 2.48176765442\n",
      "epoch 14, batch 167, train_loss 2.50617146492\n",
      "epoch 14, batch 168, train_loss 2.70227599144\n",
      "epoch 14, batch 169, train_loss 2.53206396103\n",
      "epoch 14, batch 170, train_loss 3.0068821907\n",
      "epoch 14, batch 171, train_loss 2.45487356186\n",
      "epoch 14, batch 172, train_loss 3.02901935577\n",
      "epoch 14, batch 173, train_loss 2.04240202904\n",
      "epoch 14, batch 174, train_loss 3.48429179192\n",
      "epoch 14, batch 175, train_loss 3.00322437286\n",
      "epoch 14, batch 176, train_loss 2.90318703651\n",
      "epoch 14, batch 177, train_loss 2.71173286438\n",
      "epoch 14, batch 178, train_loss 2.64606690407\n",
      "epoch 14, batch 179, train_loss 2.91685342789\n",
      "epoch 14, batch 180, train_loss 2.7774040699\n",
      "epoch 14, batch 181, train_loss 3.37105464935\n",
      "epoch 14, batch 182, train_loss 2.89804697037\n",
      "epoch 14, batch 183, train_loss 3.50943350792\n",
      "epoch 14, batch 184, train_loss 2.29969120026\n",
      "epoch 14, batch 185, train_loss 2.64916872978\n",
      "epoch 14, batch 186, train_loss 3.21005821228\n",
      "epoch 14, batch 187, train_loss 2.33402776718\n",
      "epoch 14, batch 188, train_loss 2.67759084702\n",
      "epoch 14, batch 189, train_loss 2.65798091888\n",
      "epoch 14, batch 190, train_loss 3.38802194595\n",
      "epoch 14, batch 191, train_loss 3.23964500427\n",
      "epoch 14, batch 192, train_loss 4.13442087173\n",
      "epoch 14, batch 193, train_loss 3.86857938766\n",
      "epoch 14, batch 194, train_loss 4.45274496078\n",
      "epoch 14, batch 195, train_loss 4.53422021866\n",
      "epoch 14, batch 196, train_loss 3.04194188118\n",
      "epoch 14, batch 197, train_loss 2.43452072144\n",
      "epoch 14, batch 198, train_loss 2.24058914185\n",
      "epoch 14, batch 199, train_loss 3.17284917831\n",
      "epoch 14, batch 200, train_loss 2.69866895676\n",
      "epoch 14, batch 201, train_loss 2.96559071541\n",
      "epoch 14, batch 202, train_loss 3.40171694756\n",
      "epoch 14, batch 203, train_loss 3.54357981682\n",
      "epoch 14, batch 204, train_loss 3.53946304321\n",
      "epoch 14, batch 205, train_loss 2.97686290741\n",
      "epoch 14, batch 206, train_loss 3.96241688728\n",
      "epoch 14, batch 207, train_loss 3.35224723816\n",
      "epoch 14, batch 208, train_loss 4.03071737289\n",
      "epoch 14, batch 209, train_loss 3.76972961426\n",
      "epoch 14, batch 210, train_loss 2.06508398056\n",
      "epoch 14, batch 211, train_loss 2.78506183624\n",
      "epoch 14, batch 212, train_loss 3.68078780174\n",
      "epoch 14, batch 213, train_loss 3.02299714088\n",
      "epoch 14, batch 214, train_loss 2.90096879005\n",
      "epoch 14, batch 215, train_loss 2.15009880066\n",
      "epoch 14, batch 216, train_loss 3.25294780731\n",
      "epoch 14, batch 217, train_loss 2.72926521301\n",
      "epoch 14, batch 218, train_loss 3.16072654724\n",
      "epoch 14, batch 219, train_loss 3.25508975983\n",
      "epoch 14, batch 220, train_loss 3.30682778358\n",
      "epoch 14, batch 221, train_loss 3.42076683044\n",
      "epoch 14, batch 222, train_loss 2.22583079338\n",
      "epoch 14, batch 223, train_loss 2.22313022614\n",
      "epoch 14, batch 224, train_loss 2.57094550133\n",
      "epoch 14, batch 225, train_loss 2.69181251526\n",
      "epoch 14, batch 226, train_loss 3.18267059326\n",
      "epoch 14, batch 227, train_loss 2.95782351494\n",
      "epoch 14, batch 228, train_loss 3.10375237465\n",
      "epoch 14, batch 229, train_loss 2.92364406586\n",
      "epoch 14, batch 230, train_loss 2.80560183525\n",
      "epoch 14, batch 231, train_loss 2.51683521271\n",
      "epoch 14, batch 232, train_loss 3.14563775063\n",
      "epoch 14, batch 233, train_loss 3.38842558861\n",
      "epoch 14, batch 234, train_loss 2.98098659515\n",
      "epoch 14, batch 235, train_loss 3.50851106644\n",
      "epoch 14, batch 236, train_loss 3.07911872864\n",
      "epoch 14, batch 237, train_loss 2.96538376808\n",
      "epoch 14, batch 238, train_loss 3.00580596924\n",
      "epoch 14, batch 239, train_loss 2.83434224129\n",
      "epoch 14, batch 240, train_loss 3.25752997398\n",
      "epoch 14, batch 241, train_loss 3.18667030334\n",
      "epoch 14, batch 242, train_loss 3.00267553329\n",
      "epoch 14, batch 243, train_loss 3.19979715347\n",
      "epoch 14, batch 244, train_loss 3.33445310593\n",
      "epoch 14, batch 245, train_loss 3.1058280468\n",
      "epoch 14, batch 246, train_loss 3.01755595207\n",
      "epoch 14, batch 247, train_loss 3.20111465454\n",
      "epoch 14, batch 248, train_loss 2.94868159294\n",
      "epoch 14, batch 249, train_loss 3.61072921753\n",
      "epoch 14, batch 250, train_loss 3.36187148094\n",
      "epoch 14, batch 251, train_loss 3.71238994598\n",
      "epoch 14, batch 252, train_loss 2.57707977295\n",
      "epoch 14, batch 253, train_loss 2.11711478233\n",
      "epoch 14, batch 254, train_loss 2.71003937721\n",
      "epoch 14, batch 255, train_loss 2.10973834991\n",
      "epoch 14, batch 256, train_loss 2.48472976685\n",
      "epoch 14, batch 257, train_loss 2.91577482224\n",
      "epoch 14, batch 258, train_loss 2.32330012321\n",
      "epoch 14, batch 259, train_loss 2.2682902813\n",
      "epoch 14, batch 260, train_loss 2.04258060455\n",
      "epoch 14, batch 261, train_loss 3.32073736191\n",
      "epoch 14, batch 262, train_loss 3.90820813179\n",
      "epoch 14, batch 263, train_loss 3.05625200272\n",
      "epoch 14, batch 264, train_loss 2.768004179\n",
      "epoch 14, batch 265, train_loss 3.3593993187\n",
      "epoch 14, batch 266, train_loss 3.40051150322\n",
      "epoch 14, batch 267, train_loss 2.69391345978\n",
      "epoch 14, batch 268, train_loss 2.23542451859\n",
      "epoch 14, batch 269, train_loss 2.32089066505\n",
      "epoch 14, batch 270, train_loss 2.61408543587\n",
      "epoch 14, batch 271, train_loss 2.93616914749\n",
      "epoch 14, batch 272, train_loss 2.95744776726\n",
      "epoch 14, batch 273, train_loss 3.06679296494\n",
      "epoch 14, batch 274, train_loss 2.88305974007\n",
      "epoch 14, batch 275, train_loss 2.90674209595\n",
      "epoch 14, batch 276, train_loss 2.7064166069\n",
      "epoch 14, batch 277, train_loss 3.50172829628\n",
      "epoch 14, batch 278, train_loss 2.64566302299\n",
      "epoch 14, batch 279, train_loss 3.15774583817\n",
      "epoch 14, batch 280, train_loss 3.13256382942\n",
      "epoch 14, batch 281, train_loss 2.97747802734\n",
      "epoch 14, batch 282, train_loss 2.61213541031\n",
      "epoch 14, batch 283, train_loss 4.0903711319\n",
      "epoch 14, batch 284, train_loss 3.0834441185\n",
      "epoch 14, batch 285, train_loss 1.90939724445\n",
      "epoch 14, batch 286, train_loss 2.15204977989\n",
      "epoch 14, batch 287, train_loss 2.24790835381\n",
      "epoch 14, batch 288, train_loss 2.88236188889\n",
      "epoch 14, batch 289, train_loss 2.92140579224\n",
      "epoch 14, batch 290, train_loss 2.7453224659\n",
      "epoch 14, batch 291, train_loss 2.63368105888\n",
      "epoch 14, batch 292, train_loss 2.17906808853\n",
      "epoch 14, batch 293, train_loss 3.19282817841\n",
      "epoch 14, batch 294, train_loss 2.80244874954\n",
      "epoch 14, batch 295, train_loss 2.53730010986\n",
      "epoch 14, batch 296, train_loss 2.81263685226\n",
      "epoch 14, batch 297, train_loss 2.49305725098\n",
      "epoch 14, batch 298, train_loss 2.71019244194\n",
      "epoch 14, batch 299, train_loss 3.18646931648\n",
      "epoch 14, batch 300, train_loss 2.96006321907\n",
      "epoch 14, batch 301, train_loss 3.85781097412\n",
      "epoch 14, batch 302, train_loss 3.87497925758\n",
      "epoch 14, batch 303, train_loss 3.58694410324\n",
      "epoch 14, batch 304, train_loss 4.18936157227\n",
      "epoch 14, batch 305, train_loss 4.17339086533\n",
      "epoch 14, batch 306, train_loss 2.87870836258\n",
      "epoch 14, batch 307, train_loss 2.4213476181\n",
      "epoch 14, batch 308, train_loss 2.84738588333\n",
      "epoch 14, batch 309, train_loss 2.99947929382\n",
      "epoch 14, batch 310, train_loss 2.83702111244\n",
      "epoch 14, batch 311, train_loss 2.9263112545\n",
      "epoch 14, batch 312, train_loss 3.09817695618\n",
      "epoch 14, batch 313, train_loss 2.82261538506\n",
      "epoch 14, batch 314, train_loss 3.13764953613\n",
      "epoch 14, batch 315, train_loss 2.64951252937\n",
      "epoch 14, batch 316, train_loss 2.80794596672\n",
      "epoch 14, batch 317, train_loss 3.26067280769\n",
      "epoch 14, batch 318, train_loss 2.41686463356\n",
      "epoch 14, batch 319, train_loss 4.03852796555\n",
      "epoch 14, batch 320, train_loss 2.96270227432\n",
      "epoch 14, batch 321, train_loss 4.13486528397\n",
      "epoch 14, batch 322, train_loss 2.31643176079\n",
      "epoch 14, batch 323, train_loss 2.19361948967\n",
      "epoch 14, batch 324, train_loss 3.26156759262\n",
      "epoch 14, batch 325, train_loss 2.96915197372\n",
      "epoch 14, batch 326, train_loss 2.95176267624\n",
      "epoch 14, batch 327, train_loss 2.92514872551\n",
      "epoch 14, batch 328, train_loss 2.79778075218\n",
      "epoch 14, batch 329, train_loss 2.49574971199\n",
      "epoch 14, batch 330, train_loss 3.78543496132\n",
      "epoch 14, batch 331, train_loss 3.4920592308\n",
      "epoch 14, batch 332, train_loss 2.35807514191\n",
      "epoch 14, batch 333, train_loss 3.35597372055\n",
      "epoch 14, batch 334, train_loss 3.06919789314\n",
      "epoch 14, batch 335, train_loss 2.98286414146\n",
      "epoch 14, batch 336, train_loss 3.50343847275\n",
      "epoch 14, batch 337, train_loss 2.16031861305\n",
      "epoch 14, batch 338, train_loss 2.9557788372\n",
      "epoch 14, batch 339, train_loss 2.75540995598\n",
      "epoch 14, batch 340, train_loss 2.94893121719\n",
      "epoch 14, batch 341, train_loss 2.89976382256\n",
      "epoch 14, batch 342, train_loss 2.92435836792\n",
      "epoch 14, batch 343, train_loss 2.82861113548\n",
      "epoch 14, batch 344, train_loss 3.69297099113\n",
      "epoch 14, batch 345, train_loss 3.14192152023\n",
      "epoch 14, batch 346, train_loss 2.9266204834\n",
      "epoch 14, batch 347, train_loss 2.74000430107\n",
      "epoch 14, batch 348, train_loss 2.99411034584\n",
      "epoch 14, batch 349, train_loss 3.94579577446\n",
      "epoch 14, batch 350, train_loss 3.91742873192\n",
      "epoch 14, batch 351, train_loss 3.74233818054\n",
      "epoch 14, batch 352, train_loss 3.78354549408\n",
      "epoch 14, batch 353, train_loss 3.08289575577\n",
      "epoch 14, batch 354, train_loss 2.47913503647\n",
      "epoch 14, batch 355, train_loss 2.80267810822\n",
      "epoch 14, batch 356, train_loss 2.7563457489\n",
      "epoch 14, batch 357, train_loss 2.6916718483\n",
      "epoch 14, batch 358, train_loss 2.28558468819\n",
      "epoch 14, batch 359, train_loss 3.21002364159\n",
      "epoch 14, batch 360, train_loss 2.72806978226\n",
      "epoch 14, batch 361, train_loss 2.42407035828\n",
      "epoch 14, batch 362, train_loss 4.06786441803\n",
      "epoch 14, batch 363, train_loss 3.01279377937\n",
      "epoch 14, batch 364, train_loss 2.91236662865\n",
      "epoch 14, batch 365, train_loss 3.71885681152\n",
      "epoch 14, batch 366, train_loss 3.8736679554\n",
      "epoch 14, batch 367, train_loss 4.59268188477\n",
      "epoch 14, batch 368, train_loss 2.40016794205\n",
      "epoch 14, batch 369, train_loss 3.28220534325\n",
      "epoch 14, batch 370, train_loss 3.6682062149\n",
      "epoch 14, batch 371, train_loss 4.56324195862\n",
      "epoch 14, batch 372, train_loss 2.20186781883\n",
      "epoch 14, batch 373, train_loss 2.3568713665\n",
      "epoch 14, batch 374, train_loss 2.69635343552\n",
      "epoch 14, batch 375, train_loss 3.06218457222\n",
      "epoch 14, batch 376, train_loss 2.36063432693\n",
      "epoch 14, batch 377, train_loss 2.74021148682\n",
      "epoch 14, batch 378, train_loss 3.40190052986\n",
      "epoch 14, batch 379, train_loss 2.61436867714\n",
      "epoch 14, batch 380, train_loss 2.63804006577\n",
      "epoch 14, batch 381, train_loss 2.74055409431\n",
      "epoch 14, batch 382, train_loss 3.53872537613\n",
      "epoch 14, batch 383, train_loss 3.15738344193\n",
      "epoch 14, batch 384, train_loss 2.84363460541\n",
      "epoch 14, batch 385, train_loss 4.16719770432\n",
      "epoch 14, batch 386, train_loss 2.53771734238\n",
      "epoch 14, batch 387, train_loss 4.21219062805\n",
      "epoch 14, batch 388, train_loss 2.54237151146\n",
      "epoch 14, batch 389, train_loss 4.02065134048\n",
      "epoch 14, batch 390, train_loss 4.03541707993\n",
      "epoch 14, batch 391, train_loss 4.08043384552\n",
      "epoch 14, batch 392, train_loss 3.858522892\n",
      "epoch 14, batch 393, train_loss 3.1114988327\n",
      "epoch 14, batch 394, train_loss 3.71469664574\n",
      "epoch 14, batch 395, train_loss 2.72302532196\n",
      "epoch 14, batch 396, train_loss 3.92444968224\n",
      "epoch 14, batch 397, train_loss 4.01783227921\n",
      "epoch 14, batch 398, train_loss 3.04129600525\n",
      "epoch 14, batch 399, train_loss 3.2448592186\n",
      "epoch 14, batch 400, train_loss 4.14806890488\n",
      "epoch 14, batch 401, train_loss 3.44913697243\n",
      "epoch 14, batch 402, train_loss 3.05915331841\n",
      "epoch 14, batch 403, train_loss 2.99722671509\n",
      "epoch 14, batch 404, train_loss 3.79316926003\n",
      "epoch 14, batch 405, train_loss 3.32345628738\n",
      "epoch 14, batch 406, train_loss 3.19102072716\n",
      "epoch 14, batch 407, train_loss 3.00258255005\n",
      "epoch 14, batch 408, train_loss 2.85020208359\n",
      "epoch 14, batch 409, train_loss 2.39474987984\n",
      "epoch 14, batch 410, train_loss 3.95519900322\n",
      "epoch 14, batch 411, train_loss 3.07676172256\n",
      "epoch 14, batch 412, train_loss 2.61130142212\n",
      "epoch 14, batch 413, train_loss 2.93408918381\n",
      "epoch 14, batch 414, train_loss 3.41750669479\n",
      "epoch 14, batch 415, train_loss 3.50397968292\n",
      "epoch 14, batch 416, train_loss 3.25686454773\n",
      "epoch 14, batch 417, train_loss 2.83071446419\n",
      "epoch 14, batch 418, train_loss 3.21176218987\n",
      "epoch 14, batch 419, train_loss 2.86957073212\n",
      "epoch 14, batch 420, train_loss 2.34093093872\n",
      "epoch 14, batch 421, train_loss 4.04972553253\n",
      "epoch 14, batch 422, train_loss 2.83945512772\n",
      "epoch 14, batch 423, train_loss 2.95494961739\n",
      "epoch 14, batch 424, train_loss 3.9885866642\n",
      "epoch 14, batch 425, train_loss 3.34651494026\n",
      "epoch 14, batch 426, train_loss 4.44219350815\n",
      "epoch 14, batch 427, train_loss 3.61316108704\n",
      "epoch 14, batch 428, train_loss 2.465716362\n",
      "epoch 14, batch 429, train_loss 3.42276406288\n",
      "epoch 14, batch 430, train_loss 3.45201969147\n",
      "epoch 14, batch 431, train_loss 3.09304976463\n",
      "epoch 14, batch 432, train_loss 3.27808403969\n",
      "epoch 14, batch 433, train_loss 2.72374653816\n",
      "epoch 14, batch 434, train_loss 3.21666765213\n",
      "epoch 14, batch 435, train_loss 3.04659628868\n",
      "epoch 14, batch 436, train_loss 3.36861562729\n",
      "epoch 14, batch 437, train_loss 3.79188871384\n",
      "epoch 14, batch 438, train_loss 3.48874807358\n",
      "epoch 14, batch 439, train_loss 3.23493933678\n",
      "epoch 14, batch 440, train_loss 4.43707132339\n",
      "epoch 14, batch 441, train_loss 4.56510829926\n",
      "epoch 14, batch 442, train_loss 3.78448319435\n",
      "epoch 14, batch 443, train_loss 2.32446050644\n",
      "epoch 14, batch 444, train_loss 2.83280706406\n",
      "epoch 14, batch 445, train_loss 4.35706949234\n",
      "epoch 14, batch 446, train_loss 3.17965555191\n",
      "epoch 14, batch 447, train_loss 2.88648915291\n",
      "epoch 14, batch 448, train_loss 4.22646903992\n",
      "epoch 14, batch 449, train_loss 3.6569750309\n",
      "epoch 14, batch 450, train_loss 2.99568486214\n",
      "epoch 14, batch 451, train_loss 2.74120354652\n",
      "epoch 14, batch 452, train_loss 2.51891732216\n",
      "epoch 14, batch 453, train_loss 4.78375387192\n",
      "epoch 14, batch 454, train_loss 4.61657333374\n",
      "epoch 14, batch 455, train_loss 2.40586566925\n",
      "epoch 14, batch 456, train_loss 3.05563235283\n",
      "epoch 14, batch 457, train_loss 3.22405219078\n",
      "epoch 14, batch 458, train_loss 2.93403339386\n",
      "epoch 14, batch 459, train_loss 3.18259859085\n",
      "epoch 14, batch 460, train_loss 3.64769220352\n",
      "epoch 14, batch 461, train_loss 3.16818714142\n",
      "epoch 14, batch 462, train_loss 2.87571954727\n",
      "epoch 14, batch 463, train_loss 3.01640152931\n",
      "epoch 14, batch 464, train_loss 3.28653407097\n",
      "epoch 14, batch 465, train_loss 2.71585822105\n",
      "epoch 14, batch 466, train_loss 3.4414255619\n",
      "epoch 14, batch 467, train_loss 3.27357316017\n",
      "epoch 14, batch 468, train_loss 3.41068792343\n",
      "epoch 14, batch 469, train_loss 2.98523807526\n",
      "epoch 14, batch 470, train_loss 2.85330748558\n",
      "epoch 14, batch 471, train_loss 2.94333815575\n",
      "epoch 14, batch 472, train_loss 2.80912256241\n",
      "epoch 14, batch 473, train_loss 3.48077082634\n",
      "epoch 14, batch 474, train_loss 4.02276611328\n",
      "epoch 14, batch 475, train_loss 2.84792137146\n",
      "epoch 14, batch 476, train_loss 2.37828731537\n",
      "epoch 14, batch 477, train_loss 2.44059419632\n",
      "epoch 14, batch 478, train_loss 2.28798604012\n",
      "epoch 14, batch 479, train_loss 2.80276203156\n",
      "epoch 14, batch 480, train_loss 2.95059227943\n",
      "epoch 14, batch 481, train_loss 4.4398560524\n",
      "epoch 14, batch 482, train_loss 2.69434452057\n",
      "epoch 14, batch 483, train_loss 3.27557086945\n",
      "epoch 14, batch 484, train_loss 2.97113537788\n",
      "epoch 14, batch 485, train_loss 2.23826146126\n",
      "epoch 14, batch 486, train_loss 2.23952960968\n",
      "epoch 14, batch 487, train_loss 2.405418396\n",
      "epoch 14, batch 488, train_loss 2.80430936813\n",
      "epoch 14, batch 489, train_loss 2.92917656898\n",
      "epoch 14, batch 490, train_loss 2.83010005951\n",
      "epoch 14, batch 491, train_loss 2.76966905594\n",
      "epoch 14, batch 492, train_loss 3.10520362854\n",
      "epoch 14, batch 493, train_loss 2.73099517822\n",
      "epoch 14, batch 494, train_loss 2.81373667717\n",
      "epoch 14, batch 495, train_loss 2.88797283173\n",
      "epoch 14, batch 496, train_loss 2.52765822411\n",
      "epoch 14, batch 497, train_loss 2.69546556473\n",
      "epoch 14, batch 498, train_loss 2.81794476509\n",
      "epoch 14, batch 499, train_loss 2.77721142769\n",
      "epoch 14, batch 500, train_loss 3.17000412941\n",
      "epoch 14, batch 501, train_loss 2.86926388741\n",
      "epoch 14, batch 502, train_loss 4.05997848511\n",
      "epoch 14, batch 503, train_loss 3.97671055794\n",
      "epoch 14, batch 504, train_loss 3.48686504364\n",
      "epoch 14, batch 505, train_loss 3.3322968483\n",
      "epoch 14, batch 506, train_loss 3.01066064835\n",
      "epoch 14, batch 507, train_loss 3.97193360329\n",
      "epoch 14, batch 508, train_loss 2.87473940849\n",
      "epoch 14, batch 509, train_loss 4.09803628922\n",
      "epoch 14, batch 510, train_loss 4.06163692474\n",
      "epoch 14, batch 511, train_loss 4.08939027786\n",
      "epoch 14, batch 512, train_loss 3.2095553875\n",
      "epoch 14, batch 513, train_loss 4.50497579575\n",
      "epoch 14, batch 514, train_loss 4.43961763382\n",
      "epoch 14, batch 515, train_loss 4.35383176804\n",
      "epoch 14, batch 516, train_loss 2.69609379768\n",
      "epoch 14, batch 517, train_loss 2.89896583557\n",
      "epoch 14, batch 518, train_loss 2.80725026131\n",
      "epoch 14, batch 519, train_loss 2.50466918945\n",
      "epoch 14, batch 520, train_loss 3.57066941261\n",
      "epoch 14, batch 521, train_loss 2.80774879456\n",
      "epoch 14, batch 522, train_loss 2.46045422554\n",
      "epoch 14, batch 523, train_loss 2.45476770401\n",
      "epoch 14, batch 524, train_loss 2.30438566208\n",
      "epoch 14, batch 525, train_loss 2.14102864265\n",
      "epoch 14, batch 526, train_loss 2.23236012459\n",
      "epoch 14, batch 527, train_loss 2.24189209938\n",
      "epoch 14, batch 528, train_loss 2.47131037712\n",
      "epoch 14, batch 529, train_loss 2.36289381981\n",
      "epoch 14, batch 530, train_loss 1.88136041164\n",
      "epoch 14, batch 531, train_loss 1.68901455402\n",
      "epoch 14, batch 532, train_loss 2.38542795181\n",
      "epoch 14, batch 533, train_loss 1.54925572872\n",
      "epoch 14, batch 534, train_loss 1.64012312889\n",
      "epoch 14, batch 535, train_loss 2.95982313156\n",
      "epoch 14, batch 536, train_loss 3.18925309181\n",
      "epoch 14, batch 537, train_loss 3.07878351212\n",
      "epoch 14, batch 538, train_loss 3.18142771721\n",
      "epoch 14, batch 539, train_loss 3.42696380615\n",
      "epoch 14, batch 540, train_loss 3.74534368515\n",
      "epoch 15, batch 0, train_loss 3.21554398537\n",
      "epoch 15, batch 1, train_loss 3.52245354652\n",
      "epoch 15, batch 2, train_loss 2.83182764053\n",
      "epoch 15, batch 3, train_loss 3.13516068459\n",
      "epoch 15, batch 4, train_loss 2.7738878727\n",
      "epoch 15, batch 5, train_loss 2.84299063683\n",
      "epoch 15, batch 6, train_loss 3.65900945663\n",
      "epoch 15, batch 7, train_loss 2.82382583618\n",
      "epoch 15, batch 8, train_loss 2.51672935486\n",
      "epoch 15, batch 9, train_loss 3.38789796829\n",
      "epoch 15, batch 10, train_loss 2.75986552238\n",
      "epoch 15, batch 11, train_loss 2.82949662209\n",
      "epoch 15, batch 12, train_loss 2.58864378929\n",
      "epoch 15, batch 13, train_loss 3.04535484314\n",
      "epoch 15, batch 14, train_loss 2.50811457634\n",
      "epoch 15, batch 15, train_loss 3.19958519936\n",
      "epoch 15, batch 16, train_loss 2.74169301987\n",
      "epoch 15, batch 17, train_loss 2.62674117088\n",
      "epoch 15, batch 18, train_loss 2.33398127556\n",
      "epoch 15, batch 19, train_loss 1.93075871468\n",
      "epoch 15, batch 20, train_loss 2.6955845356\n",
      "epoch 15, batch 21, train_loss 2.62478756905\n",
      "epoch 15, batch 22, train_loss 3.0997710228\n",
      "epoch 15, batch 23, train_loss 2.73768496513\n",
      "epoch 15, batch 24, train_loss 2.75741624832\n",
      "epoch 15, batch 25, train_loss 3.15867948532\n",
      "epoch 15, batch 26, train_loss 3.07415533066\n",
      "epoch 15, batch 27, train_loss 3.0008058548\n",
      "epoch 15, batch 28, train_loss 3.29306411743\n",
      "epoch 15, batch 29, train_loss 3.18513870239\n",
      "epoch 15, batch 30, train_loss 3.27645897865\n",
      "epoch 15, batch 31, train_loss 2.7395966053\n",
      "epoch 15, batch 32, train_loss 2.56766605377\n",
      "epoch 15, batch 33, train_loss 4.24594020844\n",
      "epoch 15, batch 34, train_loss 4.32368803024\n",
      "epoch 15, batch 35, train_loss 3.08448076248\n",
      "epoch 15, batch 36, train_loss 3.09252619743\n",
      "epoch 15, batch 37, train_loss 3.04354095459\n",
      "epoch 15, batch 38, train_loss 3.18264603615\n",
      "epoch 15, batch 39, train_loss 3.66475319862\n",
      "epoch 15, batch 40, train_loss 2.76652598381\n",
      "epoch 15, batch 41, train_loss 3.06951522827\n",
      "epoch 15, batch 42, train_loss 3.05743169785\n",
      "epoch 15, batch 43, train_loss 2.81523346901\n",
      "epoch 15, batch 44, train_loss 3.47797298431\n",
      "epoch 15, batch 45, train_loss 3.10599207878\n",
      "epoch 15, batch 46, train_loss 3.07201814651\n",
      "epoch 15, batch 47, train_loss 3.12910985947\n",
      "epoch 15, batch 48, train_loss 2.95691132545\n",
      "epoch 15, batch 49, train_loss 3.08066320419\n",
      "epoch 15, batch 50, train_loss 3.01320099831\n",
      "epoch 15, batch 51, train_loss 2.66646790504\n",
      "epoch 15, batch 52, train_loss 3.09955215454\n",
      "epoch 15, batch 53, train_loss 2.9435968399\n",
      "epoch 15, batch 54, train_loss 2.75785636902\n",
      "epoch 15, batch 55, train_loss 2.80194759369\n",
      "epoch 15, batch 56, train_loss 2.68590426445\n",
      "epoch 15, batch 57, train_loss 3.11880803108\n",
      "epoch 15, batch 58, train_loss 2.94454646111\n",
      "epoch 15, batch 59, train_loss 2.89245939255\n",
      "epoch 15, batch 60, train_loss 3.08910131454\n",
      "epoch 15, batch 61, train_loss 1.78199648857\n",
      "epoch 15, batch 62, train_loss 2.68322706223\n",
      "epoch 15, batch 63, train_loss 2.83639717102\n",
      "epoch 15, batch 64, train_loss 3.0930519104\n",
      "epoch 15, batch 65, train_loss 3.39767622948\n",
      "epoch 15, batch 66, train_loss 3.15479707718\n",
      "epoch 15, batch 67, train_loss 2.03887438774\n",
      "epoch 15, batch 68, train_loss 2.84851861\n",
      "epoch 15, batch 69, train_loss 2.22990107536\n",
      "epoch 15, batch 70, train_loss 3.64272761345\n",
      "epoch 15, batch 71, train_loss 3.73358154297\n",
      "epoch 15, batch 72, train_loss 2.65997314453\n",
      "epoch 15, batch 73, train_loss 3.04956126213\n",
      "epoch 15, batch 74, train_loss 3.57324290276\n",
      "epoch 15, batch 75, train_loss 3.07638788223\n",
      "epoch 15, batch 76, train_loss 3.35014748573\n",
      "epoch 15, batch 77, train_loss 3.96736717224\n",
      "epoch 15, batch 78, train_loss 2.9375936985\n",
      "epoch 15, batch 79, train_loss 2.95294332504\n",
      "epoch 15, batch 80, train_loss 2.77646589279\n",
      "epoch 15, batch 81, train_loss 2.94401717186\n",
      "epoch 15, batch 82, train_loss 2.84723854065\n",
      "epoch 15, batch 83, train_loss 2.97063803673\n",
      "epoch 15, batch 84, train_loss 2.72748184204\n",
      "epoch 15, batch 85, train_loss 2.74132037163\n",
      "epoch 15, batch 86, train_loss 3.26326060295\n",
      "epoch 15, batch 87, train_loss 2.47616648674\n",
      "epoch 15, batch 88, train_loss 2.79079961777\n",
      "epoch 15, batch 89, train_loss 3.0789680481\n",
      "epoch 15, batch 90, train_loss 2.77786755562\n",
      "epoch 15, batch 91, train_loss 2.80444288254\n",
      "epoch 15, batch 92, train_loss 3.23561930656\n",
      "epoch 15, batch 93, train_loss 3.91753196716\n",
      "epoch 15, batch 94, train_loss 2.81514883041\n",
      "epoch 15, batch 95, train_loss 2.88025569916\n",
      "epoch 15, batch 96, train_loss 2.29398488998\n",
      "epoch 15, batch 97, train_loss 2.83891797066\n",
      "epoch 15, batch 98, train_loss 2.81742429733\n",
      "epoch 15, batch 99, train_loss 3.01873373985\n",
      "epoch 15, batch 100, train_loss 2.82457017899\n",
      "epoch 15, batch 101, train_loss 2.96588945389\n",
      "epoch 15, batch 102, train_loss 2.79212856293\n",
      "epoch 15, batch 103, train_loss 3.98263955116\n",
      "epoch 15, batch 104, train_loss 3.29368495941\n",
      "epoch 15, batch 105, train_loss 3.12037801743\n",
      "epoch 15, batch 106, train_loss 3.49198555946\n",
      "epoch 15, batch 107, train_loss 3.324832201\n",
      "epoch 15, batch 108, train_loss 2.96219873428\n",
      "epoch 15, batch 109, train_loss 3.236764431\n",
      "epoch 15, batch 110, train_loss 3.04320669174\n",
      "epoch 15, batch 111, train_loss 3.4426856041\n",
      "epoch 15, batch 112, train_loss 3.39005994797\n",
      "epoch 15, batch 113, train_loss 2.80310535431\n",
      "epoch 15, batch 114, train_loss 3.36388969421\n",
      "epoch 15, batch 115, train_loss 4.00504350662\n",
      "epoch 15, batch 116, train_loss 3.89365315437\n",
      "epoch 15, batch 117, train_loss 4.06211662292\n",
      "epoch 15, batch 118, train_loss 3.54136037827\n",
      "epoch 15, batch 119, train_loss 2.32722973824\n",
      "epoch 15, batch 120, train_loss 3.12765550613\n",
      "epoch 15, batch 121, train_loss 2.93757534027\n",
      "epoch 15, batch 122, train_loss 3.13110637665\n",
      "epoch 15, batch 123, train_loss 2.93293237686\n",
      "epoch 15, batch 124, train_loss 2.65016102791\n",
      "epoch 15, batch 125, train_loss 2.61400413513\n",
      "epoch 15, batch 126, train_loss 2.60276246071\n",
      "epoch 15, batch 127, train_loss 2.66237950325\n",
      "epoch 15, batch 128, train_loss 2.45289850235\n",
      "epoch 15, batch 129, train_loss 2.88835477829\n",
      "epoch 15, batch 130, train_loss 2.82051134109\n",
      "epoch 15, batch 131, train_loss 3.18079376221\n",
      "epoch 15, batch 132, train_loss 2.94271421432\n",
      "epoch 15, batch 133, train_loss 2.21987271309\n",
      "epoch 15, batch 134, train_loss 2.24215984344\n",
      "epoch 15, batch 135, train_loss 3.88312125206\n",
      "epoch 15, batch 136, train_loss 3.37071585655\n",
      "epoch 15, batch 137, train_loss 2.56137371063\n",
      "epoch 15, batch 138, train_loss 2.45090579987\n",
      "epoch 15, batch 139, train_loss 2.9291908741\n",
      "epoch 15, batch 140, train_loss 2.44782948494\n",
      "epoch 15, batch 141, train_loss 2.73383569717\n",
      "epoch 15, batch 142, train_loss 3.85181856155\n",
      "epoch 15, batch 143, train_loss 2.44451451302\n",
      "epoch 15, batch 144, train_loss 1.99855434895\n",
      "epoch 15, batch 145, train_loss 2.92428588867\n",
      "epoch 15, batch 146, train_loss 3.10141706467\n",
      "epoch 15, batch 147, train_loss 2.84864759445\n",
      "epoch 15, batch 148, train_loss 2.7529091835\n",
      "epoch 15, batch 149, train_loss 3.14660477638\n",
      "epoch 15, batch 150, train_loss 3.27959132195\n",
      "epoch 15, batch 151, train_loss 2.01371216774\n",
      "epoch 15, batch 152, train_loss 2.22693133354\n",
      "epoch 15, batch 153, train_loss 3.78219485283\n",
      "epoch 15, batch 154, train_loss 3.01973962784\n",
      "epoch 15, batch 155, train_loss 2.62433934212\n",
      "epoch 15, batch 156, train_loss 2.82202243805\n",
      "epoch 15, batch 157, train_loss 2.43729734421\n",
      "epoch 15, batch 158, train_loss 2.44898581505\n",
      "epoch 15, batch 159, train_loss 2.64925122261\n",
      "epoch 15, batch 160, train_loss 2.73114061356\n",
      "epoch 15, batch 161, train_loss 3.24487709999\n",
      "epoch 15, batch 162, train_loss 3.24562478065\n",
      "epoch 15, batch 163, train_loss 4.6191110611\n",
      "epoch 15, batch 164, train_loss 2.225461483\n",
      "epoch 15, batch 165, train_loss 2.19877338409\n",
      "epoch 15, batch 166, train_loss 2.46641373634\n",
      "epoch 15, batch 167, train_loss 2.48762083054\n",
      "epoch 15, batch 168, train_loss 2.67955446243\n",
      "epoch 15, batch 169, train_loss 2.51367712021\n",
      "epoch 15, batch 170, train_loss 2.98055195808\n",
      "epoch 15, batch 171, train_loss 2.43637013435\n",
      "epoch 15, batch 172, train_loss 3.00603961945\n",
      "epoch 15, batch 173, train_loss 2.02971053123\n",
      "epoch 15, batch 174, train_loss 3.45476412773\n",
      "epoch 15, batch 175, train_loss 2.97900390625\n",
      "epoch 15, batch 176, train_loss 2.88493871689\n",
      "epoch 15, batch 177, train_loss 2.69157886505\n",
      "epoch 15, batch 178, train_loss 2.62433862686\n",
      "epoch 15, batch 179, train_loss 2.89709854126\n",
      "epoch 15, batch 180, train_loss 2.75628423691\n",
      "epoch 15, batch 181, train_loss 3.34651660919\n",
      "epoch 15, batch 182, train_loss 2.8771238327\n",
      "epoch 15, batch 183, train_loss 3.4815132618\n",
      "epoch 15, batch 184, train_loss 2.2851600647\n",
      "epoch 15, batch 185, train_loss 2.63128042221\n",
      "epoch 15, batch 186, train_loss 3.18550181389\n",
      "epoch 15, batch 187, train_loss 2.31469917297\n",
      "epoch 15, batch 188, train_loss 2.65799617767\n",
      "epoch 15, batch 189, train_loss 2.63939428329\n",
      "epoch 15, batch 190, train_loss 3.36247754097\n",
      "epoch 15, batch 191, train_loss 3.21296977997\n",
      "epoch 15, batch 192, train_loss 4.1034116745\n",
      "epoch 15, batch 193, train_loss 3.83771014214\n",
      "epoch 15, batch 194, train_loss 4.41836547852\n",
      "epoch 15, batch 195, train_loss 4.49977779388\n",
      "epoch 15, batch 196, train_loss 3.01958799362\n",
      "epoch 15, batch 197, train_loss 2.41989445686\n",
      "epoch 15, batch 198, train_loss 2.2272207737\n",
      "epoch 15, batch 199, train_loss 3.14583063126\n",
      "epoch 15, batch 200, train_loss 2.6820526123\n",
      "epoch 15, batch 201, train_loss 2.94213199615\n",
      "epoch 15, batch 202, train_loss 3.37895035744\n",
      "epoch 15, batch 203, train_loss 3.5197250843\n",
      "epoch 15, batch 204, train_loss 3.51534295082\n",
      "epoch 15, batch 205, train_loss 2.95485019684\n",
      "epoch 15, batch 206, train_loss 3.93512248993\n",
      "epoch 15, batch 207, train_loss 3.33210229874\n",
      "epoch 15, batch 208, train_loss 4.00249433517\n",
      "epoch 15, batch 209, train_loss 3.73718523979\n",
      "epoch 15, batch 210, train_loss 2.0514793396\n",
      "epoch 15, batch 211, train_loss 2.76393866539\n",
      "epoch 15, batch 212, train_loss 3.6539452076\n",
      "epoch 15, batch 213, train_loss 2.99853587151\n",
      "epoch 15, batch 214, train_loss 2.8802087307\n",
      "epoch 15, batch 215, train_loss 2.13634443283\n",
      "epoch 15, batch 216, train_loss 3.23131036758\n",
      "epoch 15, batch 217, train_loss 2.71240210533\n",
      "epoch 15, batch 218, train_loss 3.13772344589\n",
      "epoch 15, batch 219, train_loss 3.23269820213\n",
      "epoch 15, batch 220, train_loss 3.2868540287\n",
      "epoch 15, batch 221, train_loss 3.39489722252\n",
      "epoch 15, batch 222, train_loss 2.21063303947\n",
      "epoch 15, batch 223, train_loss 2.20807552338\n",
      "epoch 15, batch 224, train_loss 2.55351638794\n",
      "epoch 15, batch 225, train_loss 2.67476868629\n",
      "epoch 15, batch 226, train_loss 3.15901112556\n",
      "epoch 15, batch 227, train_loss 2.93654227257\n",
      "epoch 15, batch 228, train_loss 3.08390212059\n",
      "epoch 15, batch 229, train_loss 2.90257358551\n",
      "epoch 15, batch 230, train_loss 2.78578782082\n",
      "epoch 15, batch 231, train_loss 2.498711586\n",
      "epoch 15, batch 232, train_loss 3.12373614311\n",
      "epoch 15, batch 233, train_loss 3.36282062531\n",
      "epoch 15, batch 234, train_loss 2.95882153511\n",
      "epoch 15, batch 235, train_loss 3.48426151276\n",
      "epoch 15, batch 236, train_loss 3.05933213234\n",
      "epoch 15, batch 237, train_loss 2.94320273399\n",
      "epoch 15, batch 238, train_loss 2.98713707924\n",
      "epoch 15, batch 239, train_loss 2.8164126873\n",
      "epoch 15, batch 240, train_loss 3.23564124107\n",
      "epoch 15, batch 241, train_loss 3.16703438759\n",
      "epoch 15, batch 242, train_loss 2.98088002205\n",
      "epoch 15, batch 243, train_loss 3.17774295807\n",
      "epoch 15, batch 244, train_loss 3.31361865997\n",
      "epoch 15, batch 245, train_loss 3.08598279953\n",
      "epoch 15, batch 246, train_loss 2.99656105042\n",
      "epoch 15, batch 247, train_loss 3.18020892143\n",
      "epoch 15, batch 248, train_loss 2.92894363403\n",
      "epoch 15, batch 249, train_loss 3.58232164383\n",
      "epoch 15, batch 250, train_loss 3.33305764198\n",
      "epoch 15, batch 251, train_loss 3.68434429169\n",
      "epoch 15, batch 252, train_loss 2.56018304825\n",
      "epoch 15, batch 253, train_loss 2.10001516342\n",
      "epoch 15, batch 254, train_loss 2.68897390366\n",
      "epoch 15, batch 255, train_loss 2.09552073479\n",
      "epoch 15, batch 256, train_loss 2.46739125252\n",
      "epoch 15, batch 257, train_loss 2.8948302269\n",
      "epoch 15, batch 258, train_loss 2.30767130852\n",
      "epoch 15, batch 259, train_loss 2.25505113602\n",
      "epoch 15, batch 260, train_loss 2.03159308434\n",
      "epoch 15, batch 261, train_loss 3.29863095284\n",
      "epoch 15, batch 262, train_loss 3.87866044044\n",
      "epoch 15, batch 263, train_loss 3.03887057304\n",
      "epoch 15, batch 264, train_loss 2.75141048431\n",
      "epoch 15, batch 265, train_loss 3.33513617516\n",
      "epoch 15, batch 266, train_loss 3.37533354759\n",
      "epoch 15, batch 267, train_loss 2.67617869377\n",
      "epoch 15, batch 268, train_loss 2.21858167648\n",
      "epoch 15, batch 269, train_loss 2.30639195442\n",
      "epoch 15, batch 270, train_loss 2.59725356102\n",
      "epoch 15, batch 271, train_loss 2.91522789001\n",
      "epoch 15, batch 272, train_loss 2.93784070015\n",
      "epoch 15, batch 273, train_loss 3.04552674294\n",
      "epoch 15, batch 274, train_loss 2.86003994942\n",
      "epoch 15, batch 275, train_loss 2.88873505592\n",
      "epoch 15, batch 276, train_loss 2.69001173973\n",
      "epoch 15, batch 277, train_loss 3.48179030418\n",
      "epoch 15, batch 278, train_loss 2.6257185936\n",
      "epoch 15, batch 279, train_loss 3.13200640678\n",
      "epoch 15, batch 280, train_loss 3.10751223564\n",
      "epoch 15, batch 281, train_loss 2.95720434189\n",
      "epoch 15, batch 282, train_loss 2.59366488457\n",
      "epoch 15, batch 283, train_loss 4.0600605011\n",
      "epoch 15, batch 284, train_loss 3.05722045898\n",
      "epoch 15, batch 285, train_loss 1.89525830746\n",
      "epoch 15, batch 286, train_loss 2.13825702667\n",
      "epoch 15, batch 287, train_loss 2.23312091827\n",
      "epoch 15, batch 288, train_loss 2.86671566963\n",
      "epoch 15, batch 289, train_loss 2.90384936333\n",
      "epoch 15, batch 290, train_loss 2.72374486923\n",
      "epoch 15, batch 291, train_loss 2.61507034302\n",
      "epoch 15, batch 292, train_loss 2.16691398621\n",
      "epoch 15, batch 293, train_loss 3.1669356823\n",
      "epoch 15, batch 294, train_loss 2.78300309181\n",
      "epoch 15, batch 295, train_loss 2.52096533775\n",
      "epoch 15, batch 296, train_loss 2.79493117332\n",
      "epoch 15, batch 297, train_loss 2.47840547562\n",
      "epoch 15, batch 298, train_loss 2.6922109127\n",
      "epoch 15, batch 299, train_loss 3.15635561943\n",
      "epoch 15, batch 300, train_loss 2.93668866158\n",
      "epoch 15, batch 301, train_loss 3.82269358635\n",
      "epoch 15, batch 302, train_loss 3.84181451797\n",
      "epoch 15, batch 303, train_loss 3.55928969383\n",
      "epoch 15, batch 304, train_loss 4.15406084061\n",
      "epoch 15, batch 305, train_loss 4.14074134827\n",
      "epoch 15, batch 306, train_loss 2.85474586487\n",
      "epoch 15, batch 307, train_loss 2.4072508812\n",
      "epoch 15, batch 308, train_loss 2.8293390274\n",
      "epoch 15, batch 309, train_loss 2.98011660576\n",
      "epoch 15, batch 310, train_loss 2.8201379776\n",
      "epoch 15, batch 311, train_loss 2.9065759182\n",
      "epoch 15, batch 312, train_loss 3.07768344879\n",
      "epoch 15, batch 313, train_loss 2.80517554283\n",
      "epoch 15, batch 314, train_loss 3.11740398407\n",
      "epoch 15, batch 315, train_loss 2.63569760323\n",
      "epoch 15, batch 316, train_loss 2.79346752167\n",
      "epoch 15, batch 317, train_loss 3.24123287201\n",
      "epoch 15, batch 318, train_loss 2.40131354332\n",
      "epoch 15, batch 319, train_loss 4.00492954254\n",
      "epoch 15, batch 320, train_loss 2.94046521187\n",
      "epoch 15, batch 321, train_loss 4.10289716721\n",
      "epoch 15, batch 322, train_loss 2.30305528641\n",
      "epoch 15, batch 323, train_loss 2.18067407608\n",
      "epoch 15, batch 324, train_loss 3.24028372765\n",
      "epoch 15, batch 325, train_loss 2.95389342308\n",
      "epoch 15, batch 326, train_loss 2.93456554413\n",
      "epoch 15, batch 327, train_loss 2.90736889839\n",
      "epoch 15, batch 328, train_loss 2.77823448181\n",
      "epoch 15, batch 329, train_loss 2.47829627991\n",
      "epoch 15, batch 330, train_loss 3.76291513443\n",
      "epoch 15, batch 331, train_loss 3.47113704681\n",
      "epoch 15, batch 332, train_loss 2.34417104721\n",
      "epoch 15, batch 333, train_loss 3.33280777931\n",
      "epoch 15, batch 334, train_loss 3.04926776886\n",
      "epoch 15, batch 335, train_loss 2.96336531639\n",
      "epoch 15, batch 336, train_loss 3.47563958168\n",
      "epoch 15, batch 337, train_loss 2.14514446259\n",
      "epoch 15, batch 338, train_loss 2.93707537651\n",
      "epoch 15, batch 339, train_loss 2.739320755\n",
      "epoch 15, batch 340, train_loss 2.92925786972\n",
      "epoch 15, batch 341, train_loss 2.88334417343\n",
      "epoch 15, batch 342, train_loss 2.90541815758\n",
      "epoch 15, batch 343, train_loss 2.80986881256\n",
      "epoch 15, batch 344, train_loss 3.6631705761\n",
      "epoch 15, batch 345, train_loss 3.11795973778\n",
      "epoch 15, batch 346, train_loss 2.90575098991\n",
      "epoch 15, batch 347, train_loss 2.72069859505\n",
      "epoch 15, batch 348, train_loss 2.97425746918\n",
      "epoch 15, batch 349, train_loss 3.92127943039\n",
      "epoch 15, batch 350, train_loss 3.8894135952\n",
      "epoch 15, batch 351, train_loss 3.71543002129\n",
      "epoch 15, batch 352, train_loss 3.75589132309\n",
      "epoch 15, batch 353, train_loss 3.06161499023\n",
      "epoch 15, batch 354, train_loss 2.46056008339\n",
      "epoch 15, batch 355, train_loss 2.78547763824\n",
      "epoch 15, batch 356, train_loss 2.73874688148\n",
      "epoch 15, batch 357, train_loss 2.67135286331\n",
      "epoch 15, batch 358, train_loss 2.26995778084\n",
      "epoch 15, batch 359, train_loss 3.18467569351\n",
      "epoch 15, batch 360, train_loss 2.70704269409\n",
      "epoch 15, batch 361, train_loss 2.40729260445\n",
      "epoch 15, batch 362, train_loss 4.03793048859\n",
      "epoch 15, batch 363, train_loss 2.99358940125\n",
      "epoch 15, batch 364, train_loss 2.89223480225\n",
      "epoch 15, batch 365, train_loss 3.68277835846\n",
      "epoch 15, batch 366, train_loss 3.84543681145\n",
      "epoch 15, batch 367, train_loss 4.55816698074\n",
      "epoch 15, batch 368, train_loss 2.38348507881\n",
      "epoch 15, batch 369, train_loss 3.25900554657\n",
      "epoch 15, batch 370, train_loss 3.64409708977\n",
      "epoch 15, batch 371, train_loss 4.53157997131\n",
      "epoch 15, batch 372, train_loss 2.18658161163\n",
      "epoch 15, batch 373, train_loss 2.34090304375\n",
      "epoch 15, batch 374, train_loss 2.67957043648\n",
      "epoch 15, batch 375, train_loss 3.04246783257\n",
      "epoch 15, batch 376, train_loss 2.3485159874\n",
      "epoch 15, batch 377, train_loss 2.72367787361\n",
      "epoch 15, batch 378, train_loss 3.37745714188\n",
      "epoch 15, batch 379, train_loss 2.59852218628\n",
      "epoch 15, batch 380, train_loss 2.62094211578\n",
      "epoch 15, batch 381, train_loss 2.72414779663\n",
      "epoch 15, batch 382, train_loss 3.51352643967\n",
      "epoch 15, batch 383, train_loss 3.13452315331\n",
      "epoch 15, batch 384, train_loss 2.82558679581\n",
      "epoch 15, batch 385, train_loss 4.1382317543\n",
      "epoch 15, batch 386, train_loss 2.52000951767\n",
      "epoch 15, batch 387, train_loss 4.16194343567\n",
      "epoch 15, batch 388, train_loss 2.52511358261\n",
      "epoch 15, batch 389, train_loss 3.98531913757\n",
      "epoch 15, batch 390, train_loss 4.00762796402\n",
      "epoch 15, batch 391, train_loss 4.05351543427\n",
      "epoch 15, batch 392, train_loss 3.83402276039\n",
      "epoch 15, batch 393, train_loss 3.08982515335\n",
      "epoch 15, batch 394, train_loss 3.69272708893\n",
      "epoch 15, batch 395, train_loss 2.70596313477\n",
      "epoch 15, batch 396, train_loss 3.89580273628\n",
      "epoch 15, batch 397, train_loss 3.98714566231\n",
      "epoch 15, batch 398, train_loss 3.01748275757\n",
      "epoch 15, batch 399, train_loss 3.21940469742\n",
      "epoch 15, batch 400, train_loss 4.11424875259\n",
      "epoch 15, batch 401, train_loss 3.42421174049\n",
      "epoch 15, batch 402, train_loss 3.03685665131\n",
      "epoch 15, batch 403, train_loss 2.97723150253\n",
      "epoch 15, batch 404, train_loss 3.76645994186\n",
      "epoch 15, batch 405, train_loss 3.29956436157\n",
      "epoch 15, batch 406, train_loss 3.1706905365\n",
      "epoch 15, batch 407, train_loss 2.9819085598\n",
      "epoch 15, batch 408, train_loss 2.82912063599\n",
      "epoch 15, batch 409, train_loss 2.37918305397\n",
      "epoch 15, batch 410, train_loss 3.9290997982\n",
      "epoch 15, batch 411, train_loss 3.05513691902\n",
      "epoch 15, batch 412, train_loss 2.59497952461\n",
      "epoch 15, batch 413, train_loss 2.91532945633\n",
      "epoch 15, batch 414, train_loss 3.39142918587\n",
      "epoch 15, batch 415, train_loss 3.47954034805\n",
      "epoch 15, batch 416, train_loss 3.23299717903\n",
      "epoch 15, batch 417, train_loss 2.81158065796\n",
      "epoch 15, batch 418, train_loss 3.19086027145\n",
      "epoch 15, batch 419, train_loss 2.85441112518\n",
      "epoch 15, batch 420, train_loss 2.32710289955\n",
      "epoch 15, batch 421, train_loss 4.02363395691\n",
      "epoch 15, batch 422, train_loss 2.820333004\n",
      "epoch 15, batch 423, train_loss 2.93404316902\n",
      "epoch 15, batch 424, train_loss 3.9596350193\n",
      "epoch 15, batch 425, train_loss 3.32625579834\n",
      "epoch 15, batch 426, train_loss 4.41325616837\n",
      "epoch 15, batch 427, train_loss 3.59115433693\n",
      "epoch 15, batch 428, train_loss 2.45027685165\n",
      "epoch 15, batch 429, train_loss 3.39924955368\n",
      "epoch 15, batch 430, train_loss 3.42355489731\n",
      "epoch 15, batch 431, train_loss 3.07187128067\n",
      "epoch 15, batch 432, train_loss 3.25281786919\n",
      "epoch 15, batch 433, train_loss 2.70386123657\n",
      "epoch 15, batch 434, train_loss 3.19413805008\n",
      "epoch 15, batch 435, train_loss 3.02773809433\n",
      "epoch 15, batch 436, train_loss 3.34761238098\n",
      "epoch 15, batch 437, train_loss 3.76842808723\n",
      "epoch 15, batch 438, train_loss 3.47073197365\n",
      "epoch 15, batch 439, train_loss 3.21393013\n",
      "epoch 15, batch 440, train_loss 4.40309381485\n",
      "epoch 15, batch 441, train_loss 4.53164863586\n",
      "epoch 15, batch 442, train_loss 3.75746273994\n",
      "epoch 15, batch 443, train_loss 2.30944442749\n",
      "epoch 15, batch 444, train_loss 2.81395411491\n",
      "epoch 15, batch 445, train_loss 4.32792520523\n",
      "epoch 15, batch 446, train_loss 3.16027903557\n",
      "epoch 15, batch 447, train_loss 2.87078356743\n",
      "epoch 15, batch 448, train_loss 4.19716835022\n",
      "epoch 15, batch 449, train_loss 3.63353490829\n",
      "epoch 15, batch 450, train_loss 2.97568750381\n",
      "epoch 15, batch 451, train_loss 2.72269964218\n",
      "epoch 15, batch 452, train_loss 2.50198340416\n",
      "epoch 15, batch 453, train_loss 4.73369646072\n",
      "epoch 15, batch 454, train_loss 4.56126022339\n",
      "epoch 15, batch 455, train_loss 2.38801193237\n",
      "epoch 15, batch 456, train_loss 3.03456783295\n",
      "epoch 15, batch 457, train_loss 3.1952483654\n",
      "epoch 15, batch 458, train_loss 2.91084527969\n",
      "epoch 15, batch 459, train_loss 3.15908002853\n",
      "epoch 15, batch 460, train_loss 3.62232398987\n",
      "epoch 15, batch 461, train_loss 3.14379954338\n",
      "epoch 15, batch 462, train_loss 2.85558176041\n",
      "epoch 15, batch 463, train_loss 2.99410104752\n",
      "epoch 15, batch 464, train_loss 3.26415944099\n",
      "epoch 15, batch 465, train_loss 2.69741845131\n",
      "epoch 15, batch 466, train_loss 3.41868591309\n",
      "epoch 15, batch 467, train_loss 3.25160002708\n",
      "epoch 15, batch 468, train_loss 3.39019799232\n",
      "epoch 15, batch 469, train_loss 2.96281671524\n",
      "epoch 15, batch 470, train_loss 2.82883739471\n",
      "epoch 15, batch 471, train_loss 2.92182135582\n",
      "epoch 15, batch 472, train_loss 2.78658413887\n",
      "epoch 15, batch 473, train_loss 3.45761370659\n",
      "epoch 15, batch 474, train_loss 3.99046516418\n",
      "epoch 15, batch 475, train_loss 2.82880973816\n",
      "epoch 15, batch 476, train_loss 2.36216211319\n",
      "epoch 15, batch 477, train_loss 2.42010474205\n",
      "epoch 15, batch 478, train_loss 2.26948928833\n",
      "epoch 15, batch 479, train_loss 2.7854244709\n",
      "epoch 15, batch 480, train_loss 2.92704176903\n",
      "epoch 15, batch 481, train_loss 4.40665388107\n",
      "epoch 15, batch 482, train_loss 2.6747739315\n",
      "epoch 15, batch 483, train_loss 3.24690318108\n",
      "epoch 15, batch 484, train_loss 2.9495074749\n",
      "epoch 15, batch 485, train_loss 2.22090792656\n",
      "epoch 15, batch 486, train_loss 2.22144556046\n",
      "epoch 15, batch 487, train_loss 2.38799214363\n",
      "epoch 15, batch 488, train_loss 2.78732323647\n",
      "epoch 15, batch 489, train_loss 2.90982627869\n",
      "epoch 15, batch 490, train_loss 2.80867147446\n",
      "epoch 15, batch 491, train_loss 2.75221014023\n",
      "epoch 15, batch 492, train_loss 3.08374238014\n",
      "epoch 15, batch 493, train_loss 2.71613049507\n",
      "epoch 15, batch 494, train_loss 2.79963636398\n",
      "epoch 15, batch 495, train_loss 2.87264418602\n",
      "epoch 15, batch 496, train_loss 2.51466655731\n",
      "epoch 15, batch 497, train_loss 2.68024706841\n",
      "epoch 15, batch 498, train_loss 2.80180501938\n",
      "epoch 15, batch 499, train_loss 2.75943756104\n",
      "epoch 15, batch 500, train_loss 3.14372181892\n",
      "epoch 15, batch 501, train_loss 2.84869265556\n",
      "epoch 15, batch 502, train_loss 4.026055336\n",
      "epoch 15, batch 503, train_loss 3.93994617462\n",
      "epoch 15, batch 504, train_loss 3.45984888077\n",
      "epoch 15, batch 505, train_loss 3.31305837631\n",
      "epoch 15, batch 506, train_loss 2.98938798904\n",
      "epoch 15, batch 507, train_loss 3.9448363781\n",
      "epoch 15, batch 508, train_loss 2.85989022255\n",
      "epoch 15, batch 509, train_loss 4.06896352768\n",
      "epoch 15, batch 510, train_loss 4.03232955933\n",
      "epoch 15, batch 511, train_loss 4.06166172028\n",
      "epoch 15, batch 512, train_loss 3.18995499611\n",
      "epoch 15, batch 513, train_loss 4.4764957428\n",
      "epoch 15, batch 514, train_loss 4.40917015076\n",
      "epoch 15, batch 515, train_loss 4.32742929459\n",
      "epoch 15, batch 516, train_loss 2.67564225197\n",
      "epoch 15, batch 517, train_loss 2.88068270683\n",
      "epoch 15, batch 518, train_loss 2.79160308838\n",
      "epoch 15, batch 519, train_loss 2.4894862175\n",
      "epoch 15, batch 520, train_loss 3.53945040703\n",
      "epoch 15, batch 521, train_loss 2.78775787354\n",
      "epoch 15, batch 522, train_loss 2.44598317146\n",
      "epoch 15, batch 523, train_loss 2.44026184082\n",
      "epoch 15, batch 524, train_loss 2.29130482674\n",
      "epoch 15, batch 525, train_loss 2.12785887718\n",
      "epoch 15, batch 526, train_loss 2.21691036224\n",
      "epoch 15, batch 527, train_loss 2.22411108017\n",
      "epoch 15, batch 528, train_loss 2.44981241226\n",
      "epoch 15, batch 529, train_loss 2.34447574615\n",
      "epoch 15, batch 530, train_loss 1.86660468578\n",
      "epoch 15, batch 531, train_loss 1.68070793152\n",
      "epoch 15, batch 532, train_loss 2.34932851791\n",
      "epoch 15, batch 533, train_loss 1.53698217869\n",
      "epoch 15, batch 534, train_loss 1.63152492046\n",
      "epoch 15, batch 535, train_loss 2.94052171707\n",
      "epoch 15, batch 536, train_loss 3.16807627678\n",
      "epoch 15, batch 537, train_loss 3.05141830444\n",
      "epoch 15, batch 538, train_loss 3.15542984009\n",
      "epoch 15, batch 539, train_loss 3.39584422112\n",
      "epoch 15, batch 540, train_loss 3.71137356758\n",
      "epoch 16, batch 0, train_loss 3.20480799675\n",
      "epoch 16, batch 1, train_loss 3.49569821358\n",
      "epoch 16, batch 2, train_loss 2.79526090622\n",
      "epoch 16, batch 3, train_loss 3.1106133461\n",
      "epoch 16, batch 4, train_loss 2.73240494728\n",
      "epoch 16, batch 5, train_loss 2.80208778381\n",
      "epoch 16, batch 6, train_loss 3.59458947182\n",
      "epoch 16, batch 7, train_loss 2.78453540802\n",
      "epoch 16, batch 8, train_loss 2.48044157028\n",
      "epoch 16, batch 9, train_loss 3.33702611923\n",
      "epoch 16, batch 10, train_loss 2.73099374771\n",
      "epoch 16, batch 11, train_loss 2.80020666122\n",
      "epoch 16, batch 12, train_loss 2.5589530468\n",
      "epoch 16, batch 13, train_loss 3.00749349594\n",
      "epoch 16, batch 14, train_loss 2.4782242775\n",
      "epoch 16, batch 15, train_loss 3.16249918938\n",
      "epoch 16, batch 16, train_loss 2.71079826355\n",
      "epoch 16, batch 17, train_loss 2.60314154625\n",
      "epoch 16, batch 18, train_loss 2.31568264961\n",
      "epoch 16, batch 19, train_loss 1.91319561005\n",
      "epoch 16, batch 20, train_loss 2.66658425331\n",
      "epoch 16, batch 21, train_loss 2.58701276779\n",
      "epoch 16, batch 22, train_loss 3.06699943542\n",
      "epoch 16, batch 23, train_loss 2.71696352959\n",
      "epoch 16, batch 24, train_loss 2.73927235603\n",
      "epoch 16, batch 25, train_loss 3.13806533813\n",
      "epoch 16, batch 26, train_loss 3.05256009102\n",
      "epoch 16, batch 27, train_loss 2.98024773598\n",
      "epoch 16, batch 28, train_loss 3.26988720894\n",
      "epoch 16, batch 29, train_loss 3.16345191002\n",
      "epoch 16, batch 30, train_loss 3.24833250046\n",
      "epoch 16, batch 31, train_loss 2.72114109993\n",
      "epoch 16, batch 32, train_loss 2.55176138878\n",
      "epoch 16, batch 33, train_loss 4.21138763428\n",
      "epoch 16, batch 34, train_loss 4.28803062439\n",
      "epoch 16, batch 35, train_loss 3.06296443939\n",
      "epoch 16, batch 36, train_loss 3.06995725632\n",
      "epoch 16, batch 37, train_loss 3.01933026314\n",
      "epoch 16, batch 38, train_loss 3.1602935791\n",
      "epoch 16, batch 39, train_loss 3.6453742981\n",
      "epoch 16, batch 40, train_loss 2.74558973312\n",
      "epoch 16, batch 41, train_loss 3.04639840126\n",
      "epoch 16, batch 42, train_loss 3.03497076035\n",
      "epoch 16, batch 43, train_loss 2.7904715538\n",
      "epoch 16, batch 44, train_loss 3.45408964157\n",
      "epoch 16, batch 45, train_loss 3.08809280396\n",
      "epoch 16, batch 46, train_loss 3.05321550369\n",
      "epoch 16, batch 47, train_loss 3.10820889473\n",
      "epoch 16, batch 48, train_loss 2.93208527565\n",
      "epoch 16, batch 49, train_loss 3.05837321281\n",
      "epoch 16, batch 50, train_loss 2.99288368225\n",
      "epoch 16, batch 51, train_loss 2.64460015297\n",
      "epoch 16, batch 52, train_loss 3.07475805283\n",
      "epoch 16, batch 53, train_loss 2.91935753822\n",
      "epoch 16, batch 54, train_loss 2.74051785469\n",
      "epoch 16, batch 55, train_loss 2.78368830681\n",
      "epoch 16, batch 56, train_loss 2.66732931137\n",
      "epoch 16, batch 57, train_loss 3.09513282776\n",
      "epoch 16, batch 58, train_loss 2.92297959328\n",
      "epoch 16, batch 59, train_loss 2.8708024025\n",
      "epoch 16, batch 60, train_loss 3.06647205353\n",
      "epoch 16, batch 61, train_loss 1.76885652542\n",
      "epoch 16, batch 62, train_loss 2.66422200203\n",
      "epoch 16, batch 63, train_loss 2.81593132019\n",
      "epoch 16, batch 64, train_loss 3.07378149033\n",
      "epoch 16, batch 65, train_loss 3.37786984444\n",
      "epoch 16, batch 66, train_loss 3.13324594498\n",
      "epoch 16, batch 67, train_loss 2.02818918228\n",
      "epoch 16, batch 68, train_loss 2.82899069786\n",
      "epoch 16, batch 69, train_loss 2.21561336517\n",
      "epoch 16, batch 70, train_loss 3.61448264122\n",
      "epoch 16, batch 71, train_loss 3.70508813858\n",
      "epoch 16, batch 72, train_loss 2.64304590225\n",
      "epoch 16, batch 73, train_loss 3.02979803085\n",
      "epoch 16, batch 74, train_loss 3.54788303375\n",
      "epoch 16, batch 75, train_loss 3.05351185799\n",
      "epoch 16, batch 76, train_loss 3.32489681244\n",
      "epoch 16, batch 77, train_loss 3.93504214287\n",
      "epoch 16, batch 78, train_loss 2.91821575165\n",
      "epoch 16, batch 79, train_loss 2.93237304688\n",
      "epoch 16, batch 80, train_loss 2.75534820557\n",
      "epoch 16, batch 81, train_loss 2.92783403397\n",
      "epoch 16, batch 82, train_loss 2.83039355278\n",
      "epoch 16, batch 83, train_loss 2.95391321182\n",
      "epoch 16, batch 84, train_loss 2.71084928513\n",
      "epoch 16, batch 85, train_loss 2.72395038605\n",
      "epoch 16, batch 86, train_loss 3.24317550659\n",
      "epoch 16, batch 87, train_loss 2.46001291275\n",
      "epoch 16, batch 88, train_loss 2.77473855019\n",
      "epoch 16, batch 89, train_loss 3.06173324585\n",
      "epoch 16, batch 90, train_loss 2.75978064537\n",
      "epoch 16, batch 91, train_loss 2.78486132622\n",
      "epoch 16, batch 92, train_loss 3.21453285217\n",
      "epoch 16, batch 93, train_loss 3.88439226151\n",
      "epoch 16, batch 94, train_loss 2.79924464226\n",
      "epoch 16, batch 95, train_loss 2.86399388313\n",
      "epoch 16, batch 96, train_loss 2.2811794281\n",
      "epoch 16, batch 97, train_loss 2.81658554077\n",
      "epoch 16, batch 98, train_loss 2.80128550529\n",
      "epoch 16, batch 99, train_loss 2.99632191658\n",
      "epoch 16, batch 100, train_loss 2.80997824669\n",
      "epoch 16, batch 101, train_loss 2.94466352463\n",
      "epoch 16, batch 102, train_loss 2.77367401123\n",
      "epoch 16, batch 103, train_loss 3.9530146122\n",
      "epoch 16, batch 104, train_loss 3.27241563797\n",
      "epoch 16, batch 105, train_loss 3.10154104233\n",
      "epoch 16, batch 106, train_loss 3.47127175331\n",
      "epoch 16, batch 107, train_loss 3.30458950996\n",
      "epoch 16, batch 108, train_loss 2.94538784027\n",
      "epoch 16, batch 109, train_loss 3.21798372269\n",
      "epoch 16, batch 110, train_loss 3.02613329887\n",
      "epoch 16, batch 111, train_loss 3.42082262039\n",
      "epoch 16, batch 112, train_loss 3.37099075317\n",
      "epoch 16, batch 113, train_loss 2.78340506554\n",
      "epoch 16, batch 114, train_loss 3.34462165833\n",
      "epoch 16, batch 115, train_loss 3.9724984169\n",
      "epoch 16, batch 116, train_loss 3.86492776871\n",
      "epoch 16, batch 117, train_loss 4.03460216522\n",
      "epoch 16, batch 118, train_loss 3.51885962486\n",
      "epoch 16, batch 119, train_loss 2.30863881111\n",
      "epoch 16, batch 120, train_loss 3.10750293732\n",
      "epoch 16, batch 121, train_loss 2.91974020004\n",
      "epoch 16, batch 122, train_loss 3.11001348495\n",
      "epoch 16, batch 123, train_loss 2.91346430779\n",
      "epoch 16, batch 124, train_loss 2.6344666481\n",
      "epoch 16, batch 125, train_loss 2.59563755989\n",
      "epoch 16, batch 126, train_loss 2.58732652664\n",
      "epoch 16, batch 127, train_loss 2.64565324783\n",
      "epoch 16, batch 128, train_loss 2.42903542519\n",
      "epoch 16, batch 129, train_loss 2.87078332901\n",
      "epoch 16, batch 130, train_loss 2.79946279526\n",
      "epoch 16, batch 131, train_loss 3.15941262245\n",
      "epoch 16, batch 132, train_loss 2.92211437225\n",
      "epoch 16, batch 133, train_loss 2.20588350296\n",
      "epoch 16, batch 134, train_loss 2.22604322433\n",
      "epoch 16, batch 135, train_loss 3.85389280319\n",
      "epoch 16, batch 136, train_loss 3.34824538231\n",
      "epoch 16, batch 137, train_loss 2.54321360588\n",
      "epoch 16, batch 138, train_loss 2.43577528\n",
      "epoch 16, batch 139, train_loss 2.90962290764\n",
      "epoch 16, batch 140, train_loss 2.43236398697\n",
      "epoch 16, batch 141, train_loss 2.71484804153\n",
      "epoch 16, batch 142, train_loss 3.81988072395\n",
      "epoch 16, batch 143, train_loss 2.42802023888\n",
      "epoch 16, batch 144, train_loss 1.98676502705\n",
      "epoch 16, batch 145, train_loss 2.90509271622\n",
      "epoch 16, batch 146, train_loss 3.08037543297\n",
      "epoch 16, batch 147, train_loss 2.83061480522\n",
      "epoch 16, batch 148, train_loss 2.73510885239\n",
      "epoch 16, batch 149, train_loss 3.12477779388\n",
      "epoch 16, batch 150, train_loss 3.25813698769\n",
      "epoch 16, batch 151, train_loss 2.00199866295\n",
      "epoch 16, batch 152, train_loss 2.2109401226\n",
      "epoch 16, batch 153, train_loss 3.75473451614\n",
      "epoch 16, batch 154, train_loss 3.00060939789\n",
      "epoch 16, batch 155, train_loss 2.6066505909\n",
      "epoch 16, batch 156, train_loss 2.80355215073\n",
      "epoch 16, batch 157, train_loss 2.42283701897\n",
      "epoch 16, batch 158, train_loss 2.43479681015\n",
      "epoch 16, batch 159, train_loss 2.63341331482\n",
      "epoch 16, batch 160, train_loss 2.71460962296\n",
      "epoch 16, batch 161, train_loss 3.22362375259\n",
      "epoch 16, batch 162, train_loss 3.2279548645\n",
      "epoch 16, batch 163, train_loss 4.59009361267\n",
      "epoch 16, batch 164, train_loss 2.21097254753\n",
      "epoch 16, batch 165, train_loss 2.18716716766\n",
      "epoch 16, batch 166, train_loss 2.45200872421\n",
      "epoch 16, batch 167, train_loss 2.47101807594\n",
      "epoch 16, batch 168, train_loss 2.65766453743\n",
      "epoch 16, batch 169, train_loss 2.49850010872\n",
      "epoch 16, batch 170, train_loss 2.95658540726\n",
      "epoch 16, batch 171, train_loss 2.41959357262\n",
      "epoch 16, batch 172, train_loss 2.98364567757\n",
      "epoch 16, batch 173, train_loss 2.01760935783\n",
      "epoch 16, batch 174, train_loss 3.42746019363\n",
      "epoch 16, batch 175, train_loss 2.95640206337\n",
      "epoch 16, batch 176, train_loss 2.86488652229\n",
      "epoch 16, batch 177, train_loss 2.67308163643\n",
      "epoch 16, batch 178, train_loss 2.60496878624\n",
      "epoch 16, batch 179, train_loss 2.88014745712\n",
      "epoch 16, batch 180, train_loss 2.73798465729\n",
      "epoch 16, batch 181, train_loss 3.3247461319\n",
      "epoch 16, batch 182, train_loss 2.85761642456\n",
      "epoch 16, batch 183, train_loss 3.45526123047\n",
      "epoch 16, batch 184, train_loss 2.27141475677\n",
      "epoch 16, batch 185, train_loss 2.61526274681\n",
      "epoch 16, batch 186, train_loss 3.16305494308\n",
      "epoch 16, batch 187, train_loss 2.29620528221\n",
      "epoch 16, batch 188, train_loss 2.63981175423\n",
      "epoch 16, batch 189, train_loss 2.62303853035\n",
      "epoch 16, batch 190, train_loss 3.33760595322\n",
      "epoch 16, batch 191, train_loss 3.18913626671\n",
      "epoch 16, batch 192, train_loss 4.07136917114\n",
      "epoch 16, batch 193, train_loss 3.81067490578\n",
      "epoch 16, batch 194, train_loss 4.38725566864\n",
      "epoch 16, batch 195, train_loss 4.46862792969\n",
      "epoch 16, batch 196, train_loss 2.99848365784\n",
      "epoch 16, batch 197, train_loss 2.40531992912\n",
      "epoch 16, batch 198, train_loss 2.21398162842\n",
      "epoch 16, batch 199, train_loss 3.11915707588\n",
      "epoch 16, batch 200, train_loss 2.66597223282\n",
      "epoch 16, batch 201, train_loss 2.91978526115\n",
      "epoch 16, batch 202, train_loss 3.35810518265\n",
      "epoch 16, batch 203, train_loss 3.50036859512\n",
      "epoch 16, batch 204, train_loss 3.49543356895\n",
      "epoch 16, batch 205, train_loss 2.93320059776\n",
      "epoch 16, batch 206, train_loss 3.90713524818\n",
      "epoch 16, batch 207, train_loss 3.31163096428\n",
      "epoch 16, batch 208, train_loss 3.97750258446\n",
      "epoch 16, batch 209, train_loss 3.707239151\n",
      "epoch 16, batch 210, train_loss 2.03947973251\n",
      "epoch 16, batch 211, train_loss 2.74368619919\n",
      "epoch 16, batch 212, train_loss 3.6261985302\n",
      "epoch 16, batch 213, train_loss 2.97592425346\n",
      "epoch 16, batch 214, train_loss 2.86109757423\n",
      "epoch 16, batch 215, train_loss 2.12266230583\n",
      "epoch 16, batch 216, train_loss 3.20960617065\n",
      "epoch 16, batch 217, train_loss 2.69595909119\n",
      "epoch 16, batch 218, train_loss 3.11628556252\n",
      "epoch 16, batch 219, train_loss 3.21128606796\n",
      "epoch 16, batch 220, train_loss 3.26984024048\n",
      "epoch 16, batch 221, train_loss 3.37073731422\n",
      "epoch 16, batch 222, train_loss 2.19684863091\n",
      "epoch 16, batch 223, train_loss 2.19484782219\n",
      "epoch 16, batch 224, train_loss 2.53759789467\n",
      "epoch 16, batch 225, train_loss 2.65857434273\n",
      "epoch 16, batch 226, train_loss 3.13811254501\n",
      "epoch 16, batch 227, train_loss 2.91850042343\n",
      "epoch 16, batch 228, train_loss 3.06573724747\n",
      "epoch 16, batch 229, train_loss 2.88496994972\n",
      "epoch 16, batch 230, train_loss 2.76717019081\n",
      "epoch 16, batch 231, train_loss 2.48292326927\n",
      "epoch 16, batch 232, train_loss 3.10465908051\n",
      "epoch 16, batch 233, train_loss 3.33880114555\n",
      "epoch 16, batch 234, train_loss 2.93881678581\n",
      "epoch 16, batch 235, train_loss 3.46258306503\n",
      "epoch 16, batch 236, train_loss 3.04141807556\n",
      "epoch 16, batch 237, train_loss 2.92370438576\n",
      "epoch 16, batch 238, train_loss 2.9706556797\n",
      "epoch 16, batch 239, train_loss 2.80115890503\n",
      "epoch 16, batch 240, train_loss 3.21577072144\n",
      "epoch 16, batch 241, train_loss 3.14956974983\n",
      "epoch 16, batch 242, train_loss 2.96003079414\n",
      "epoch 16, batch 243, train_loss 3.15844225883\n",
      "epoch 16, batch 244, train_loss 3.29450106621\n",
      "epoch 16, batch 245, train_loss 3.06681275368\n",
      "epoch 16, batch 246, train_loss 2.97694587708\n",
      "epoch 16, batch 247, train_loss 3.16175889969\n",
      "epoch 16, batch 248, train_loss 2.9104757309\n",
      "epoch 16, batch 249, train_loss 3.55591154099\n",
      "epoch 16, batch 250, train_loss 3.30670881271\n",
      "epoch 16, batch 251, train_loss 3.65852689743\n",
      "epoch 16, batch 252, train_loss 2.54403543472\n",
      "epoch 16, batch 253, train_loss 2.08468532562\n",
      "epoch 16, batch 254, train_loss 2.66895604134\n",
      "epoch 16, batch 255, train_loss 2.08131241798\n",
      "epoch 16, batch 256, train_loss 2.4526052475\n",
      "epoch 16, batch 257, train_loss 2.876090765\n",
      "epoch 16, batch 258, train_loss 2.29304742813\n",
      "epoch 16, batch 259, train_loss 2.24180698395\n",
      "epoch 16, batch 260, train_loss 2.02117657661\n",
      "epoch 16, batch 261, train_loss 3.27723097801\n",
      "epoch 16, batch 262, train_loss 3.85419654846\n",
      "epoch 16, batch 263, train_loss 3.02407431602\n",
      "epoch 16, batch 264, train_loss 2.73566794395\n",
      "epoch 16, batch 265, train_loss 3.31425237656\n",
      "epoch 16, batch 266, train_loss 3.35296869278\n",
      "epoch 16, batch 267, train_loss 2.66093087196\n",
      "epoch 16, batch 268, train_loss 2.20310688019\n",
      "epoch 16, batch 269, train_loss 2.29399490356\n",
      "epoch 16, batch 270, train_loss 2.58408665657\n",
      "epoch 16, batch 271, train_loss 2.89729356766\n",
      "epoch 16, batch 272, train_loss 2.92033934593\n",
      "epoch 16, batch 273, train_loss 3.02714920044\n",
      "epoch 16, batch 274, train_loss 2.84031391144\n",
      "epoch 16, batch 275, train_loss 2.87206840515\n",
      "epoch 16, batch 276, train_loss 2.67521166801\n",
      "epoch 16, batch 277, train_loss 3.45979595184\n",
      "epoch 16, batch 278, train_loss 2.60791254044\n",
      "epoch 16, batch 279, train_loss 3.10938763618\n",
      "epoch 16, batch 280, train_loss 3.08470892906\n",
      "epoch 16, batch 281, train_loss 2.93875026703\n",
      "epoch 16, batch 282, train_loss 2.57684278488\n",
      "epoch 16, batch 283, train_loss 4.0208697319\n",
      "epoch 16, batch 284, train_loss 3.03373599052\n",
      "epoch 16, batch 285, train_loss 1.88162088394\n",
      "epoch 16, batch 286, train_loss 2.12492752075\n",
      "epoch 16, batch 287, train_loss 2.22059845924\n",
      "epoch 16, batch 288, train_loss 2.85257029533\n",
      "epoch 16, batch 289, train_loss 2.88902378082\n",
      "epoch 16, batch 290, train_loss 2.70437240601\n",
      "epoch 16, batch 291, train_loss 2.59853863716\n",
      "epoch 16, batch 292, train_loss 2.15514850616\n",
      "epoch 16, batch 293, train_loss 3.14411139488\n",
      "epoch 16, batch 294, train_loss 2.76529049873\n",
      "epoch 16, batch 295, train_loss 2.50608658791\n",
      "epoch 16, batch 296, train_loss 2.77796411514\n",
      "epoch 16, batch 297, train_loss 2.46459460258\n",
      "epoch 16, batch 298, train_loss 2.67610907555\n",
      "epoch 16, batch 299, train_loss 3.12699460983\n",
      "epoch 16, batch 300, train_loss 2.91457891464\n",
      "epoch 16, batch 301, train_loss 3.79072380066\n",
      "epoch 16, batch 302, train_loss 3.81177616119\n",
      "epoch 16, batch 303, train_loss 3.53260469437\n",
      "epoch 16, batch 304, train_loss 4.12268686295\n",
      "epoch 16, batch 305, train_loss 4.11401700974\n",
      "epoch 16, batch 306, train_loss 2.83249258995\n",
      "epoch 16, batch 307, train_loss 2.39189147949\n",
      "epoch 16, batch 308, train_loss 2.80893445015\n",
      "epoch 16, batch 309, train_loss 2.96042180061\n",
      "epoch 16, batch 310, train_loss 2.80304074287\n",
      "epoch 16, batch 311, train_loss 2.88752508163\n",
      "epoch 16, batch 312, train_loss 3.05856728554\n",
      "epoch 16, batch 313, train_loss 2.78497862816\n",
      "epoch 16, batch 314, train_loss 3.09397649765\n",
      "epoch 16, batch 315, train_loss 2.61967945099\n",
      "epoch 16, batch 316, train_loss 2.77692651749\n",
      "epoch 16, batch 317, train_loss 3.22092676163\n",
      "epoch 16, batch 318, train_loss 2.38448476791\n",
      "epoch 16, batch 319, train_loss 3.97521877289\n",
      "epoch 16, batch 320, train_loss 2.91811418533\n",
      "epoch 16, batch 321, train_loss 4.07584190369\n",
      "epoch 16, batch 322, train_loss 2.29117131233\n",
      "epoch 16, batch 323, train_loss 2.16877841949\n",
      "epoch 16, batch 324, train_loss 3.22108864784\n",
      "epoch 16, batch 325, train_loss 2.94031906128\n",
      "epoch 16, batch 326, train_loss 2.91899490356\n",
      "epoch 16, batch 327, train_loss 2.89027690887\n",
      "epoch 16, batch 328, train_loss 2.75934696198\n",
      "epoch 16, batch 329, train_loss 2.46334552765\n",
      "epoch 16, batch 330, train_loss 3.7423813343\n",
      "epoch 16, batch 331, train_loss 3.45128703117\n",
      "epoch 16, batch 332, train_loss 2.33206057549\n",
      "epoch 16, batch 333, train_loss 3.31169176102\n",
      "epoch 16, batch 334, train_loss 3.03008651733\n",
      "epoch 16, batch 335, train_loss 2.94590091705\n",
      "epoch 16, batch 336, train_loss 3.45116114616\n",
      "epoch 16, batch 337, train_loss 2.1315972805\n",
      "epoch 16, batch 338, train_loss 2.92015981674\n",
      "epoch 16, batch 339, train_loss 2.72391176224\n",
      "epoch 16, batch 340, train_loss 2.91268444061\n",
      "epoch 16, batch 341, train_loss 2.86823511124\n",
      "epoch 16, batch 342, train_loss 2.887373209\n",
      "epoch 16, batch 343, train_loss 2.79238271713\n",
      "epoch 16, batch 344, train_loss 3.6374540329\n",
      "epoch 16, batch 345, train_loss 3.09628629684\n",
      "epoch 16, batch 346, train_loss 2.88681864738\n",
      "epoch 16, batch 347, train_loss 2.70321273804\n",
      "epoch 16, batch 348, train_loss 2.95593500137\n",
      "epoch 16, batch 349, train_loss 3.89824509621\n",
      "epoch 16, batch 350, train_loss 3.86371064186\n",
      "epoch 16, batch 351, train_loss 3.68958425522\n",
      "epoch 16, batch 352, train_loss 3.73196482658\n",
      "epoch 16, batch 353, train_loss 3.04255080223\n",
      "epoch 16, batch 354, train_loss 2.44312119484\n",
      "epoch 16, batch 355, train_loss 2.76972961426\n",
      "epoch 16, batch 356, train_loss 2.72313213348\n",
      "epoch 16, batch 357, train_loss 2.65413570404\n",
      "epoch 16, batch 358, train_loss 2.25543904305\n",
      "epoch 16, batch 359, train_loss 3.16192483902\n",
      "epoch 16, batch 360, train_loss 2.68799471855\n",
      "epoch 16, batch 361, train_loss 2.39216375351\n",
      "epoch 16, batch 362, train_loss 4.00765371323\n",
      "epoch 16, batch 363, train_loss 2.97581458092\n",
      "epoch 16, batch 364, train_loss 2.87554597855\n",
      "epoch 16, batch 365, train_loss 3.64771056175\n",
      "epoch 16, batch 366, train_loss 3.81909704208\n",
      "epoch 16, batch 367, train_loss 4.52531337738\n",
      "epoch 16, batch 368, train_loss 2.36869239807\n",
      "epoch 16, batch 369, train_loss 3.23683905602\n",
      "epoch 16, batch 370, train_loss 3.61920332909\n",
      "epoch 16, batch 371, train_loss 4.50049829483\n",
      "epoch 16, batch 372, train_loss 2.17246007919\n",
      "epoch 16, batch 373, train_loss 2.32509064674\n",
      "epoch 16, batch 374, train_loss 2.66558933258\n",
      "epoch 16, batch 375, train_loss 3.02323389053\n",
      "epoch 16, batch 376, train_loss 2.33801960945\n",
      "epoch 16, batch 377, train_loss 2.7077152729\n",
      "epoch 16, batch 378, train_loss 3.35544514656\n",
      "epoch 16, batch 379, train_loss 2.58405065536\n",
      "epoch 16, batch 380, train_loss 2.60604166985\n",
      "epoch 16, batch 381, train_loss 2.70976209641\n",
      "epoch 16, batch 382, train_loss 3.49095797539\n",
      "epoch 16, batch 383, train_loss 3.11321210861\n",
      "epoch 16, batch 384, train_loss 2.80875778198\n",
      "epoch 16, batch 385, train_loss 4.11231231689\n",
      "epoch 16, batch 386, train_loss 2.50530791283\n",
      "epoch 16, batch 387, train_loss 4.11905193329\n",
      "epoch 16, batch 388, train_loss 2.50975584984\n",
      "epoch 16, batch 389, train_loss 3.95759701729\n",
      "epoch 16, batch 390, train_loss 3.98503994942\n",
      "epoch 16, batch 391, train_loss 4.02846288681\n",
      "epoch 16, batch 392, train_loss 3.81096410751\n",
      "epoch 16, batch 393, train_loss 3.07155799866\n",
      "epoch 16, batch 394, train_loss 3.67203974724\n",
      "epoch 16, batch 395, train_loss 2.69109797478\n",
      "epoch 16, batch 396, train_loss 3.86971640587\n",
      "epoch 16, batch 397, train_loss 3.96132683754\n",
      "epoch 16, batch 398, train_loss 2.9972217083\n",
      "epoch 16, batch 399, train_loss 3.19567131996\n",
      "epoch 16, batch 400, train_loss 4.08283758163\n",
      "epoch 16, batch 401, train_loss 3.40204429626\n",
      "epoch 16, batch 402, train_loss 3.01633906364\n",
      "epoch 16, batch 403, train_loss 2.9589445591\n",
      "epoch 16, batch 404, train_loss 3.74096918106\n",
      "epoch 16, batch 405, train_loss 3.27749633789\n",
      "epoch 16, batch 406, train_loss 3.1522629261\n",
      "epoch 16, batch 407, train_loss 2.96366477013\n",
      "epoch 16, batch 408, train_loss 2.81137013435\n",
      "epoch 16, batch 409, train_loss 2.36590623856\n",
      "epoch 16, batch 410, train_loss 3.90614771843\n",
      "epoch 16, batch 411, train_loss 3.03606987\n",
      "epoch 16, batch 412, train_loss 2.58161735535\n",
      "epoch 16, batch 413, train_loss 2.89852905273\n",
      "epoch 16, batch 414, train_loss 3.36687898636\n",
      "epoch 16, batch 415, train_loss 3.45689082146\n",
      "epoch 16, batch 416, train_loss 3.21111416817\n",
      "epoch 16, batch 417, train_loss 2.7936360836\n",
      "epoch 16, batch 418, train_loss 3.17238688469\n",
      "epoch 16, batch 419, train_loss 2.84168314934\n",
      "epoch 16, batch 420, train_loss 2.31492161751\n",
      "epoch 16, batch 421, train_loss 3.99990606308\n",
      "epoch 16, batch 422, train_loss 2.80214929581\n",
      "epoch 16, batch 423, train_loss 2.91319775581\n",
      "epoch 16, batch 424, train_loss 3.93137216568\n",
      "epoch 16, batch 425, train_loss 3.30655741692\n",
      "epoch 16, batch 426, train_loss 4.38334941864\n",
      "epoch 16, batch 427, train_loss 3.56971359253\n",
      "epoch 16, batch 428, train_loss 2.43657875061\n",
      "epoch 16, batch 429, train_loss 3.3785982132\n",
      "epoch 16, batch 430, train_loss 3.39993906021\n",
      "epoch 16, batch 431, train_loss 3.05347037315\n",
      "epoch 16, batch 432, train_loss 3.2305829525\n",
      "epoch 16, batch 433, train_loss 2.68606972694\n",
      "epoch 16, batch 434, train_loss 3.17225384712\n",
      "epoch 16, batch 435, train_loss 3.00933766365\n",
      "epoch 16, batch 436, train_loss 3.32678389549\n",
      "epoch 16, batch 437, train_loss 3.74606275558\n",
      "epoch 16, batch 438, train_loss 3.45176625252\n",
      "epoch 16, batch 439, train_loss 3.19413232803\n",
      "epoch 16, batch 440, train_loss 4.37273073196\n",
      "epoch 16, batch 441, train_loss 4.50108766556\n",
      "epoch 16, batch 442, train_loss 3.73227310181\n",
      "epoch 16, batch 443, train_loss 2.29522681236\n",
      "epoch 16, batch 444, train_loss 2.79633307457\n",
      "epoch 16, batch 445, train_loss 4.29918861389\n",
      "epoch 16, batch 446, train_loss 3.14301729202\n",
      "epoch 16, batch 447, train_loss 2.85595798492\n",
      "epoch 16, batch 448, train_loss 4.16983747482\n",
      "epoch 16, batch 449, train_loss 3.61321687698\n",
      "epoch 16, batch 450, train_loss 2.95754885674\n",
      "epoch 16, batch 451, train_loss 2.70568990707\n",
      "epoch 16, batch 452, train_loss 2.48700547218\n",
      "epoch 16, batch 453, train_loss 4.68790102005\n",
      "epoch 16, batch 454, train_loss 4.51288366318\n",
      "epoch 16, batch 455, train_loss 2.37188386917\n",
      "epoch 16, batch 456, train_loss 3.01423430443\n",
      "epoch 16, batch 457, train_loss 3.16951155663\n",
      "epoch 16, batch 458, train_loss 2.8903028965\n",
      "epoch 16, batch 459, train_loss 3.13800215721\n",
      "epoch 16, batch 460, train_loss 3.59731674194\n",
      "epoch 16, batch 461, train_loss 3.12006521225\n",
      "epoch 16, batch 462, train_loss 2.8358130455\n",
      "epoch 16, batch 463, train_loss 2.97352361679\n",
      "epoch 16, batch 464, train_loss 3.24286341667\n",
      "epoch 16, batch 465, train_loss 2.68009829521\n",
      "epoch 16, batch 466, train_loss 3.39754152298\n",
      "epoch 16, batch 467, train_loss 3.23142766953\n",
      "epoch 16, batch 468, train_loss 3.371311903\n",
      "epoch 16, batch 469, train_loss 2.94242167473\n",
      "epoch 16, batch 470, train_loss 2.80691480637\n",
      "epoch 16, batch 471, train_loss 2.90077757835\n",
      "epoch 16, batch 472, train_loss 2.76679992676\n",
      "epoch 16, batch 473, train_loss 3.43521618843\n",
      "epoch 16, batch 474, train_loss 3.95929288864\n",
      "epoch 16, batch 475, train_loss 2.8108355999\n",
      "epoch 16, batch 476, train_loss 2.34747219086\n",
      "epoch 16, batch 477, train_loss 2.40092396736\n",
      "epoch 16, batch 478, train_loss 2.2529335022\n",
      "epoch 16, batch 479, train_loss 2.76776814461\n",
      "epoch 16, batch 480, train_loss 2.90461015701\n",
      "epoch 16, batch 481, train_loss 4.37416267395\n",
      "epoch 16, batch 482, train_loss 2.65665793419\n",
      "epoch 16, batch 483, train_loss 3.22079586983\n",
      "epoch 16, batch 484, train_loss 2.92824316025\n",
      "epoch 16, batch 485, train_loss 2.20373940468\n",
      "epoch 16, batch 486, train_loss 2.20440745354\n",
      "epoch 16, batch 487, train_loss 2.37050223351\n",
      "epoch 16, batch 488, train_loss 2.76895093918\n",
      "epoch 16, batch 489, train_loss 2.89089727402\n",
      "epoch 16, batch 490, train_loss 2.78857851028\n",
      "epoch 16, batch 491, train_loss 2.73666858673\n",
      "epoch 16, batch 492, train_loss 3.06445026398\n",
      "epoch 16, batch 493, train_loss 2.70203185081\n",
      "epoch 16, batch 494, train_loss 2.78370046616\n",
      "epoch 16, batch 495, train_loss 2.85576820374\n",
      "epoch 16, batch 496, train_loss 2.50148868561\n",
      "epoch 16, batch 497, train_loss 2.66640663147\n",
      "epoch 16, batch 498, train_loss 2.78742742538\n",
      "epoch 16, batch 499, train_loss 2.74382591248\n",
      "epoch 16, batch 500, train_loss 3.11796617508\n",
      "epoch 16, batch 501, train_loss 2.83060336113\n",
      "epoch 16, batch 502, train_loss 3.99438333511\n",
      "epoch 16, batch 503, train_loss 3.90543079376\n",
      "epoch 16, batch 504, train_loss 3.43420410156\n",
      "epoch 16, batch 505, train_loss 3.29525494576\n",
      "epoch 16, batch 506, train_loss 2.96982383728\n",
      "epoch 16, batch 507, train_loss 3.92003011703\n",
      "epoch 16, batch 508, train_loss 2.84526133537\n",
      "epoch 16, batch 509, train_loss 4.04031085968\n",
      "epoch 16, batch 510, train_loss 4.00415945053\n",
      "epoch 16, batch 511, train_loss 4.0363073349\n",
      "epoch 16, batch 512, train_loss 3.17257785797\n",
      "epoch 16, batch 513, train_loss 4.45210313797\n",
      "epoch 16, batch 514, train_loss 4.38108062744\n",
      "epoch 16, batch 515, train_loss 4.30303907394\n",
      "epoch 16, batch 516, train_loss 2.65812659264\n",
      "epoch 16, batch 517, train_loss 2.8649532795\n",
      "epoch 16, batch 518, train_loss 2.7787129879\n",
      "epoch 16, batch 519, train_loss 2.47482061386\n",
      "epoch 16, batch 520, train_loss 3.51239442825\n",
      "epoch 16, batch 521, train_loss 2.77021002769\n",
      "epoch 16, batch 522, train_loss 2.4305472374\n",
      "epoch 16, batch 523, train_loss 2.42655491829\n",
      "epoch 16, batch 524, train_loss 2.27897357941\n",
      "epoch 16, batch 525, train_loss 2.1178381443\n",
      "epoch 16, batch 526, train_loss 2.20485424995\n",
      "epoch 16, batch 527, train_loss 2.20809006691\n",
      "epoch 16, batch 528, train_loss 2.43004798889\n",
      "epoch 16, batch 529, train_loss 2.32661557198\n",
      "epoch 16, batch 530, train_loss 1.85275578499\n",
      "epoch 16, batch 531, train_loss 1.67100119591\n",
      "epoch 16, batch 532, train_loss 2.3217663765\n",
      "epoch 16, batch 533, train_loss 1.52109992504\n",
      "epoch 16, batch 534, train_loss 1.62310159206\n",
      "epoch 16, batch 535, train_loss 2.92433834076\n",
      "epoch 16, batch 536, train_loss 3.15554451942\n",
      "epoch 16, batch 537, train_loss 3.03400182724\n",
      "epoch 16, batch 538, train_loss 3.13206243515\n",
      "epoch 16, batch 539, train_loss 3.37317585945\n",
      "epoch 16, batch 540, train_loss 3.68019175529\n",
      "epoch 17, batch 0, train_loss 3.17115950584\n",
      "epoch 17, batch 1, train_loss 3.47121405602\n",
      "epoch 17, batch 2, train_loss 2.78006482124\n",
      "epoch 17, batch 3, train_loss 3.09439492226\n",
      "epoch 17, batch 4, train_loss 2.70160412788\n",
      "epoch 17, batch 5, train_loss 2.76377034187\n",
      "epoch 17, batch 6, train_loss 3.53973698616\n",
      "epoch 17, batch 7, train_loss 2.75117731094\n",
      "epoch 17, batch 8, train_loss 2.45514845848\n",
      "epoch 17, batch 9, train_loss 3.29801177979\n",
      "epoch 17, batch 10, train_loss 2.70110368729\n",
      "epoch 17, batch 11, train_loss 2.76583361626\n",
      "epoch 17, batch 12, train_loss 2.5291287899\n",
      "epoch 17, batch 13, train_loss 2.9719748497\n",
      "epoch 17, batch 14, train_loss 2.45163822174\n",
      "epoch 17, batch 15, train_loss 3.1351275444\n",
      "epoch 17, batch 16, train_loss 2.68396711349\n",
      "epoch 17, batch 17, train_loss 2.57673287392\n",
      "epoch 17, batch 18, train_loss 2.29546976089\n",
      "epoch 17, batch 19, train_loss 1.89728748798\n",
      "epoch 17, batch 20, train_loss 2.64086008072\n",
      "epoch 17, batch 21, train_loss 2.55274271965\n",
      "epoch 17, batch 22, train_loss 3.03408694267\n",
      "epoch 17, batch 23, train_loss 2.69260025024\n",
      "epoch 17, batch 24, train_loss 2.7180249691\n",
      "epoch 17, batch 25, train_loss 3.11070013046\n",
      "epoch 17, batch 26, train_loss 3.02821016312\n",
      "epoch 17, batch 27, train_loss 2.95936965942\n",
      "epoch 17, batch 28, train_loss 3.24751281738\n",
      "epoch 17, batch 29, train_loss 3.13960027695\n",
      "epoch 17, batch 30, train_loss 3.22492003441\n",
      "epoch 17, batch 31, train_loss 2.70265769958\n",
      "epoch 17, batch 32, train_loss 2.53894734383\n",
      "epoch 17, batch 33, train_loss 4.17427444458\n",
      "epoch 17, batch 34, train_loss 4.2496638298\n",
      "epoch 17, batch 35, train_loss 3.04429006577\n",
      "epoch 17, batch 36, train_loss 3.04801559448\n",
      "epoch 17, batch 37, train_loss 2.99677801132\n",
      "epoch 17, batch 38, train_loss 3.13884401321\n",
      "epoch 17, batch 39, train_loss 3.62442922592\n",
      "epoch 17, batch 40, train_loss 2.72665572166\n",
      "epoch 17, batch 41, train_loss 3.02614068985\n",
      "epoch 17, batch 42, train_loss 3.01604700089\n",
      "epoch 17, batch 43, train_loss 2.76978230476\n",
      "epoch 17, batch 44, train_loss 3.43233156204\n",
      "epoch 17, batch 45, train_loss 3.07071447372\n",
      "epoch 17, batch 46, train_loss 3.03331279755\n",
      "epoch 17, batch 47, train_loss 3.08777713776\n",
      "epoch 17, batch 48, train_loss 2.90981674194\n",
      "epoch 17, batch 49, train_loss 3.03511738777\n",
      "epoch 17, batch 50, train_loss 2.97276711464\n",
      "epoch 17, batch 51, train_loss 2.62391972542\n",
      "epoch 17, batch 52, train_loss 3.05146670341\n",
      "epoch 17, batch 53, train_loss 2.89525389671\n",
      "epoch 17, batch 54, train_loss 2.72236633301\n",
      "epoch 17, batch 55, train_loss 2.7685008049\n",
      "epoch 17, batch 56, train_loss 2.64935874939\n",
      "epoch 17, batch 57, train_loss 3.07076787949\n",
      "epoch 17, batch 58, train_loss 2.90261936188\n",
      "epoch 17, batch 59, train_loss 2.85175728798\n",
      "epoch 17, batch 60, train_loss 3.04540014267\n",
      "epoch 17, batch 61, train_loss 1.757158041\n",
      "epoch 17, batch 62, train_loss 2.64646220207\n",
      "epoch 17, batch 63, train_loss 2.7957239151\n",
      "epoch 17, batch 64, train_loss 3.05710148811\n",
      "epoch 17, batch 65, train_loss 3.36050915718\n",
      "epoch 17, batch 66, train_loss 3.11340904236\n",
      "epoch 17, batch 67, train_loss 2.01722168922\n",
      "epoch 17, batch 68, train_loss 2.81139874458\n",
      "epoch 17, batch 69, train_loss 2.20359134674\n",
      "epoch 17, batch 70, train_loss 3.58950781822\n",
      "epoch 17, batch 71, train_loss 3.67948555946\n",
      "epoch 17, batch 72, train_loss 2.62736201286\n",
      "epoch 17, batch 73, train_loss 3.01189613342\n",
      "epoch 17, batch 74, train_loss 3.52432489395\n",
      "epoch 17, batch 75, train_loss 3.03296351433\n",
      "epoch 17, batch 76, train_loss 3.3013253212\n",
      "epoch 17, batch 77, train_loss 3.90468883514\n",
      "epoch 17, batch 78, train_loss 2.90008711815\n",
      "epoch 17, batch 79, train_loss 2.91437768936\n",
      "epoch 17, batch 80, train_loss 2.73416090012\n",
      "epoch 17, batch 81, train_loss 2.91079974174\n",
      "epoch 17, batch 82, train_loss 2.8173520565\n",
      "epoch 17, batch 83, train_loss 2.94003868103\n",
      "epoch 17, batch 84, train_loss 2.69574308395\n",
      "epoch 17, batch 85, train_loss 2.70732665062\n",
      "epoch 17, batch 86, train_loss 3.22749090195\n",
      "epoch 17, batch 87, train_loss 2.44643092155\n",
      "epoch 17, batch 88, train_loss 2.76003313065\n",
      "epoch 17, batch 89, train_loss 3.04639053345\n",
      "epoch 17, batch 90, train_loss 2.74289274216\n",
      "epoch 17, batch 91, train_loss 2.76628708839\n",
      "epoch 17, batch 92, train_loss 3.19528079033\n",
      "epoch 17, batch 93, train_loss 3.85378074646\n",
      "epoch 17, batch 94, train_loss 2.78245854378\n",
      "epoch 17, batch 95, train_loss 2.84613895416\n",
      "epoch 17, batch 96, train_loss 2.26869821548\n",
      "epoch 17, batch 97, train_loss 2.79453396797\n",
      "epoch 17, batch 98, train_loss 2.78674697876\n",
      "epoch 17, batch 99, train_loss 2.97644305229\n",
      "epoch 17, batch 100, train_loss 2.79104447365\n",
      "epoch 17, batch 101, train_loss 2.92545676231\n",
      "epoch 17, batch 102, train_loss 2.75643968582\n",
      "epoch 17, batch 103, train_loss 3.92322492599\n",
      "epoch 17, batch 104, train_loss 3.25344896317\n",
      "epoch 17, batch 105, train_loss 3.08388996124\n",
      "epoch 17, batch 106, train_loss 3.45156550407\n",
      "epoch 17, batch 107, train_loss 3.28629565239\n",
      "epoch 17, batch 108, train_loss 2.92997479439\n",
      "epoch 17, batch 109, train_loss 3.20150303841\n",
      "epoch 17, batch 110, train_loss 3.01221489906\n",
      "epoch 17, batch 111, train_loss 3.40270161629\n",
      "epoch 17, batch 112, train_loss 3.35377025604\n",
      "epoch 17, batch 113, train_loss 2.76635050774\n",
      "epoch 17, batch 114, train_loss 3.33130311966\n",
      "epoch 17, batch 115, train_loss 3.94359993935\n",
      "epoch 17, batch 116, train_loss 3.83804678917\n",
      "epoch 17, batch 117, train_loss 4.00969266891\n",
      "epoch 17, batch 118, train_loss 3.49736905098\n",
      "epoch 17, batch 119, train_loss 2.2915251255\n",
      "epoch 17, batch 120, train_loss 3.09043288231\n",
      "epoch 17, batch 121, train_loss 2.90532946587\n",
      "epoch 17, batch 122, train_loss 3.09362435341\n",
      "epoch 17, batch 123, train_loss 2.89868593216\n",
      "epoch 17, batch 124, train_loss 2.6193780899\n",
      "epoch 17, batch 125, train_loss 2.58070921898\n",
      "epoch 17, batch 126, train_loss 2.57225513458\n",
      "epoch 17, batch 127, train_loss 2.62978363037\n",
      "epoch 17, batch 128, train_loss 2.41001725197\n",
      "epoch 17, batch 129, train_loss 2.85261487961\n",
      "epoch 17, batch 130, train_loss 2.77821230888\n",
      "epoch 17, batch 131, train_loss 3.13853335381\n",
      "epoch 17, batch 132, train_loss 2.90394163132\n",
      "epoch 17, batch 133, train_loss 2.19163060188\n",
      "epoch 17, batch 134, train_loss 2.21312785149\n",
      "epoch 17, batch 135, train_loss 3.82755947113\n",
      "epoch 17, batch 136, train_loss 3.32690048218\n",
      "epoch 17, batch 137, train_loss 2.5264146328\n",
      "epoch 17, batch 138, train_loss 2.42109966278\n",
      "epoch 17, batch 139, train_loss 2.89096450806\n",
      "epoch 17, batch 140, train_loss 2.41790819168\n",
      "epoch 17, batch 141, train_loss 2.69835877419\n",
      "epoch 17, batch 142, train_loss 3.79066038132\n",
      "epoch 17, batch 143, train_loss 2.41316008568\n",
      "epoch 17, batch 144, train_loss 1.97520422935\n",
      "epoch 17, batch 145, train_loss 2.88634371758\n",
      "epoch 17, batch 146, train_loss 3.0619020462\n",
      "epoch 17, batch 147, train_loss 2.81425476074\n",
      "epoch 17, batch 148, train_loss 2.71853303909\n",
      "epoch 17, batch 149, train_loss 3.10497641563\n",
      "epoch 17, batch 150, train_loss 3.23785972595\n",
      "epoch 17, batch 151, train_loss 1.99102699757\n",
      "epoch 17, batch 152, train_loss 2.19604301453\n",
      "epoch 17, batch 153, train_loss 3.72883296013\n",
      "epoch 17, batch 154, train_loss 2.9839425087\n",
      "epoch 17, batch 155, train_loss 2.58966135979\n",
      "epoch 17, batch 156, train_loss 2.78738117218\n",
      "epoch 17, batch 157, train_loss 2.4088408947\n",
      "epoch 17, batch 158, train_loss 2.42065143585\n",
      "epoch 17, batch 159, train_loss 2.62028717995\n",
      "epoch 17, batch 160, train_loss 2.69997501373\n",
      "epoch 17, batch 161, train_loss 3.20367074013\n",
      "epoch 17, batch 162, train_loss 3.21075034142\n",
      "epoch 17, batch 163, train_loss 4.5629529953\n",
      "epoch 17, batch 164, train_loss 2.19863772392\n",
      "epoch 17, batch 165, train_loss 2.17654466629\n",
      "epoch 17, batch 166, train_loss 2.43933153152\n",
      "epoch 17, batch 167, train_loss 2.45586705208\n",
      "epoch 17, batch 168, train_loss 2.6379597187\n",
      "epoch 17, batch 169, train_loss 2.48431229591\n",
      "epoch 17, batch 170, train_loss 2.93551015854\n",
      "epoch 17, batch 171, train_loss 2.40283560753\n",
      "epoch 17, batch 172, train_loss 2.9638273716\n",
      "epoch 17, batch 173, train_loss 2.00635313988\n",
      "epoch 17, batch 174, train_loss 3.40302968025\n",
      "epoch 17, batch 175, train_loss 2.93512749672\n",
      "epoch 17, batch 176, train_loss 2.84806799889\n",
      "epoch 17, batch 177, train_loss 2.65576601028\n",
      "epoch 17, batch 178, train_loss 2.58748292923\n",
      "epoch 17, batch 179, train_loss 2.86371994019\n",
      "epoch 17, batch 180, train_loss 2.72094392776\n",
      "epoch 17, batch 181, train_loss 3.3040292263\n",
      "epoch 17, batch 182, train_loss 2.83986759186\n",
      "epoch 17, batch 183, train_loss 3.42916226387\n",
      "epoch 17, batch 184, train_loss 2.25890088081\n",
      "epoch 17, batch 185, train_loss 2.60058307648\n",
      "epoch 17, batch 186, train_loss 3.14128065109\n",
      "epoch 17, batch 187, train_loss 2.28016281128\n",
      "epoch 17, batch 188, train_loss 2.62445211411\n",
      "epoch 17, batch 189, train_loss 2.60858654976\n",
      "epoch 17, batch 190, train_loss 3.31586670876\n",
      "epoch 17, batch 191, train_loss 3.16944336891\n",
      "epoch 17, batch 192, train_loss 4.04367494583\n",
      "epoch 17, batch 193, train_loss 3.78620028496\n",
      "epoch 17, batch 194, train_loss 4.35968255997\n",
      "epoch 17, batch 195, train_loss 4.44022607803\n",
      "epoch 17, batch 196, train_loss 2.98028802872\n",
      "epoch 17, batch 197, train_loss 2.3934700489\n",
      "epoch 17, batch 198, train_loss 2.2032058239\n",
      "epoch 17, batch 199, train_loss 3.09783482552\n",
      "epoch 17, batch 200, train_loss 2.65135240555\n",
      "epoch 17, batch 201, train_loss 2.90234279633\n",
      "epoch 17, batch 202, train_loss 3.34483098984\n",
      "epoch 17, batch 203, train_loss 3.48260378838\n",
      "epoch 17, batch 204, train_loss 3.47655248642\n",
      "epoch 17, batch 205, train_loss 2.91371273994\n",
      "epoch 17, batch 206, train_loss 3.88771748543\n",
      "epoch 17, batch 207, train_loss 3.29662656784\n",
      "epoch 17, batch 208, train_loss 3.95570516586\n",
      "epoch 17, batch 209, train_loss 3.68243002892\n",
      "epoch 17, batch 210, train_loss 2.02764463425\n",
      "epoch 17, batch 211, train_loss 2.72614121437\n",
      "epoch 17, batch 212, train_loss 3.6013777256\n",
      "epoch 17, batch 213, train_loss 2.95434498787\n",
      "epoch 17, batch 214, train_loss 2.84259486198\n",
      "epoch 17, batch 215, train_loss 2.1112344265\n",
      "epoch 17, batch 216, train_loss 3.19050836563\n",
      "epoch 17, batch 217, train_loss 2.68176007271\n",
      "epoch 17, batch 218, train_loss 3.09711384773\n",
      "epoch 17, batch 219, train_loss 3.19245195389\n",
      "epoch 17, batch 220, train_loss 3.25319123268\n",
      "epoch 17, batch 221, train_loss 3.35047006607\n",
      "epoch 17, batch 222, train_loss 2.18407893181\n",
      "epoch 17, batch 223, train_loss 2.1824734211\n",
      "epoch 17, batch 224, train_loss 2.52411746979\n",
      "epoch 17, batch 225, train_loss 2.64399647713\n",
      "epoch 17, batch 226, train_loss 3.11872911453\n",
      "epoch 17, batch 227, train_loss 2.90082931519\n",
      "epoch 17, batch 228, train_loss 3.04899120331\n",
      "epoch 17, batch 229, train_loss 2.86591649055\n",
      "epoch 17, batch 230, train_loss 2.75075531006\n",
      "epoch 17, batch 231, train_loss 2.46667861938\n",
      "epoch 17, batch 232, train_loss 3.08626532555\n",
      "epoch 17, batch 233, train_loss 3.31683325768\n",
      "epoch 17, batch 234, train_loss 2.92091107368\n",
      "epoch 17, batch 235, train_loss 3.44319844246\n",
      "epoch 17, batch 236, train_loss 3.02362418175\n",
      "epoch 17, batch 237, train_loss 2.90552258492\n",
      "epoch 17, batch 238, train_loss 2.95556855202\n",
      "epoch 17, batch 239, train_loss 2.78748488426\n",
      "epoch 17, batch 240, train_loss 3.19810700417\n",
      "epoch 17, batch 241, train_loss 3.13295149803\n",
      "epoch 17, batch 242, train_loss 2.94160652161\n",
      "epoch 17, batch 243, train_loss 3.14038991928\n",
      "epoch 17, batch 244, train_loss 3.27602672577\n",
      "epoch 17, batch 245, train_loss 3.05005717278\n",
      "epoch 17, batch 246, train_loss 2.95960378647\n",
      "epoch 17, batch 247, train_loss 3.14457201958\n",
      "epoch 17, batch 248, train_loss 2.89385986328\n",
      "epoch 17, batch 249, train_loss 3.53156542778\n",
      "epoch 17, batch 250, train_loss 3.2828104496\n",
      "epoch 17, batch 251, train_loss 3.63595747948\n",
      "epoch 17, batch 252, train_loss 2.52849149704\n",
      "epoch 17, batch 253, train_loss 2.07128071785\n",
      "epoch 17, batch 254, train_loss 2.65059423447\n",
      "epoch 17, batch 255, train_loss 2.0688765049\n",
      "epoch 17, batch 256, train_loss 2.4386382103\n",
      "epoch 17, batch 257, train_loss 2.85836625099\n",
      "epoch 17, batch 258, train_loss 2.27975225449\n",
      "epoch 17, batch 259, train_loss 2.23135709763\n",
      "epoch 17, batch 260, train_loss 2.01227664948\n",
      "epoch 17, batch 261, train_loss 3.25750160217\n",
      "epoch 17, batch 262, train_loss 3.8297829628\n",
      "epoch 17, batch 263, train_loss 3.01056885719\n",
      "epoch 17, batch 264, train_loss 2.72222518921\n",
      "epoch 17, batch 265, train_loss 3.29455065727\n",
      "epoch 17, batch 266, train_loss 3.3318605423\n",
      "epoch 17, batch 267, train_loss 2.64419412613\n",
      "epoch 17, batch 268, train_loss 2.18979549408\n",
      "epoch 17, batch 269, train_loss 2.28173518181\n",
      "epoch 17, batch 270, train_loss 2.57076501846\n",
      "epoch 17, batch 271, train_loss 2.88126921654\n",
      "epoch 17, batch 272, train_loss 2.90464949608\n",
      "epoch 17, batch 273, train_loss 3.00996279716\n",
      "epoch 17, batch 274, train_loss 2.82178735733\n",
      "epoch 17, batch 275, train_loss 2.85687923431\n",
      "epoch 17, batch 276, train_loss 2.66214036942\n",
      "epoch 17, batch 277, train_loss 3.44224715233\n",
      "epoch 17, batch 278, train_loss 2.59078192711\n",
      "epoch 17, batch 279, train_loss 3.08742713928\n",
      "epoch 17, batch 280, train_loss 3.06282043457\n",
      "epoch 17, batch 281, train_loss 2.9209792614\n",
      "epoch 17, batch 282, train_loss 2.56157851219\n",
      "epoch 17, batch 283, train_loss 3.99131798744\n",
      "epoch 17, batch 284, train_loss 3.01123857498\n",
      "epoch 17, batch 285, train_loss 1.86848425865\n",
      "epoch 17, batch 286, train_loss 2.11341452599\n",
      "epoch 17, batch 287, train_loss 2.20861768723\n",
      "epoch 17, batch 288, train_loss 2.83906888962\n",
      "epoch 17, batch 289, train_loss 2.87601780891\n",
      "epoch 17, batch 290, train_loss 2.68760943413\n",
      "epoch 17, batch 291, train_loss 2.58447432518\n",
      "epoch 17, batch 292, train_loss 2.14538812637\n",
      "epoch 17, batch 293, train_loss 3.1233932972\n",
      "epoch 17, batch 294, train_loss 2.74861574173\n",
      "epoch 17, batch 295, train_loss 2.4928753376\n",
      "epoch 17, batch 296, train_loss 2.76202058792\n",
      "epoch 17, batch 297, train_loss 2.45157933235\n",
      "epoch 17, batch 298, train_loss 2.66122508049\n",
      "epoch 17, batch 299, train_loss 3.10073113441\n",
      "epoch 17, batch 300, train_loss 2.89475178719\n",
      "epoch 17, batch 301, train_loss 3.76087927818\n",
      "epoch 17, batch 302, train_loss 3.78358221054\n",
      "epoch 17, batch 303, train_loss 3.50848770142\n",
      "epoch 17, batch 304, train_loss 4.09312963486\n",
      "epoch 17, batch 305, train_loss 4.08933544159\n",
      "epoch 17, batch 306, train_loss 2.81134605408\n",
      "epoch 17, batch 307, train_loss 2.37980866432\n",
      "epoch 17, batch 308, train_loss 2.79211521149\n",
      "epoch 17, batch 309, train_loss 2.94222974777\n",
      "epoch 17, batch 310, train_loss 2.78768253326\n",
      "epoch 17, batch 311, train_loss 2.87002372742\n",
      "epoch 17, batch 312, train_loss 3.03895759583\n",
      "epoch 17, batch 313, train_loss 2.76766085625\n",
      "epoch 17, batch 314, train_loss 3.07423377037\n",
      "epoch 17, batch 315, train_loss 2.60630774498\n",
      "epoch 17, batch 316, train_loss 2.76211166382\n",
      "epoch 17, batch 317, train_loss 3.20346689224\n",
      "epoch 17, batch 318, train_loss 2.36998200417\n",
      "epoch 17, batch 319, train_loss 3.94702339172\n",
      "epoch 17, batch 320, train_loss 2.89785313606\n",
      "epoch 17, batch 321, train_loss 4.04999160767\n",
      "epoch 17, batch 322, train_loss 2.27987790108\n",
      "epoch 17, batch 323, train_loss 2.15816259384\n",
      "epoch 17, batch 324, train_loss 3.20318436623\n",
      "epoch 17, batch 325, train_loss 2.92784452438\n",
      "epoch 17, batch 326, train_loss 2.90441203117\n",
      "epoch 17, batch 327, train_loss 2.87638902664\n",
      "epoch 17, batch 328, train_loss 2.74277901649\n",
      "epoch 17, batch 329, train_loss 2.44646430016\n",
      "epoch 17, batch 330, train_loss 3.72249126434\n",
      "epoch 17, batch 331, train_loss 3.43205189705\n",
      "epoch 17, batch 332, train_loss 2.3193757534\n",
      "epoch 17, batch 333, train_loss 3.29207539558\n",
      "epoch 17, batch 334, train_loss 3.01322817802\n",
      "epoch 17, batch 335, train_loss 2.92991256714\n",
      "epoch 17, batch 336, train_loss 3.42895388603\n",
      "epoch 17, batch 337, train_loss 2.11840653419\n",
      "epoch 17, batch 338, train_loss 2.90504527092\n",
      "epoch 17, batch 339, train_loss 2.71129703522\n",
      "epoch 17, batch 340, train_loss 2.89732575417\n",
      "epoch 17, batch 341, train_loss 2.85472297668\n",
      "epoch 17, batch 342, train_loss 2.87073874474\n",
      "epoch 17, batch 343, train_loss 2.7753970623\n",
      "epoch 17, batch 344, train_loss 3.61227083206\n",
      "epoch 17, batch 345, train_loss 3.07506465912\n",
      "epoch 17, batch 346, train_loss 2.86926484108\n",
      "epoch 17, batch 347, train_loss 2.68756985664\n",
      "epoch 17, batch 348, train_loss 2.93929195404\n",
      "epoch 17, batch 349, train_loss 3.87745046616\n",
      "epoch 17, batch 350, train_loss 3.84099125862\n",
      "epoch 17, batch 351, train_loss 3.6673309803\n",
      "epoch 17, batch 352, train_loss 3.70846247673\n",
      "epoch 17, batch 353, train_loss 3.02472257614\n",
      "epoch 17, batch 354, train_loss 2.42735362053\n",
      "epoch 17, batch 355, train_loss 2.75411295891\n",
      "epoch 17, batch 356, train_loss 2.7075381279\n",
      "epoch 17, batch 357, train_loss 2.6371486187\n",
      "epoch 17, batch 358, train_loss 2.2412481308\n",
      "epoch 17, batch 359, train_loss 3.14003562927\n",
      "epoch 17, batch 360, train_loss 2.67041087151\n",
      "epoch 17, batch 361, train_loss 2.37774777412\n",
      "epoch 17, batch 362, train_loss 3.9824359417\n",
      "epoch 17, batch 363, train_loss 2.96054911613\n",
      "epoch 17, batch 364, train_loss 2.86008882523\n",
      "epoch 17, batch 365, train_loss 3.61597537994\n",
      "epoch 17, batch 366, train_loss 3.79467654228\n",
      "epoch 17, batch 367, train_loss 4.49478435516\n",
      "epoch 17, batch 368, train_loss 2.35540652275\n",
      "epoch 17, batch 369, train_loss 3.21632146835\n",
      "epoch 17, batch 370, train_loss 3.59894537926\n",
      "epoch 17, batch 371, train_loss 4.47388792038\n",
      "epoch 17, batch 372, train_loss 2.15976023674\n",
      "epoch 17, batch 373, train_loss 2.31126570702\n",
      "epoch 17, batch 374, train_loss 2.65066885948\n",
      "epoch 17, batch 375, train_loss 3.00585007668\n",
      "epoch 17, batch 376, train_loss 2.32856917381\n",
      "epoch 17, batch 377, train_loss 2.69241738319\n",
      "epoch 17, batch 378, train_loss 3.3343269825\n",
      "epoch 17, batch 379, train_loss 2.56990742683\n",
      "epoch 17, batch 380, train_loss 2.59261131287\n",
      "epoch 17, batch 381, train_loss 2.69568371773\n",
      "epoch 17, batch 382, train_loss 3.46832919121\n",
      "epoch 17, batch 383, train_loss 3.0937538147\n",
      "epoch 17, batch 384, train_loss 2.79280042648\n",
      "epoch 17, batch 385, train_loss 4.08855009079\n",
      "epoch 17, batch 386, train_loss 2.49144721031\n",
      "epoch 17, batch 387, train_loss 4.07732725143\n",
      "epoch 17, batch 388, train_loss 2.49483728409\n",
      "epoch 17, batch 389, train_loss 3.92945289612\n",
      "epoch 17, batch 390, train_loss 3.96327710152\n",
      "epoch 17, batch 391, train_loss 4.00540399551\n",
      "epoch 17, batch 392, train_loss 3.78972482681\n",
      "epoch 17, batch 393, train_loss 3.05477762222\n",
      "epoch 17, batch 394, train_loss 3.6528904438\n",
      "epoch 17, batch 395, train_loss 2.67551755905\n",
      "epoch 17, batch 396, train_loss 3.84594130516\n",
      "epoch 17, batch 397, train_loss 3.9378592968\n",
      "epoch 17, batch 398, train_loss 2.97781920433\n",
      "epoch 17, batch 399, train_loss 3.17354774475\n",
      "epoch 17, batch 400, train_loss 4.05514001846\n",
      "epoch 17, batch 401, train_loss 3.38227725029\n",
      "epoch 17, batch 402, train_loss 2.99787569046\n",
      "epoch 17, batch 403, train_loss 2.94201803207\n",
      "epoch 17, batch 404, train_loss 3.71823263168\n",
      "epoch 17, batch 405, train_loss 3.25619077682\n",
      "epoch 17, batch 406, train_loss 3.13488531113\n",
      "epoch 17, batch 407, train_loss 2.94773244858\n",
      "epoch 17, batch 408, train_loss 2.79475045204\n",
      "epoch 17, batch 409, train_loss 2.3562476635\n",
      "epoch 17, batch 410, train_loss 3.88611698151\n",
      "epoch 17, batch 411, train_loss 3.01898384094\n",
      "epoch 17, batch 412, train_loss 2.56941437721\n",
      "epoch 17, batch 413, train_loss 2.8844332695\n",
      "epoch 17, batch 414, train_loss 3.34425330162\n",
      "epoch 17, batch 415, train_loss 3.43571996689\n",
      "epoch 17, batch 416, train_loss 3.19092178345\n",
      "epoch 17, batch 417, train_loss 2.77650332451\n",
      "epoch 17, batch 418, train_loss 3.15604376793\n",
      "epoch 17, batch 419, train_loss 2.82988023758\n",
      "epoch 17, batch 420, train_loss 2.30344247818\n",
      "epoch 17, batch 421, train_loss 3.98050522804\n",
      "epoch 17, batch 422, train_loss 2.78654551506\n",
      "epoch 17, batch 423, train_loss 2.89473462105\n",
      "epoch 17, batch 424, train_loss 3.9056854248\n",
      "epoch 17, batch 425, train_loss 3.28875041008\n",
      "epoch 17, batch 426, train_loss 4.35520648956\n",
      "epoch 17, batch 427, train_loss 3.54912948608\n",
      "epoch 17, batch 428, train_loss 2.42278933525\n",
      "epoch 17, batch 429, train_loss 3.36050462723\n",
      "epoch 17, batch 430, train_loss 3.37631869316\n",
      "epoch 17, batch 431, train_loss 3.03676843643\n",
      "epoch 17, batch 432, train_loss 3.21164488792\n",
      "epoch 17, batch 433, train_loss 2.67031478882\n",
      "epoch 17, batch 434, train_loss 3.15320730209\n",
      "epoch 17, batch 435, train_loss 2.99259734154\n",
      "epoch 17, batch 436, train_loss 3.3083486557\n",
      "epoch 17, batch 437, train_loss 3.72511076927\n",
      "epoch 17, batch 438, train_loss 3.43489742279\n",
      "epoch 17, batch 439, train_loss 3.17484140396\n",
      "epoch 17, batch 440, train_loss 4.34509801865\n",
      "epoch 17, batch 441, train_loss 4.47344493866\n",
      "epoch 17, batch 442, train_loss 3.71114945412\n",
      "epoch 17, batch 443, train_loss 2.2820353508\n",
      "epoch 17, batch 444, train_loss 2.78108048439\n",
      "epoch 17, batch 445, train_loss 4.27294540405\n",
      "epoch 17, batch 446, train_loss 3.12733817101\n",
      "epoch 17, batch 447, train_loss 2.84271168709\n",
      "epoch 17, batch 448, train_loss 4.14358949661\n",
      "epoch 17, batch 449, train_loss 3.59385156631\n",
      "epoch 17, batch 450, train_loss 2.94087839127\n",
      "epoch 17, batch 451, train_loss 2.69055366516\n",
      "epoch 17, batch 452, train_loss 2.47286462784\n",
      "epoch 17, batch 453, train_loss 4.64164638519\n",
      "epoch 17, batch 454, train_loss 4.46650695801\n",
      "epoch 17, batch 455, train_loss 2.3564260006\n",
      "epoch 17, batch 456, train_loss 2.99581432343\n",
      "epoch 17, batch 457, train_loss 3.14529037476\n",
      "epoch 17, batch 458, train_loss 2.87097239494\n",
      "epoch 17, batch 459, train_loss 3.11721467972\n",
      "epoch 17, batch 460, train_loss 3.57403182983\n",
      "epoch 17, batch 461, train_loss 3.09744787216\n",
      "epoch 17, batch 462, train_loss 2.81622433662\n",
      "epoch 17, batch 463, train_loss 2.95452737808\n",
      "epoch 17, batch 464, train_loss 3.2218503952\n",
      "epoch 17, batch 465, train_loss 2.66513514519\n",
      "epoch 17, batch 466, train_loss 3.37664294243\n",
      "epoch 17, batch 467, train_loss 3.21035408974\n",
      "epoch 17, batch 468, train_loss 3.35232758522\n",
      "epoch 17, batch 469, train_loss 2.92272901535\n",
      "epoch 17, batch 470, train_loss 2.78769183159\n",
      "epoch 17, batch 471, train_loss 2.88178133965\n",
      "epoch 17, batch 472, train_loss 2.74795889854\n",
      "epoch 17, batch 473, train_loss 3.41569709778\n",
      "epoch 17, batch 474, train_loss 3.9304933548\n",
      "epoch 17, batch 475, train_loss 2.79464411736\n",
      "epoch 17, batch 476, train_loss 2.33457255363\n",
      "epoch 17, batch 477, train_loss 2.38283157349\n",
      "epoch 17, batch 478, train_loss 2.23701310158\n",
      "epoch 17, batch 479, train_loss 2.75217413902\n",
      "epoch 17, batch 480, train_loss 2.88410282135\n",
      "epoch 17, batch 481, train_loss 4.34282207489\n",
      "epoch 17, batch 482, train_loss 2.63951802254\n",
      "epoch 17, batch 483, train_loss 3.19715666771\n",
      "epoch 17, batch 484, train_loss 2.90827250481\n",
      "epoch 17, batch 485, train_loss 2.18862962723\n",
      "epoch 17, batch 486, train_loss 2.18969154358\n",
      "epoch 17, batch 487, train_loss 2.35558223724\n",
      "epoch 17, batch 488, train_loss 2.75164628029\n",
      "epoch 17, batch 489, train_loss 2.87339925766\n",
      "epoch 17, batch 490, train_loss 2.7686882019\n",
      "epoch 17, batch 491, train_loss 2.72021436691\n",
      "epoch 17, batch 492, train_loss 3.04712605476\n",
      "epoch 17, batch 493, train_loss 2.68833875656\n",
      "epoch 17, batch 494, train_loss 2.76928830147\n",
      "epoch 17, batch 495, train_loss 2.84023070335\n",
      "epoch 17, batch 496, train_loss 2.48827338219\n",
      "epoch 17, batch 497, train_loss 2.65113902092\n",
      "epoch 17, batch 498, train_loss 2.7722632885\n",
      "epoch 17, batch 499, train_loss 2.72718763351\n",
      "epoch 17, batch 500, train_loss 3.09425497055\n",
      "epoch 17, batch 501, train_loss 2.8140668869\n",
      "epoch 17, batch 502, train_loss 3.96638989449\n",
      "epoch 17, batch 503, train_loss 3.87339425087\n",
      "epoch 17, batch 504, train_loss 3.41016292572\n",
      "epoch 17, batch 505, train_loss 3.27587962151\n",
      "epoch 17, batch 506, train_loss 2.95386242867\n",
      "epoch 17, batch 507, train_loss 3.89827895164\n",
      "epoch 17, batch 508, train_loss 2.83366775513\n",
      "epoch 17, batch 509, train_loss 4.01533842087\n",
      "epoch 17, batch 510, train_loss 3.97767949104\n",
      "epoch 17, batch 511, train_loss 4.01048946381\n",
      "epoch 17, batch 512, train_loss 3.15626311302\n",
      "epoch 17, batch 513, train_loss 4.43135595322\n",
      "epoch 17, batch 514, train_loss 4.35575628281\n",
      "epoch 17, batch 515, train_loss 4.28077745438\n",
      "epoch 17, batch 516, train_loss 2.64157104492\n",
      "epoch 17, batch 517, train_loss 2.84556531906\n",
      "epoch 17, batch 518, train_loss 2.76181864738\n",
      "epoch 17, batch 519, train_loss 2.46203494072\n",
      "epoch 17, batch 520, train_loss 3.48762893677\n",
      "epoch 17, batch 521, train_loss 2.75561380386\n",
      "epoch 17, batch 522, train_loss 2.41896700859\n",
      "epoch 17, batch 523, train_loss 2.41546440125\n",
      "epoch 17, batch 524, train_loss 2.26517152786\n",
      "epoch 17, batch 525, train_loss 2.1053686142\n",
      "epoch 17, batch 526, train_loss 2.18940639496\n",
      "epoch 17, batch 527, train_loss 2.19137692451\n",
      "epoch 17, batch 528, train_loss 2.41135787964\n",
      "epoch 17, batch 529, train_loss 2.30645084381\n",
      "epoch 17, batch 530, train_loss 1.83977651596\n",
      "epoch 17, batch 531, train_loss 1.66181457043\n",
      "epoch 17, batch 532, train_loss 2.289270401\n",
      "epoch 17, batch 533, train_loss 1.5077047348\n",
      "epoch 17, batch 534, train_loss 1.6156244278\n",
      "epoch 17, batch 535, train_loss 2.90299892426\n",
      "epoch 17, batch 536, train_loss 3.12996673584\n",
      "epoch 17, batch 537, train_loss 3.01115942001\n",
      "epoch 17, batch 538, train_loss 3.10963845253\n",
      "epoch 17, batch 539, train_loss 3.3473906517\n",
      "epoch 17, batch 540, train_loss 3.65935587883\n",
      "epoch 18, batch 0, train_loss 3.15282058716\n",
      "epoch 18, batch 1, train_loss 3.43834614754\n",
      "epoch 18, batch 2, train_loss 2.73942446709\n",
      "epoch 18, batch 3, train_loss 3.07200860977\n",
      "epoch 18, batch 4, train_loss 2.66988897324\n",
      "epoch 18, batch 5, train_loss 2.73523950577\n",
      "epoch 18, batch 6, train_loss 3.48872256279\n",
      "epoch 18, batch 7, train_loss 2.71835613251\n",
      "epoch 18, batch 8, train_loss 2.42230963707\n",
      "epoch 18, batch 9, train_loss 3.25233435631\n",
      "epoch 18, batch 10, train_loss 2.68215274811\n",
      "epoch 18, batch 11, train_loss 2.74566149712\n",
      "epoch 18, batch 12, train_loss 2.51148867607\n",
      "epoch 18, batch 13, train_loss 2.9483165741\n",
      "epoch 18, batch 14, train_loss 2.43150448799\n",
      "epoch 18, batch 15, train_loss 3.10364890099\n",
      "epoch 18, batch 16, train_loss 2.65605187416\n",
      "epoch 18, batch 17, train_loss 2.55425286293\n",
      "epoch 18, batch 18, train_loss 2.27734899521\n",
      "epoch 18, batch 19, train_loss 1.88352763653\n",
      "epoch 18, batch 20, train_loss 2.61912488937\n",
      "epoch 18, batch 21, train_loss 2.52652144432\n",
      "epoch 18, batch 22, train_loss 3.00477218628\n",
      "epoch 18, batch 23, train_loss 2.67746973038\n",
      "epoch 18, batch 24, train_loss 2.70305132866\n",
      "epoch 18, batch 25, train_loss 3.08884978294\n",
      "epoch 18, batch 26, train_loss 3.00824642181\n",
      "epoch 18, batch 27, train_loss 2.93773245811\n",
      "epoch 18, batch 28, train_loss 3.22313380241\n",
      "epoch 18, batch 29, train_loss 3.1199362278\n",
      "epoch 18, batch 30, train_loss 3.2013938427\n",
      "epoch 18, batch 31, train_loss 2.68911910057\n",
      "epoch 18, batch 32, train_loss 2.52910065651\n",
      "epoch 18, batch 33, train_loss 4.14289569855\n",
      "epoch 18, batch 34, train_loss 4.21755170822\n",
      "epoch 18, batch 35, train_loss 3.0250442028\n",
      "epoch 18, batch 36, train_loss 3.02734780312\n",
      "epoch 18, batch 37, train_loss 2.97159099579\n",
      "epoch 18, batch 38, train_loss 3.117654562\n",
      "epoch 18, batch 39, train_loss 3.60332584381\n",
      "epoch 18, batch 40, train_loss 2.71166419983\n",
      "epoch 18, batch 41, train_loss 3.00940346718\n",
      "epoch 18, batch 42, train_loss 2.99945449829\n",
      "epoch 18, batch 43, train_loss 2.75040102005\n",
      "epoch 18, batch 44, train_loss 3.40815043449\n",
      "epoch 18, batch 45, train_loss 3.05669593811\n",
      "epoch 18, batch 46, train_loss 3.01744937897\n",
      "epoch 18, batch 47, train_loss 3.07126569748\n",
      "epoch 18, batch 48, train_loss 2.88720870018\n",
      "epoch 18, batch 49, train_loss 3.01553416252\n",
      "epoch 18, batch 50, train_loss 2.95344805717\n",
      "epoch 18, batch 51, train_loss 2.60459160805\n",
      "epoch 18, batch 52, train_loss 3.03222727776\n",
      "epoch 18, batch 53, train_loss 2.87238121033\n",
      "epoch 18, batch 54, train_loss 2.70569705963\n",
      "epoch 18, batch 55, train_loss 2.75655937195\n",
      "epoch 18, batch 56, train_loss 2.63373303413\n",
      "epoch 18, batch 57, train_loss 3.05129766464\n",
      "epoch 18, batch 58, train_loss 2.8819129467\n",
      "epoch 18, batch 59, train_loss 2.83267498016\n",
      "epoch 18, batch 60, train_loss 3.02500677109\n",
      "epoch 18, batch 61, train_loss 1.74575769901\n",
      "epoch 18, batch 62, train_loss 2.63072037697\n",
      "epoch 18, batch 63, train_loss 2.77721142769\n",
      "epoch 18, batch 64, train_loss 3.04115605354\n",
      "epoch 18, batch 65, train_loss 3.34454417229\n",
      "epoch 18, batch 66, train_loss 3.09360575676\n",
      "epoch 18, batch 67, train_loss 2.00760364532\n",
      "epoch 18, batch 68, train_loss 2.79460883141\n",
      "epoch 18, batch 69, train_loss 2.1926574707\n",
      "epoch 18, batch 70, train_loss 3.56965088844\n",
      "epoch 18, batch 71, train_loss 3.65801525116\n",
      "epoch 18, batch 72, train_loss 2.61188769341\n",
      "epoch 18, batch 73, train_loss 2.99059200287\n",
      "epoch 18, batch 74, train_loss 3.50289535522\n",
      "epoch 18, batch 75, train_loss 3.01451182365\n",
      "epoch 18, batch 76, train_loss 3.28005313873\n",
      "epoch 18, batch 77, train_loss 3.87733721733\n",
      "epoch 18, batch 78, train_loss 2.88128590584\n",
      "epoch 18, batch 79, train_loss 2.89659714699\n",
      "epoch 18, batch 80, train_loss 2.71539759636\n",
      "epoch 18, batch 81, train_loss 2.89599919319\n",
      "epoch 18, batch 82, train_loss 2.80486416817\n",
      "epoch 18, batch 83, train_loss 2.92668700218\n",
      "epoch 18, batch 84, train_loss 2.68118810654\n",
      "epoch 18, batch 85, train_loss 2.69160652161\n",
      "epoch 18, batch 86, train_loss 3.20941257477\n",
      "epoch 18, batch 87, train_loss 2.43135643005\n",
      "epoch 18, batch 88, train_loss 2.74570846558\n",
      "epoch 18, batch 89, train_loss 3.03314948082\n",
      "epoch 18, batch 90, train_loss 2.72753858566\n",
      "epoch 18, batch 91, train_loss 2.75123381615\n",
      "epoch 18, batch 92, train_loss 3.17751216888\n",
      "epoch 18, batch 93, train_loss 3.82589530945\n",
      "epoch 18, batch 94, train_loss 2.76559305191\n",
      "epoch 18, batch 95, train_loss 2.83083987236\n",
      "epoch 18, batch 96, train_loss 2.25689482689\n",
      "epoch 18, batch 97, train_loss 2.77978563309\n",
      "epoch 18, batch 98, train_loss 2.77331590652\n",
      "epoch 18, batch 99, train_loss 2.95972466469\n",
      "epoch 18, batch 100, train_loss 2.77683734894\n",
      "epoch 18, batch 101, train_loss 2.9088807106\n",
      "epoch 18, batch 102, train_loss 2.73916745186\n",
      "epoch 18, batch 103, train_loss 3.89797925949\n",
      "epoch 18, batch 104, train_loss 3.2360162735\n",
      "epoch 18, batch 105, train_loss 3.06628966331\n"
     ]
    }
   ],
   "source": [
    "logits, last_state, _, _, _ = neural_network()\n",
    "# 转为 1 维\n",
    "targets = tf.reshape(output_targets, [-1])\n",
    "\"\"\"\n",
    "通过 seq2seq 的 sequence_loss_by_example 来得到 loss\n",
    "logits 为 batch_size x num_labels 的数组的数组，故此在外面加了个 []，就是说这个函数可以实现多个 batch 的同时计算 loss，只不过我们这里只计算一个 batch\n",
    "targets 为一维序列化的结果，我们看到是在上一步中转化为 1 维的\n",
    "tf.ones_like(targets, dtype=tf.float32) 是个一维权重，和 targets 有相同的维度 (xxx_like)\n",
    "\"\"\"\n",
    "loss = tf.nn.seq2seq.sequence_loss_by_example([logits], [targets], [tf.ones_like(targets, dtype=tf.float32)], len(poetry.words))\n",
    "cost = tf.reduce_mean(loss)\n",
    "\n",
    "learning_rate = tf.Variable(0.0, trainable=False)\n",
    "# 取出可训练变量\n",
    "tvars = tf.trainable_variables()\n",
    "# 修正梯度值，用于控制梯度爆炸的问题\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5)    # 这里报错  'NoneType' object has no attribute 'grad_context'\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        sess.run(tf.assign(learning_rate, 0.002 * (0.97 ** epoch)))\n",
    "        for batch in range(n_chunk):\n",
    "            train_loss, _, _ = sess.run([cost, last_state, train_op], feed_dict={input_data: x_batches[batch], output_targets: y_batches[batch]})\n",
    "            print(\"epoch {}, batch {}, train_loss {}\".format(epoch, batch, train_loss))\n",
    "        if epoch > 0 and epoch % 7 == 0:\n",
    "            saver.save(sess, 'poetry.module', global_step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拿着保存好的模型来生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "先看看 np.cumsum 的定义 (累积和)\n",
    ">>> a = np.array([[1,2,3], [4,5,6]])\n",
    ">>> a\n",
    "array([[1, 2, 3],\n",
    "       [4, 5, 6]])\n",
    "       \n",
    ">>> np.cumsum(a)\n",
    "array([ 1,  3,  6, 10, 15, 21])\n",
    "\n",
    ">>> np.cumsum(a,axis=0)      # sum over rows for each of the 3 columns\n",
    "array([[1, 2, 3],\n",
    "       [5, 7, 9]])\n",
    "\n",
    ">>> np.cumsum(a,axis=1)      # sum over columns for each of the 2 rows\n",
    "array([[ 1,  3,  6],\n",
    "       [ 4,  9, 15]])\n",
    "\"\"\"\n",
    "# 通过概率得到汉字\n",
    "def to_word(prob):\n",
    "    # 例如 [0.2, 0.3, 0.5] ==> [0.2, 0.5, 1.0]，这样，如果随机一个数为 0.7，就落在 0.5 和 1.0 之间，得到 1.0 的索引\n",
    "    t = np.cumsum(prob)     \n",
    "    s = np.sum(prob)\n",
    "    sample = int(np.searchsorted(t, np.random.rand(1) * s))    # 找到插入点的索引\n",
    "    return peotry.words[sample]\n",
    "\n",
    "def gen_poetry(module_file):\n",
    "    _, last_state, probs, cell, initial_state = neural_network()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())        # 初始化参数\n",
    "        saver = tf.train.Saver(tf.all_variables())     # 指定 saver 为所有的变量做保存\n",
    "        saver.restore(sess, module_file)               # 这样，这个 restore 就是为所有的变量做恢复\n",
    "        \n",
    "        state_ = sess.run(cell.zero_state(1, tf.float32))    # 这里 1 为 batch_size，就是说只给一条数据初始化状态，训练时则是 batch_size 条数据初始化状态\n",
    "        x = np.array([list(map(poetry.word_num_map.get, '['))])    # x 为只有一个元素的batch，这个元素为数组，初始化为起始符号 '[' 在词典中的索引，即 x = array([[$index]])\n",
    "        # 这里使用前面生成的一个元素的 state_ 来替换 neural_network 函数中定义的 batch_size 个元素的 initial_state\n",
    "        [probs_, state_] = sess.run([probs, last_state], feed_dict={input_data: x, initial_state: state_})\n",
    "        # 通过初始值 '[' 预测下一个字，循环，直到出现 ']' 结束符\n",
    "        word = to_word(probs_)\n",
    "        poem = ''\n",
    "        while word != ']':\n",
    "            poem += word\n",
    "            x = np.zeros((1,1))\n",
    "            # 再以 word 对应的索引为初始值，来预测下一个字\n",
    "            x[0,0] = peotry.word_num_map[word]\n",
    "            # 这里的 initial_state 已经转移为上一步得到的 state_ 了\n",
    "            [probs_, state_] = sess.run([probs, last_state], feed_dict={input_data: x, initial_state: state_})\n",
    "            word = to_word(probs_)\n",
    "        return poem    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(gen_poetry('poetry.module-49'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来个藏头诗看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_poetry_with_head(head， module_file):\n",
    "    _, last_state, probs, cell, initial_state = neural_network()\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    " \n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        saver.restore(sess, module_file)\n",
    " \n",
    "        state_ = sess.run(cell.zero_state(1, tf.float32))\n",
    "        poem = ''\n",
    "        i = 0\n",
    "        for word in head:\n",
    "            while word != '，' and word != '。':\n",
    "                poem += word\n",
    "                x = np.array([list(map(poetry.word_num_map.get, word))])\n",
    "                [probs_, state_] = sess.run([probs, last_state], feed_dict={input_data: x, initial_state: state_})\n",
    "                word = to_word(probs_)\n",
    "            if i % 2 == 0:\n",
    "                poem += '，'\n",
    "            else:\n",
    "                poem += '。'\n",
    "            i += 1\n",
    "        return poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(gen_poetry_with_head(u'一二三四', 'poetry.module-49'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
