{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talking Bot with Seq2Seq RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节参考 [斗大的熊猫](http://blog.topspeedsnail.com/archives/10735) ，使用 Seq2Seq RNN 模型在中文对话语料上进行 RNN 学习和生成\n",
    "\n",
    "代码参考\n",
    "\n",
    "- [使用深度学习打造智能聊天机器人](http://blog.csdn.net/malefactor/article/details/51901115)\n",
    "- [脑洞大开：基于美剧字幕的聊天语料库建设方案](http://www.shareditor.com/blogshow/?blogId=105)\n",
    "- [Seq2Seq](https://www.tensorflow.org/versions/r0.12/tutorials/seq2seq/index.html)\n",
    "\n",
    "语料来自 [中文对白语料](https://github.com/rustch3n/dgk_lost_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import codecs\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "\"\"\"\n",
    "文件格式\n",
    "E\n",
    "M 畹/华/吾/侄/\n",
    "M 你/接/到/这/封/信/的/时/候/\n",
    "M 不/知/道/大/伯/还/在/不/在/人/世/了/\n",
    "E\n",
    "M 咱/们/梅/家/从/你/爷/爷/起/\n",
    "M 就/一/直/小/心/翼/翼/地/唱/戏/\n",
    "..........\n",
    "M 就/因/为/没/穿/红/让/人/赏/咱/一/纸/枷/锁/\n",
    "M 爷/您/别/给/我/戴/这/纸/枷/锁/呀/\n",
    "E\n",
    "..........\n",
    "\"\"\"\n",
    "datafile = './data/shooter/dgk_shooter_min.conv'\n",
    "\n",
    "# 特殊标记，用来填充标记对话\n",
    "PAD = \"__PAD__\"\n",
    "GO = \"__GO__\"\n",
    "EOS = \"__EOS__\"  # 对话结束\n",
    "UNK = \"__UNK__\"  # 标记未出现在词汇表中的字符\n",
    "START_VOCABULART = [PAD, GO, EOS, UNK]    # 在词典中居前 4 位\n",
    "# 在词典中位置\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "TEST_SIZE = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_convs(datafile):\n",
    "    \"\"\"\n",
    "    返回对话数组，每个对话指两个 E 之间的部分；结果类似下面\n",
    "    [ ['畹华吾侄', '你接到这封信的时候', '不知道大伯还在不在人世了'],\n",
    "      ['咱们梅家从你爷爷起', '就一直小心翼翼地唱戏', ......],\n",
    "      ......\n",
    "    ]\n",
    "    \"\"\"\n",
    "    convs = []   # store conversation\n",
    "    with codecs.open(datafile, 'r', 'utf-8') as fp:\n",
    "        conv = []\n",
    "        for line in fp:\n",
    "            line = line.strip().replace('/', '')\n",
    "            if line == '':\n",
    "                continue\n",
    "            # end of conversation\n",
    "            if line[0] == 'E':\n",
    "                if conv:\n",
    "                    convs.append(conv)\n",
    "                conv = []\n",
    "            elif line[0] == 'M':\n",
    "                conv.append(line.split(' ')[1])\n",
    "    print \"total conversations: {}\".format(len(convs))\n",
    "    return convs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convs_to_qafile(convs):\n",
    "    \"\"\"\n",
    "    把对话拆分为问答\n",
    "    这个分法比较简单粗暴，故此最后结果也不会非常之好\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for conv in convs:\n",
    "        # 如果对话只有一句，那么无法拆分\n",
    "        if len(conv) == 1:\n",
    "            continue\n",
    "        # 如果奇数对话，那么转为偶数，扔掉最后一句\n",
    "        if len(conv) % 2 != 0:\n",
    "            conv = conv[: -1]\n",
    "        for i, sentence in enumerate(conv):\n",
    "            if i % 2 == 0:\n",
    "                questions.append(sentence)\n",
    "            else:\n",
    "                answers.append(sentence)\n",
    "    print \"Total questions/answers: {}\".format(questions)\n",
    "\n",
    "    train_enc = codecs.open('./data/shooter/train.enc', 'w', 'utf-8')\n",
    "    train_dec = codecs.open('./data/shooter/train.dec', 'w', 'utf-8')\n",
    "    test_enc = codecs.open('./data/shooter/test.enc', 'w', 'utf-8')\n",
    "    test_dec = codecs.open('./data/shooter/test.dec', 'w', 'utf-8')\n",
    "\n",
    "    vocab_enc = codecs.open('./data/shooter/vocab.enc', 'w', 'utf-8')\n",
    "    vocab_dec = codecs.open('./data/shooter/vocab.dec', 'w', 'utf-8')\n",
    "    words_enc = dd(int)\n",
    "    words_dec = dd(int)\n",
    "\n",
    "    # 取出 TEST_SIZE 个作为测试集\n",
    "    test_index = random.sample([i for i in range(len(questions))], TEST_SIZE)\n",
    "    for i, question in enumerate(questions):\n",
    "        # 分别统计 q / a 的词频\n",
    "        for w in question:\n",
    "            words_enc[w] += 1\n",
    "        for w in answers[i]:\n",
    "            words_dec[w] += 1\n",
    "\n",
    "        # 把 q / a 划分到 train / test 集\n",
    "        if i in test_index:\n",
    "            test_enc.write(question + '\\n')\n",
    "            test_dec.write(answers[i] + '\\n')\n",
    "        else:\n",
    "            train_enc.write(question + '\\n')\n",
    "            train_dec.write(answers[i] + '\\n')\n",
    "        if i % 1000 == 0:\n",
    "            print \"{} qa pairs processed\".format(i)\n",
    "    train_enc.close()\n",
    "    train_dec.close()\n",
    "    test_enc.close()\n",
    "    test_dec.close()\n",
    "\n",
    "    for words, vocabfp in [(words_enc, vocab_enc), (words_dec, vocab_dec)]:\n",
    "        # 把字符按出现次数倒序排列，并在前面加上特殊字符\n",
    "        ordered_vocab = START_VOCABULART + sorted(words, key=words.get, reverse=True)\n",
    "        # 取前 VOCAB_SIZE 个常见字，这里其实可以做更多的数据梳理\n",
    "        ordered_vocab = ordered_vocab[: VOCAB_SIZE]\n",
    "        for w in ordered_vocab:\n",
    "            vocabfp.write(w + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_vector(infile, vocabfile, outfile):\n",
    "    vocabs = []\n",
    "    with codecs.open(vocabfile, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            vocabs.append(line.strip())\n",
    "    vocabs = dict([(x, y) for (y, x) in enumerate(vocabs)])\n",
    "\n",
    "    with open(outfile, 'w') as outfp:\n",
    "        with codecs.open(infile, 'r', 'utf-8') as infp:\n",
    "            for line in infp:\n",
    "                # 把每行句子转为矢量保存\n",
    "                vec = []\n",
    "                for w in line.strip():\n",
    "                    vec.append(vocabs.get(w, UNK_ID))\n",
    "                # 索引之间空格相隔\n",
    "                outfp.write(\" \".join([str(idx) for idx in vec]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "layer_size = 256\n",
    "num_layers = 3\n",
    "batch_size = 64\n",
    "\n",
    "def s2s_read_data(enc_path, dec_path, max_size=None):\n",
    "    data_set = [[] for _ in buckets]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
