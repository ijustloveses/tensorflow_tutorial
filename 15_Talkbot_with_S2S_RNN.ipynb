{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talking Bot with Seq2Seq RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节参考 [斗大的熊猫](http://blog.topspeedsnail.com/archives/10735) ，使用 Seq2Seq RNN 模型在中文对话语料上进行 RNN 学习和生成\n",
    "\n",
    "代码参考\n",
    "\n",
    "- [使用深度学习打造智能聊天机器人](http://blog.csdn.net/malefactor/article/details/51901115)\n",
    "- [脑洞大开：基于美剧字幕的聊天语料库建设方案](http://www.shareditor.com/blogshow/?blogId=105)\n",
    "- [Seq2Seq](https://www.tensorflow.org/versions/r0.12/tutorials/seq2seq/index.html)\n",
    "\n",
    "语料来自 [中文对白语料](https://github.com/rustch3n/dgk_lost_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import codecs\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "\"\"\"\n",
    "文件格式\n",
    "E\n",
    "M 畹/华/吾/侄/\n",
    "M 你/接/到/这/封/信/的/时/候/\n",
    "M 不/知/道/大/伯/还/在/不/在/人/世/了/\n",
    "E\n",
    "M 咱/们/梅/家/从/你/爷/爷/起/\n",
    "M 就/一/直/小/心/翼/翼/地/唱/戏/\n",
    "..........\n",
    "M 就/因/为/没/穿/红/让/人/赏/咱/一/纸/枷/锁/\n",
    "M 爷/您/别/给/我/戴/这/纸/枷/锁/呀/\n",
    "E\n",
    "..........\n",
    "\"\"\"\n",
    "datafile = './data/shooter/dgk_shooter_min.conv'\n",
    "\n",
    "# 特殊标记，用来填充标记对话\n",
    "PAD = \"__PAD__\"\n",
    "GO = \"__GO__\"\n",
    "EOS = \"__EOS__\"  # 对话结束\n",
    "UNK = \"__UNK__\"  # 标记未出现在词汇表中的字符\n",
    "START_VOCABULART = [PAD, GO, EOS, UNK]    # 在词典中居前 4 位\n",
    "# 在词典中位置\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "TEST_SIZE = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_convs(datafile):\n",
    "    \"\"\"\n",
    "    返回对话数组，每个对话指两个 E 之间的部分；结果类似下面\n",
    "    [ ['畹华吾侄', '你接到这封信的时候', '不知道大伯还在不在人世了'],\n",
    "      ['咱们梅家从你爷爷起', '就一直小心翼翼地唱戏', ......],\n",
    "      ......\n",
    "    ]\n",
    "    \"\"\"\n",
    "    convs = []   # store conversation\n",
    "    with codecs.open(datafile, 'r', 'utf-8') as fp:\n",
    "        conv = []\n",
    "        for line in fp:\n",
    "            line = line.strip().replace('/', '')\n",
    "            if line == '':\n",
    "                continue\n",
    "            # end of conversation\n",
    "            if line[0] == 'E':\n",
    "                if conv:\n",
    "                    convs.append(conv)\n",
    "                conv = []\n",
    "            elif line[0] == 'M':\n",
    "                conv.append(line.split(' ')[1])\n",
    "    print \"total conversations: {}\".format(len(convs))\n",
    "    return convs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convs_to_qafile(convs):\n",
    "    \"\"\"\n",
    "    把对话拆分为问答\n",
    "    这个分法比较简单粗暴，故此最后结果也不会非常之好\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for conv in convs:\n",
    "        # 如果对话只有一句，那么无法拆分\n",
    "        if len(conv) == 1:\n",
    "            continue\n",
    "        # 如果奇数对话，那么转为偶数，扔掉最后一句\n",
    "        if len(conv) % 2 != 0:\n",
    "            conv = conv[: -1]\n",
    "        for i, sentence in enumerate(conv):\n",
    "            if i % 2 == 0:\n",
    "                questions.append(sentence)\n",
    "            else:\n",
    "                answers.append(sentence)\n",
    "    print \"Total questions/answers: {}\".format(len(questions))\n",
    "\n",
    "    train_enc = codecs.open('./data/shooter/train.enc', 'w', 'utf-8')\n",
    "    train_dec = codecs.open('./data/shooter/train.dec', 'w', 'utf-8')\n",
    "    test_enc = codecs.open('./data/shooter/test.enc', 'w', 'utf-8')\n",
    "    test_dec = codecs.open('./data/shooter/test.dec', 'w', 'utf-8')\n",
    "\n",
    "    vocab_enc = codecs.open('./data/shooter/vocab.enc', 'w', 'utf-8')\n",
    "    vocab_dec = codecs.open('./data/shooter/vocab.dec', 'w', 'utf-8')\n",
    "    # questions 和 answers 各自做一个词典，而不是公用一个词典\n",
    "    words_enc = dd(int)\n",
    "    words_dec = dd(int)\n",
    "\n",
    "    # 取出 TEST_SIZE 个作为测试集\n",
    "    test_index = random.sample([i for i in range(len(questions))], TEST_SIZE)\n",
    "    for i, question in enumerate(questions):\n",
    "        # 分别统计 q / a 的词频\n",
    "        for w in question:\n",
    "            words_enc[w] += 1\n",
    "        for w in answers[i]:\n",
    "            words_dec[w] += 1\n",
    "\n",
    "        # 把 q / a 划分到 train / test 集\n",
    "        if i in test_index:\n",
    "            test_enc.write(question + '\\n')\n",
    "            test_dec.write(answers[i] + '\\n')\n",
    "        else:\n",
    "            train_enc.write(question + '\\n')\n",
    "            train_dec.write(answers[i] + '\\n')\n",
    "        if i % 1000 == 0:\n",
    "            print \"{} qa pairs processed\".format(i)\n",
    "    train_enc.close()\n",
    "    train_dec.close()\n",
    "    test_enc.close()\n",
    "    test_dec.close()\n",
    "\n",
    "    for words, vocabfp in [(words_enc, vocab_enc), (words_dec, vocab_dec)]:\n",
    "        # 把字符按出现次数倒序排列，并在前面加上特殊字符\n",
    "        ordered_vocab = START_VOCABULART + sorted(words, key=words.get, reverse=True)\n",
    "        # 取前 VOCAB_SIZE 个常见字，这里其实可以做更多的数据梳理\n",
    "        ordered_vocab = ordered_vocab[: VOCAB_SIZE]\n",
    "        for w in ordered_vocab:\n",
    "            vocabfp.write(w + '\\n')\n",
    "\n",
    "    vocab_enc.close()\n",
    "    vocab_dec.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_vector(infile, vocabfile, outfile):\n",
    "    vocabs = []\n",
    "    with codecs.open(vocabfile, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            vocabs.append(line.strip())\n",
    "    vocabs = dict([(x, y) for (y, x) in enumerate(vocabs)])\n",
    "\n",
    "    with open(outfile, 'w') as outfp:\n",
    "        with codecs.open(infile, 'r', 'utf-8') as infp:\n",
    "            for line in infp:\n",
    "                # 把每行句子转为矢量保存\n",
    "                vec = []\n",
    "                for w in line.strip():\n",
    "                    vec.append(vocabs.get(w, UNK_ID))\n",
    "                # 索引之间空格相隔\n",
    "                outfp.write(\" \".join([str(idx) for idx in vec]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convs_to_qafile(get_convs(datafile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total conversations: 762716\n",
    "```\n",
    "$ wc -l data/shooter/*\n",
    "  4268084 data/shooter/dgk_shooter_min.conv\n",
    "     8000 data/shooter/test.dec\n",
    "     8000 data/shooter/test.enc\n",
    "  1538628 data/shooter/train.dec\n",
    "  1538628 data/shooter/train.enc\n",
    "     5000 data/shooter/vocab.dec\n",
    "     5000 data/shooter/vocab.enc\n",
    "  7371340 total\n",
    "\n",
    "```\n",
    "看到词典都是 5000 个字符；测试集的问答都是 8000 句；训练集的问答都是 1538628 句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecterize train encode file\n",
      "vecterize train decode file\n",
      "vecterize test encode file\n",
      "vecterize test decode file\n"
     ]
    }
   ],
   "source": [
    "print \"vecterize train encode file\"\n",
    "convert_to_vector(\"data/shooter/train.enc\", \"data/shooter/vocab.enc\", 'data/shooter/train_encode.vec')\n",
    "print \"vecterize train decode file\"\n",
    "convert_to_vector(\"data/shooter/train.dec\", \"data/shooter/vocab.dec\", 'data/shooter/train_decode.vec')\n",
    "print \"vecterize test encode file\" \n",
    "convert_to_vector(\"data/shooter/test.enc\", \"data/shooter/vocab.enc\", 'data/shooter/test_encode.vec')\n",
    "print \"vecterize test decode file\"\n",
    "convert_to_vector(\"data/shooter/test.dec\", \"data/shooter/vocab.dec\", 'data/shooter/test_decode.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ wc -l data/shooter/*.vec\n",
    "    8000 test_decode.vec\n",
    "    8000 test_encode.vec\n",
    " 1538628 train_decode.vec\n",
    " 1538628 train_encode.vec\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "layer_size = 256\n",
    "num_layers = 3\n",
    "batch_size = 64\n",
    "\n",
    "def s2s_read_data(enc_path, dec_path, max_size=None):\n",
    "    \"\"\" 每个桶装对应长度的数据 \"\"\"\n",
    "    data_set = [[] for _ in buckets]\n",
    "    with tf.gfile.GFile(enc_path, mode='r') as ef:\n",
    "        with tf.gfile.GFile(dec_path, mode='r') as df:\n",
    "            source, target = sf.readline(), df.readline()\n",
    "            counter = 0\n",
    "            while source and target and (not max_size or counter < max_size):\n",
    "                counter += 1\n",
    "                source_ids = [int(x) for x in source.split()]\n",
    "                target_ids = [int(x) for x in target.split()]\n",
    "                # 结束标志 EOS_ID\n",
    "                target_ids.append(EOS_ID)\n",
    "                for bucket_id, (source_size, target_size) in enumerate(buckets):\n",
    "                    if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "                        data_set[bucket_id].append([source_ids, target_ids])\n",
    "                        break\n",
    "                source, target = sf.readline(), df.readline()\n",
    "    return data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "\n",
    "model = seq2seq_model.Seq2SeqModel(source_vocab_size=VOCAB_SIZE, target_vocal_size=VOCAB_SIZE, buckets=buckets,\n",
    "                                   size=layer_size, num_layers=num_layers, max_gradient_norm=5.0, batch_size=batch_size,\n",
    "                                   learning_rate=0.5, learning_rate_decay_factor=0.97, forward_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "sess = tf.Session()\n",
    "ckpt = tf.train.get_checkpoint_state('.')\n",
    "if ckpt != None:\n",
    "    print(ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "train_set = s2s_read_data(# TODO)\n",
    "test_set = s2s_read_data(# TODO)\n",
    "\n",
    "# say [3, 5, 2]\n",
    "train_bucket_sizes = [len(train_set[i]) for i in range(len(buckets))]\n",
    "# ==> 3 + 5 + 2 = 10.0\n",
    "train_total_size = float(sum(train_bucket_sizes))\n",
    "# ==> [0.3, 0.8, 1.0]\n",
    "train_buckets_scale = [sum(train_bucket_sizes[: i + 1]) / train_total_size for i in range(train_bucket_sizes)]\n",
    "print \"train_bucket_sizes: {}\".format(train_bucket_sizes)\n",
    "print \"train_total_size: {}\".format(train_total_size)\n",
    "print \"train_buckets_scale: {}\".format(train_buckets_scale)\n",
    "\n",
    "loss = 0.0\n",
    "total_step = 0\n",
    "previous_losses = []\n",
    "# 持续训练\n",
    "print \"start training ...\"\n",
    "while True:\n",
    "    # 随机出一个 0~1 的小数\n",
    "    random_number_01 = np.random.random_sample()\n",
    "    # 找到这个随机数在 scale 中的位置，返回这个位置，也就是说，这里根据随机数找到一个桶\n",
    "    bucket_id = min([i for i in range(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\n",
    "    # 从对应的桶 id 中，读取一个 batch 的训练 enc & dec 文本，以及权重，这个由 seq2seq model 提供函数实现\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\n",
    "    # 进行训练\n",
    "    _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "\n",
    "    loss += step_loss / 500\n",
    "    total_step += 1\n",
    "\n",
    "    if total_step % 500 == 0:\n",
    "        print \"----- total_step: {} -----\".format(total_step)\n",
    "        print \"global step, learning rate & loss: \".format(model.global_step.eval(), model.learning_rate.eval(), loss)\n",
    "\n",
    "        # 如果模型没有得到提升，减小learning rate\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "            sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "\n",
    "        checkpoint_path = \"chatbot_seq2seq.ckpt\"\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "        loss = 0.0\n",
    "\n",
    "        # 通过测试数据评估\n",
    "        for bid in range(len(buckets)):\n",
    "            if len(test_set[bid]) == 0:\n",
    "                continue\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(test_set, bucket_id)\n",
    "            # 这里最后一个参数是 True，而前面训练时为 False\n",
    "            _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "            eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\n",
    "            print \"test bucket id & eval_ppx: \".format(bucket_id, eval_ppx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
