{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talking Bot with Seq2Seq RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节参考 [斗大的熊猫](http://blog.topspeedsnail.com/archives/10735) ，使用 Seq2Seq RNN 模型在中文对话语料上进行 RNN 学习和生成\n",
    "\n",
    "代码参考\n",
    "\n",
    "- [使用深度学习打造智能聊天机器人](http://blog.csdn.net/malefactor/article/details/51901115)\n",
    "- [脑洞大开：基于美剧字幕的聊天语料库建设方案](http://www.shareditor.com/blogshow/?blogId=105)\n",
    "- [Seq2Seq](https://www.tensorflow.org/versions/r0.12/tutorials/seq2seq/index.html)\n",
    "\n",
    "语料来自 [中文对白语料](https://github.com/rustch3n/dgk_lost_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import codecs\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "\"\"\"\n",
    "文件格式\n",
    "E\n",
    "M 畹/华/吾/侄/\n",
    "M 你/接/到/这/封/信/的/时/候/\n",
    "M 不/知/道/大/伯/还/在/不/在/人/世/了/\n",
    "E\n",
    "M 咱/们/梅/家/从/你/爷/爷/起/\n",
    "M 就/一/直/小/心/翼/翼/地/唱/戏/\n",
    "..........\n",
    "M 就/因/为/没/穿/红/让/人/赏/咱/一/纸/枷/锁/\n",
    "M 爷/您/别/给/我/戴/这/纸/枷/锁/呀/\n",
    "E\n",
    "..........\n",
    "\"\"\"\n",
    "datafile = './data/shooter/dgk_shooter_min.conv'\n",
    "\n",
    "# 特殊标记，用来填充标记对话\n",
    "PAD = \"__PAD__\"\n",
    "GO = \"__GO__\"\n",
    "EOS = \"__EOS__\"  # 对话结束\n",
    "UNK = \"__UNK__\"  # 标记未出现在词汇表中的字符\n",
    "START_VOCABULART = [PAD, GO, EOS, UNK]    # 在词典中居前 4 位\n",
    "# 在词典中位置\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "TEST_SIZE = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_convs(datafile):\n",
    "    \"\"\"\n",
    "    返回对话数组，每个对话指两个 E 之间的部分；结果类似下面\n",
    "    [ ['畹华吾侄', '你接到这封信的时候', '不知道大伯还在不在人世了'],\n",
    "      ['咱们梅家从你爷爷起', '就一直小心翼翼地唱戏', ......],\n",
    "      ......\n",
    "    ]\n",
    "    \"\"\"\n",
    "    convs = []   # store conversation\n",
    "    with codecs.open(datafile, 'r', 'utf-8') as fp:\n",
    "        conv = []\n",
    "        for line in fp:\n",
    "            line = line.strip().replace('/', '')\n",
    "            if line == '':\n",
    "                continue\n",
    "            # end of conversation\n",
    "            if line[0] == 'E':\n",
    "                if conv:\n",
    "                    convs.append(conv)\n",
    "                conv = []\n",
    "            elif line[0] == 'M':\n",
    "                conv.append(line.split(' ')[1])\n",
    "    print \"total conversations: {}\".format(len(convs))\n",
    "    return convs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convs_to_qafile(convs):\n",
    "    \"\"\"\n",
    "    把对话拆分为问答\n",
    "    这个分法比较简单粗暴，故此最后结果也不会非常之好\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for conv in convs:\n",
    "        # 如果对话只有一句，那么无法拆分\n",
    "        if len(conv) == 1:\n",
    "            continue\n",
    "        # 如果奇数对话，那么转为偶数，扔掉最后一句\n",
    "        if len(conv) % 2 != 0:\n",
    "            conv = conv[: -1]\n",
    "        for i, sentence in enumerate(conv):\n",
    "            if i % 2 == 0:\n",
    "                questions.append(sentence)\n",
    "            else:\n",
    "                answers.append(sentence)\n",
    "    print \"Total questions/answers: {}\".format(len(questions))\n",
    "\n",
    "    train_enc = codecs.open('./data/shooter/train.enc', 'w', 'utf-8')\n",
    "    train_dec = codecs.open('./data/shooter/train.dec', 'w', 'utf-8')\n",
    "    test_enc = codecs.open('./data/shooter/test.enc', 'w', 'utf-8')\n",
    "    test_dec = codecs.open('./data/shooter/test.dec', 'w', 'utf-8')\n",
    "\n",
    "    vocab_enc = codecs.open('./data/shooter/vocab.enc', 'w', 'utf-8')\n",
    "    vocab_dec = codecs.open('./data/shooter/vocab.dec', 'w', 'utf-8')\n",
    "    # questions 和 answers 各自做一个词典，而不是公用一个词典\n",
    "    words_enc = dd(int)\n",
    "    words_dec = dd(int)\n",
    "\n",
    "    # 取出 TEST_SIZE 个作为测试集\n",
    "    test_index = random.sample([i for i in range(len(questions))], TEST_SIZE)\n",
    "    for i, question in enumerate(questions):\n",
    "        # 分别统计 q / a 的词频\n",
    "        for w in question:\n",
    "            words_enc[w] += 1\n",
    "        for w in answers[i]:\n",
    "            words_dec[w] += 1\n",
    "\n",
    "        # 把 q / a 划分到 train / test 集\n",
    "        if i in test_index:\n",
    "            test_enc.write(question + '\\n')\n",
    "            test_dec.write(answers[i] + '\\n')\n",
    "        else:\n",
    "            train_enc.write(question + '\\n')\n",
    "            train_dec.write(answers[i] + '\\n')\n",
    "        if i % 1000 == 0:\n",
    "            print \"{} qa pairs processed\".format(i)\n",
    "    train_enc.close()\n",
    "    train_dec.close()\n",
    "    test_enc.close()\n",
    "    test_dec.close()\n",
    "\n",
    "    for words, vocabfp in [(words_enc, vocab_enc), (words_dec, vocab_dec)]:\n",
    "        # 把字符按出现次数倒序排列，并在前面加上特殊字符\n",
    "        ordered_vocab = START_VOCABULART + sorted(words, key=words.get, reverse=True)\n",
    "        # 取前 VOCAB_SIZE 个常见字，这里其实可以做更多的数据梳理\n",
    "        ordered_vocab = ordered_vocab[: VOCAB_SIZE]\n",
    "        for w in ordered_vocab:\n",
    "            vocabfp.write(w + '\\n')\n",
    "\n",
    "    vocab_enc.close()\n",
    "    vocab_dec.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_vector(infile, vocabfile, outfile):\n",
    "    vocabs = []\n",
    "    with codecs.open(vocabfile, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            vocabs.append(line.strip())\n",
    "    vocabs = dict([(x, y) for (y, x) in enumerate(vocabs)])\n",
    "\n",
    "    with open(outfile, 'w') as outfp:\n",
    "        with codecs.open(infile, 'r', 'utf-8') as infp:\n",
    "            for line in infp:\n",
    "                # 把每行句子转为矢量保存\n",
    "                vec = []\n",
    "                for w in line.strip():\n",
    "                    vec.append(vocabs.get(w, UNK_ID))\n",
    "                # 索引之间空格相隔\n",
    "                outfp.write(\" \".join([str(idx) for idx in vec]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convs_to_qafile(get_convs(datafile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total conversations: 762716\n",
    "```\n",
    "$ wc -l data/shooter/*\n",
    "  4268084 data/shooter/dgk_shooter_min.conv\n",
    "     8000 data/shooter/test.dec\n",
    "     8000 data/shooter/test.enc\n",
    "  1538628 data/shooter/train.dec\n",
    "  1538628 data/shooter/train.enc\n",
    "     5000 data/shooter/vocab.dec\n",
    "     5000 data/shooter/vocab.enc\n",
    "  7371340 total\n",
    "\n",
    "```\n",
    "看到词典都是 5000 个字符；测试集的问答都是 8000 句；训练集的问答都是 1538628 句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecterize train encode file\n",
      "vecterize train decode file\n",
      "vecterize test encode file\n",
      "vecterize test decode file\n"
     ]
    }
   ],
   "source": [
    "print \"vecterize train encode file\"\n",
    "convert_to_vector(\"data/shooter/train.enc\", \"data/shooter/vocab.enc\", 'data/shooter/train_encode.vec')\n",
    "print \"vecterize train decode file\"\n",
    "convert_to_vector(\"data/shooter/train.dec\", \"data/shooter/vocab.dec\", 'data/shooter/train_decode.vec')\n",
    "print \"vecterize test encode file\" \n",
    "convert_to_vector(\"data/shooter/test.enc\", \"data/shooter/vocab.enc\", 'data/shooter/test_encode.vec')\n",
    "print \"vecterize test decode file\"\n",
    "convert_to_vector(\"data/shooter/test.dec\", \"data/shooter/vocab.dec\", 'data/shooter/test_decode.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ wc -l data/shooter/*.vec\n",
    "    8000 test_decode.vec\n",
    "    8000 test_encode.vec\n",
    " 1538628 train_decode.vec\n",
    " 1538628 train_encode.vec\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "layer_size = 256\n",
    "num_layers = 3\n",
    "batch_size = 64\n",
    "\n",
    "def s2s_read_data(enc_path, dec_path, max_size=None):\n",
    "    \"\"\" 每个桶装对应长度的数据 \"\"\"\n",
    "    data_set = [[] for _ in buckets]\n",
    "    with tf.gfile.GFile(enc_path, mode='r') as ef:\n",
    "        with tf.gfile.GFile(dec_path, mode='r') as df:\n",
    "            source, target = ef.readline(), df.readline()\n",
    "            counter = 0\n",
    "            while source and target and (not max_size or counter < max_size):\n",
    "                counter += 1\n",
    "                source_ids = [int(x) for x in source.split()]\n",
    "                target_ids = [int(x) for x in target.split()]\n",
    "                # 结束标志 EOS_ID\n",
    "                target_ids.append(EOS_ID)\n",
    "                for bucket_id, (source_size, target_size) in enumerate(buckets):\n",
    "                    if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "                        data_set[bucket_id].append([source_ids, target_ids])\n",
    "                        break\n",
    "                source, target = ef.readline(), df.readline()\n",
    "    return data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "\n",
    "model = seq2seq_model.Seq2SeqModel(VOCAB_SIZE, VOCAB_SIZE, buckets=buckets,\n",
    "                                   size=layer_size, num_layers=num_layers, max_gradient_norm=5.0, batch_size=batch_size,\n",
    "                                   learning_rate=0.5, learning_rate_decay_factor=0.97, forward_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_bucket_sizes: [274762, 664553, 575636, 23211]\n",
      "train_total_size: 1538162\n",
      "train_buckets_scale: [0.17863007927643512, 0.6106736481592966, 0.9849099119598586, 1.0]\n",
      "start training ...\n",
      "----- total_step: 100 -----\n",
      "global step, learning rate & loss: 100, 0.5, 7.01408500671\n",
      "test bucket id:  0     eval_ppx: 257.649460128\n",
      "test bucket id:  1     eval_ppx: 238.252258232\n",
      "test bucket id:  2     eval_ppx: 214.257065068\n",
      "test bucket id:  3     eval_ppx: 213.714537918\n",
      "----- total_step: 200 -----\n",
      "global step, learning rate & loss: 200, 0.5, 5.72609845161\n",
      "test bucket id:  0     eval_ppx: 160.29752383\n",
      "test bucket id:  1     eval_ppx: 149.999082452\n",
      "test bucket id:  2     eval_ppx: 139.566065155\n",
      "test bucket id:  3     eval_ppx: 132.77818862\n",
      "----- total_step: 300 -----\n",
      "global step, learning rate & loss: 300, 0.5, 5.42118005276\n",
      "test bucket id:  0     eval_ppx: 174.560686236\n",
      "test bucket id:  1     eval_ppx: 180.965441786\n",
      "test bucket id:  2     eval_ppx: 166.261146333\n",
      "test bucket id:  3     eval_ppx: 195.680933626\n",
      "----- total_step: 400 -----\n",
      "global step, learning rate & loss: 400, 0.5, 5.27021847248\n",
      "test bucket id:  0     eval_ppx: 216.101592841\n",
      "test bucket id:  1     eval_ppx: 182.285654946\n",
      "test bucket id:  2     eval_ppx: 224.364059158\n",
      "test bucket id:  3     eval_ppx: 221.642100064\n",
      "----- total_step: 500 -----\n",
      "global step, learning rate & loss: 500, 0.5, 5.20756977081\n",
      "test bucket id:  0     eval_ppx: 228.886468197\n",
      "test bucket id:  1     eval_ppx: 224.177661564\n",
      "test bucket id:  2     eval_ppx: 218.03538867\n",
      "test bucket id:  3     eval_ppx: 191.293241158\n",
      "----- total_step: 600 -----\n",
      "global step, learning rate & loss: 600, 0.5, 5.13428425789\n",
      "test bucket id:  0     eval_ppx: 104.651443507\n",
      "test bucket id:  1     eval_ppx: 116.986466604\n",
      "test bucket id:  2     eval_ppx: 114.295333559\n",
      "test bucket id:  3     eval_ppx: 113.983310241\n",
      "----- total_step: 700 -----\n",
      "global step, learning rate & loss: 700, 0.5, 5.12845748425\n",
      "test bucket id:  0     eval_ppx: 164.423216965\n",
      "test bucket id:  1     eval_ppx: 172.532751447\n",
      "test bucket id:  2     eval_ppx: 168.304824647\n",
      "test bucket id:  3     eval_ppx: 197.848072943\n",
      "----- total_step: 800 -----\n",
      "global step, learning rate & loss: 800, 0.5, 5.07749822617\n",
      "test bucket id:  0     eval_ppx: 125.670208082\n",
      "test bucket id:  1     eval_ppx: 165.969337064\n",
      "test bucket id:  2     eval_ppx: 143.454472935\n",
      "test bucket id:  3     eval_ppx: 147.864231565\n",
      "----- total_step: 900 -----\n",
      "global step, learning rate & loss: 900, 0.5, 5.07783394337\n",
      "test bucket id:  0     eval_ppx: 198.742936774\n",
      "test bucket id:  1     eval_ppx: 163.793594214\n",
      "test bucket id:  2     eval_ppx: 190.803402432\n",
      "test bucket id:  3     eval_ppx: 150.645269438\n",
      "----- total_step: 1000 -----\n",
      "global step, learning rate & loss: 1000, 0.5, 5.06357561588\n",
      "test bucket id:  0     eval_ppx: 190.517568857\n",
      "test bucket id:  1     eval_ppx: 172.722817553\n",
      "test bucket id:  2     eval_ppx: 184.395864604\n",
      "test bucket id:  3     eval_ppx: 193.705565401\n",
      "----- total_step: 1100 -----\n",
      "global step, learning rate & loss: 1100, 0.5, 4.97391727924\n",
      "test bucket id:  0     eval_ppx: 157.598322581\n",
      "test bucket id:  1     eval_ppx: 190.935463215\n",
      "test bucket id:  2     eval_ppx: 192.214084008\n",
      "test bucket id:  3     eval_ppx: 194.592919894\n",
      "----- total_step: 1200 -----\n",
      "global step, learning rate & loss: 1200, 0.5, 4.96236652374\n",
      "test bucket id:  0     eval_ppx: 140.861585922\n",
      "test bucket id:  1     eval_ppx: 145.197316979\n",
      "test bucket id:  2     eval_ppx: 130.91895609\n",
      "test bucket id:  3     eval_ppx: 136.748472661\n",
      "----- total_step: 1300 -----\n",
      "global step, learning rate & loss: 1300, 0.5, 5.01042954922\n",
      "test bucket id:  0     eval_ppx: 176.99369002\n",
      "test bucket id:  1     eval_ppx: 191.399993332\n",
      "test bucket id:  2     eval_ppx: 188.377540453\n",
      "test bucket id:  3     eval_ppx: 172.100553344\n",
      "----- total_step: 1400 -----\n",
      "global step, learning rate & loss: 1400, 0.5, 4.97323634148\n",
      "test bucket id:  0     eval_ppx: 119.725205821\n",
      "test bucket id:  1     eval_ppx: 122.521984791\n",
      "test bucket id:  2     eval_ppx: 135.092998746\n",
      "test bucket id:  3     eval_ppx: 131.819301462\n",
      "----- total_step: 1500 -----\n",
      "global step, learning rate & loss: 1500, 0.5, 4.94378592014\n",
      "test bucket id:  0     eval_ppx: 151.773809824\n",
      "test bucket id:  1     eval_ppx: 143.521388062\n",
      "test bucket id:  2     eval_ppx: 152.607784789\n",
      "test bucket id:  3     eval_ppx: 179.913430898\n",
      "----- total_step: 1600 -----\n",
      "global step, learning rate & loss: 1600, 0.5, 4.97738465309\n",
      "test bucket id:  0     eval_ppx: 132.850829152\n",
      "test bucket id:  1     eval_ppx: 108.640634231\n",
      "test bucket id:  2     eval_ppx: 135.214544536\n",
      "test bucket id:  3     eval_ppx: 128.454603469\n",
      "----- total_step: 1700 -----\n",
      "global step, learning rate & loss: 1700, 0.5, 4.90571424484\n",
      "test bucket id:  0     eval_ppx: 135.785256077\n",
      "test bucket id:  1     eval_ppx: 123.78796435\n",
      "test bucket id:  2     eval_ppx: 128.067585461\n",
      "test bucket id:  3     eval_ppx: 134.415807172\n",
      "----- total_step: 1800 -----\n",
      "global step, learning rate & loss: 1800, 0.5, 4.97480005741\n",
      "test bucket id:  0     eval_ppx: 173.858416894\n",
      "test bucket id:  1     eval_ppx: 175.325724526\n",
      "test bucket id:  2     eval_ppx: 167.92436985\n",
      "test bucket id:  3     eval_ppx: 172.972550711\n",
      "----- total_step: 1900 -----\n",
      "global step, learning rate & loss: 1900, 0.5, 4.96046104908\n",
      "test bucket id:  0     eval_ppx: 113.65728806\n",
      "test bucket id:  1     eval_ppx: 110.843563479\n",
      "test bucket id:  2     eval_ppx: 133.033460768\n",
      "test bucket id:  3     eval_ppx: 133.016397768\n",
      "----- total_step: 2000 -----\n",
      "global step, learning rate & loss: 2000, 0.5, 4.91311383247\n",
      "test bucket id:  0     eval_ppx: 171.60609122\n",
      "test bucket id:  1     eval_ppx: 169.852371526\n",
      "test bucket id:  2     eval_ppx: 201.101477289\n",
      "test bucket id:  3     eval_ppx: 166.327913068\n",
      "----- total_step: 2100 -----\n",
      "global step, learning rate & loss: 2100, 0.5, 4.95443518162\n",
      "test bucket id:  0     eval_ppx: 146.459184863\n",
      "test bucket id:  1     eval_ppx: 182.722518883\n",
      "test bucket id:  2     eval_ppx: 183.108296618\n",
      "test bucket id:  3     eval_ppx: 144.435103445\n",
      "----- total_step: 2200 -----\n",
      "global step, learning rate & loss: 2200, 0.5, 4.91126796722\n",
      "test bucket id:  0     eval_ppx: 134.60578907\n",
      "test bucket id:  1     eval_ppx: 144.492691935\n",
      "test bucket id:  2     eval_ppx: 128.262171688\n",
      "test bucket id:  3     eval_ppx: 133.02115489\n",
      "----- total_step: 2300 -----\n",
      "global step, learning rate & loss: 2300, 0.5, 4.88919819355\n",
      "test bucket id:  0     eval_ppx: 143.303720448\n",
      "test bucket id:  1     eval_ppx: 186.126953316\n",
      "test bucket id:  2     eval_ppx: 189.268372815\n",
      "test bucket id:  3     eval_ppx: 151.301021725\n",
      "----- total_step: 2400 -----\n",
      "global step, learning rate & loss: 2400, 0.5, 4.88627801895\n",
      "test bucket id:  0     eval_ppx: 195.600891788\n",
      "test bucket id:  1     eval_ppx: 162.95318387\n",
      "test bucket id:  2     eval_ppx: 141.502479211\n",
      "test bucket id:  3     eval_ppx: 163.643237136\n",
      "----- total_step: 2500 -----\n",
      "global step, learning rate & loss: 2500, 0.5, 4.86819467545\n",
      "test bucket id:  0     eval_ppx: 174.224898521\n",
      "test bucket id:  1     eval_ppx: 170.440904203\n",
      "test bucket id:  2     eval_ppx: 172.497790228\n",
      "test bucket id:  3     eval_ppx: 147.481099335\n",
      "----- total_step: 2600 -----\n",
      "global step, learning rate & loss: 2600, 0.5, 4.87763298988\n",
      "test bucket id:  0     eval_ppx: 171.997839955\n",
      "test bucket id:  1     eval_ppx: 156.522201215\n",
      "test bucket id:  2     eval_ppx: 192.690729138\n",
      "test bucket id:  3     eval_ppx: 165.284606629\n",
      "----- total_step: 2700 -----\n",
      "global step, learning rate & loss: 2700, 0.5, 4.8489666748\n",
      "test bucket id:  0     eval_ppx: 158.241174903\n",
      "test bucket id:  1     eval_ppx: 168.818027254\n",
      "test bucket id:  2     eval_ppx: 142.702225009\n",
      "test bucket id:  3     eval_ppx: 168.683728357\n",
      "----- total_step: 2800 -----\n",
      "global step, learning rate & loss: 2800, 0.5, 4.83233982086\n",
      "test bucket id:  0     eval_ppx: 52.3347920733\n",
      "test bucket id:  1     eval_ppx: 77.6432187006\n",
      "test bucket id:  2     eval_ppx: 64.3209014427\n",
      "test bucket id:  3     eval_ppx: 57.8497383288\n",
      "----- total_step: 2900 -----\n",
      "global step, learning rate & loss: 2900, 0.5, 4.75263707161\n",
      "test bucket id:  0     eval_ppx: 112.638498182\n",
      "test bucket id:  1     eval_ppx: 98.1488210622\n",
      "test bucket id:  2     eval_ppx: 108.394430714\n",
      "test bucket id:  3     eval_ppx: 116.375342505\n",
      "----- total_step: 3000 -----\n",
      "global step, learning rate & loss: 3000, 0.5, 4.69035120964\n",
      "test bucket id:  0     eval_ppx: 132.667499342\n",
      "test bucket id:  1     eval_ppx: 107.690174158\n",
      "test bucket id:  2     eval_ppx: 159.204945406\n",
      "test bucket id:  3     eval_ppx: 146.480696295\n",
      "----- total_step: 3100 -----\n",
      "global step, learning rate & loss: 3100, 0.5, 4.65867665529\n",
      "test bucket id:  0     eval_ppx: 124.878298271\n",
      "test bucket id:  1     eval_ppx: 152.94916187\n",
      "test bucket id:  2     eval_ppx: 133.666460363\n",
      "test bucket id:  3     eval_ppx: 131.202806899\n",
      "----- total_step: 3200 -----\n",
      "global step, learning rate & loss: 3200, 0.5, 4.67894068003\n",
      "test bucket id:  0     eval_ppx: 114.783989029\n",
      "test bucket id:  1     eval_ppx: 85.6251719938\n",
      "test bucket id:  2     eval_ppx: 121.970896509\n",
      "test bucket id:  3     eval_ppx: 91.2780890606\n",
      "----- total_step: 3300 -----\n",
      "global step, learning rate & loss: 3300, 0.5, 4.62750264168\n",
      "test bucket id:  0     eval_ppx: 133.263167762\n",
      "test bucket id:  1     eval_ppx: 142.327038648\n",
      "test bucket id:  2     eval_ppx: 138.862742995\n",
      "test bucket id:  3     eval_ppx: 139.152602926\n",
      "----- total_step: 3400 -----\n",
      "global step, learning rate & loss: 3400, 0.5, 4.57317322493\n",
      "test bucket id:  0     eval_ppx: 54.0496448435\n",
      "test bucket id:  1     eval_ppx: 54.8025374093\n",
      "test bucket id:  2     eval_ppx: 49.4672660613\n",
      "test bucket id:  3     eval_ppx: 74.2561695931\n",
      "----- total_step: 3500 -----\n",
      "global step, learning rate & loss: 3500, 0.5, 4.6242885828\n",
      "test bucket id:  0     eval_ppx: 111.120707501\n",
      "test bucket id:  1     eval_ppx: 104.33007202\n",
      "test bucket id:  2     eval_ppx: 106.370906676\n",
      "test bucket id:  3     eval_ppx: 94.1492102638\n",
      "----- total_step: 3600 -----\n",
      "global step, learning rate & loss: 3600, 0.5, 4.57967345715\n",
      "test bucket id:  0     eval_ppx: 96.0518356554\n",
      "test bucket id:  1     eval_ppx: 97.5474993079\n",
      "test bucket id:  2     eval_ppx: 101.385767769\n",
      "test bucket id:  3     eval_ppx: 102.1112951\n",
      "----- total_step: 3700 -----\n",
      "global step, learning rate & loss: 3700, 0.5, 4.56412495613\n",
      "test bucket id:  0     eval_ppx: 114.509938814\n",
      "test bucket id:  1     eval_ppx: 118.908088612\n",
      "test bucket id:  2     eval_ppx: 138.904332189\n",
      "test bucket id:  3     eval_ppx: 129.698365206\n",
      "----- total_step: 3800 -----\n",
      "global step, learning rate & loss: 3800, 0.5, 4.63075876951\n",
      "test bucket id:  0     eval_ppx: 63.4774871752\n",
      "test bucket id:  1     eval_ppx: 55.905142371\n",
      "test bucket id:  2     eval_ppx: 63.1879359264\n",
      "test bucket id:  3     eval_ppx: 55.1769214903\n",
      "----- total_step: 3900 -----\n",
      "global step, learning rate & loss: 3900, 0.485000014305, 4.58464724779\n",
      "test bucket id:  0     eval_ppx: 86.6332964284\n",
      "test bucket id:  1     eval_ppx: 94.5985892226\n",
      "test bucket id:  2     eval_ppx: 81.9535583487\n",
      "test bucket id:  3     eval_ppx: 125.618204618\n",
      "----- total_step: 4000 -----\n",
      "global step, learning rate & loss: 4000, 0.485000014305, 4.5441268754\n",
      "test bucket id:  0     eval_ppx: 92.4829468463\n",
      "test bucket id:  1     eval_ppx: 99.9011592482\n",
      "test bucket id:  2     eval_ppx: 96.9676153062\n",
      "test bucket id:  3     eval_ppx: 110.165936567\n",
      "----- total_step: 4100 -----\n",
      "global step, learning rate & loss: 4100, 0.485000014305, 4.53524596453\n",
      "test bucket id:  0     eval_ppx: 105.240487944\n",
      "test bucket id:  1     eval_ppx: 145.222590127\n",
      "test bucket id:  2     eval_ppx: 120.1660608\n",
      "test bucket id:  3     eval_ppx: 118.330195651\n",
      "----- total_step: 4200 -----\n",
      "global step, learning rate & loss: 4200, 0.485000014305, 4.52477028847\n",
      "test bucket id:  0     eval_ppx: 128.034369113\n",
      "test bucket id:  1     eval_ppx: 107.20896694\n",
      "test bucket id:  2     eval_ppx: 128.236915025\n",
      "test bucket id:  3     eval_ppx: 138.124298248\n",
      "----- total_step: 4300 -----\n",
      "global step, learning rate & loss: 4300, 0.485000014305, 4.50334331036\n",
      "test bucket id:  0     eval_ppx: 98.512387765\n",
      "test bucket id:  1     eval_ppx: 105.050668107\n",
      "test bucket id:  2     eval_ppx: 104.236636537\n",
      "test bucket id:  3     eval_ppx: 87.6793332016\n"
     ]
    }
   ],
   "source": [
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC'\n",
    "from __future__ import division\n",
    "import math\n",
    "\n",
    "sess = tf.Session()\n",
    "# 不要在当前目录下找 checkpoint，他会找全部 ckpt-xx 结尾的 checkpoint 文件，可能会读取其他项目的 checkpoint\n",
    "ckpt = tf.train.get_checkpoint_state('data/shooter/')\n",
    "if ckpt != None:\n",
    "    print(ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "train_set = s2s_read_data('data/shooter/train_encode.vec', 'data/shooter/train_decode.vec')\n",
    "test_set = s2s_read_data('data/shooter/test_encode.vec', 'data/shooter/test_decode.vec')\n",
    "\n",
    "# say [3, 5, 2]\n",
    "train_bucket_sizes = [len(train_set[i]) for i in range(len(buckets))]\n",
    "# ==> 3 + 5 + 2 = 10\n",
    "train_total_size = sum(train_bucket_sizes)\n",
    "# ==> [0.3, 0.8, 1.0]\n",
    "train_buckets_scale = [sum(train_bucket_sizes[: i + 1]) / train_total_size for i in range(len(train_bucket_sizes))]\n",
    "print \"train_bucket_sizes: {}\".format(train_bucket_sizes)\n",
    "print \"train_total_size: {}\".format(train_total_size)\n",
    "print \"train_buckets_scale: {}\".format(train_buckets_scale)\n",
    "\n",
    "loss = 0.0\n",
    "total_step = 0\n",
    "previous_losses = []\n",
    "# 持续训练\n",
    "print \"start training ...\"\n",
    "while True:\n",
    "    # 随机出一个 0~1 的小数\n",
    "    random_number_01 = np.random.random_sample()\n",
    "    # 找到这个随机数在 scale 中的位置，返回这个位置，也就是说，这里根据随机数找到一个桶\n",
    "    bucket_id = min([i for i in range(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\n",
    "    # 从对应的桶 id 中，读取一个 batch 的训练 enc & dec 文本，以及权重，这个由 seq2seq model 提供函数实现\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\n",
    "    # 进行训练，最后 forward_only 参数为 False\n",
    "    _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "\n",
    "    loss += step_loss / 100\n",
    "    total_step += 1\n",
    "\n",
    "    if total_step % 100 == 0:\n",
    "        print \"----- total_step: {} -----\".format(total_step)\n",
    "        print \"global step, learning rate & loss: {}, {}, {}\".format(model.global_step.eval(session=sess), model.learning_rate.eval(session=sess), loss)\n",
    "\n",
    "        # 如果模型没有得到提升，减小learning rate\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "            sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "\n",
    "        checkpoint_path = \"data/shooter/chatbot_seq2seq.ckpt\"\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "        loss = 0.0\n",
    "\n",
    "        # 通过测试数据评估\n",
    "        for bid in range(len(buckets)):\n",
    "            if len(test_set[bid]) == 0:\n",
    "                continue\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(test_set, bucket_id)\n",
    "            # 这里最后一个参数 forward_only 是 True，而前面训练时为 False\n",
    "            _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "            eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\n",
    "            print \"test bucket id:  {}     eval_ppx: {}\".format(bid, eval_ppx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**开始使用生成的模型来生成对话**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import codecs\n",
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "\n",
    "def read_vocabulary(vocabfile):\n",
    "    vocabs = []\n",
    "    with codecs.open(vocabfile, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            vocabs.append(line.strip())\n",
    "\n",
    "    vocab_dict = dict([(x, y) for (y, x) in enumerate(vocabs)])\n",
    "    return vocab_dict, vocabs\n",
    "\n",
    "\n",
    "vocab_enc, _ = read_vocabulary(\"data/shooter/vocab.enc\")\n",
    "_, vocab_dec = read_vocabulary(\"data/shooter/vocab.dec\")\n",
    "# batch size 在生成过程中，设置为 1，一问一答！\n",
    "batch_size = 1\n",
    "\n",
    "# 训练时， forward_only 为 False\n",
    "model = seq2seq_model.Seq2SeqModel(VOCAB_SIZE, VOCAB_SIZE, buckets=buckets, size=layer_size, num_layers=num_layers, max_gradient_norm= 5.0,\n",
    "                                   batch_size=batch_size, learning_rate=0.5, learning_rate_decay_factor=0.99, forward_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/shooter/chatbot_seq2seq.ckpt-4900\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "ckpt = tf.train.get_checkpoint_state('data/shooter')\n",
    "if ckpt != None:\n",
    "    print(ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    print(\"Failed to find model file\")\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    input_string = raw_input('Me > ').decode('utf-8')\n",
    "    if input_string == 'quit':\n",
    "        sys.exit(0)\n",
    "\n",
    "    input_string_vec = []\n",
    "    for w in input_string.strip():\n",
    "        input_string_vec.append(vocab_enc.get(w, UNK_ID))\n",
    "    bucket_id = min([i for i in range(len(buckets)) if buckets[i][0] > len(input_string_vec)])\n",
    "    # 从第一个参数中取 batch，而第一个参数只是一个 bucket id 对应的数据集，数据集中也只有一个元素，而这个元素只有 encoder 部分，没有 decoder\n",
    "    # 故此，很显然取到的 batch 只有 input_string_vec 一条 encoder\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch({bucket_id: [(input_string_vec, [])]}, bucket_id)\n",
    "    # 最后一个参数 forward_only 为 True\n",
    "    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "    # 取 logits 中每个 logit 所最大化对应的词典中的字；就是说，返回的 output_logits 是一个数组，对应一句话\n",
    "    # 其中每个元素为一个 logit，对应句子中的一个字，维度为 VOCAB_SIZE，对应词典中每个字的概率\n",
    "    # 这个对返回的句子每个字取最可能的值\n",
    "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "    if EOS_ID in outputs:\n",
    "        outputs = outputs[: outputs.index(EOS_ID)]\n",
    "\n",
    "    response = \"\".join([tf.compat.as_str(vocab_dec[output]) for output in outputs])\n",
    "    print('AI > ' + response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ipython notebook 似乎对 raw_input 和 unicode 显示支持的不是很好，那么在 shell 下运行结果如下：\n",
    "```\n",
    "data/shooter/chatbot_seq2seq.ckpt-4900\n",
    "Me > 嘿\n",
    "AI > 我\n",
    "Me > 你好\n",
    "AI > 我\n",
    "Me > 你只会说我？\n",
    "AI > 我是我\n",
    "Me > 你到底是谁\n",
    "AI > 我是我\n",
    "Me > 今天天气不错\n",
    "AI > 我是我\n",
    "Me > 听风和日丽的\n",
    "AI > 我是\n",
    "Me > 你疯了？\n",
    "AI > 我是\n",
    "Me > 谢谢你\n",
    "AI > 我\n",
    "Me > 昨天听说有一场不错的演唱会呢\n",
    "AI > 我们是我的\n",
    "Me > 你除了说我还会说什么呢？\n",
    "AI > 我是我\n",
    "```\n",
    "\n",
    "训练的轮数还不多，看到效果也不是很好哈"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
