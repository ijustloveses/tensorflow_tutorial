{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Checker by Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节参考 [斗大的熊猫](http://blog.topspeedsnail.com/archives/10833) ，根据姓名判断性别\n",
    "\n",
    "代码参考 [kaggle baby name competetion](https://github.com/tensorflow/models/tree/master/namignizer)\n",
    "\n",
    "使用的数据集：[名字](https://pan.baidu.com/s/1hsHTEU4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于文件很大，无用字段很多，故此使用预处理脚本删除掉无用字段，同时还会生成全词汇数组文件 lexicon.pickle\n",
    "\n",
    "> python2.7 sentiment140_preprocess.py\n",
    "\n",
    "运行后得到 3 个结果文件\n",
    "\n",
    "- data/sentiment140/lexicon.pickle  是由得到的 7176 个 lexicon 词典数组经过 cPickle.dump 得到的文件\n",
    "- data/sentiment140/traindata   格式如 [1, 0, 0]:%:%:%[5217,2341,13]:%:%:%how are you，表示 positive，3个词在词典中位置为 5217,2341,13，不过顺序无关，也就是说 how 不一定在 5217 位置\n",
    "- data/sentiment140/testdata   格式同上\n",
    "\n",
    "上面的脚本需要使用 nltk 模块，且安装 nltk.download('punkt') 和 nltk.download('wordnet')\n",
    "\n",
    "最后的 lexicon file 中共计 Total word count in lexicon: 7176 个词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后还实现了一个 python module，叫做 sentiment140.py ，用于处理训练和测试数据，生成喂给 CNN 的训练和测试数据，主要实现两个方法\n",
    "\n",
    "- get_train_batch(n=150)\n",
    "- get_test_dataset()\n",
    "\n",
    "这两个方法都返回 X, Y 两个数组\n",
    "\n",
    "- X 数组中的每个元素也是一个数组，对应一条 tweet，长度为词典 lex 总数，该 tweet 在词典中存在的词的索引为 1， 否则为 0，即词 one-hot 编码之和\n",
    "- Y 数组中的每个元素也是一个数组，对应一条 tweet 的标签，[1, 0, 0] 为 positive [0, 1, 0] 为 neutral [0, 0, 1] 为 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle\n",
    "\n",
    "from sentiment140 import Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part I. 探索数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498 7176\n",
      "498 3\n"
     ]
    }
   ],
   "source": [
    "senti = Sentiment140()\n",
    "test_x, test_y = senti.get_test_dataset()\n",
    "print len(test_x), len(test_x[0])\n",
    "print len(test_y), len(test_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看到测试集中 498 个记录；每条记录的 x 为 7176 维，也即词典长度，y 为 3 维，也即 positive/neutral/negative\n",
    "\n",
    "训练集过大，故此没有一次性导入，Sentiment140.py 提供一个取随机 batch 的函数；维度和上面是一致的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7176\n"
     ]
    }
   ],
   "source": [
    "input_size = len(senti.lex)    # 这里就不去重新加载 lex 词典了，因为都已经在 Sentiment140 类中封装好了，故此通过这个方法取词典长度\n",
    "num_classes = 3\n",
    "print input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Part II. Layer definations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "\n",
    "\n",
    "batch_size = 90    # 测试集 batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面要做 embedding，故此这里先熟悉一下 tf.nn.embedding_lookup 函数\n",
    "```\n",
    ">>> sess = tf.InteractiveSession()\n",
    ">>> w = tf.random_uniform([5, 2], -1.0, 1.0)       # 这里假设词典中词个数为 5 维，embedding 到 2 维，初始化为 -1~1 之间的随机数\n",
    ">>> w = sess.run(w)\n",
    ">>> x = [[1,1,0,0,0], [1,0,0,0,0], [0,0,1,1,1]]     # x 为 3 x 5 维， 3 代表 3 个样本，5 为词典中词个数，1 表示该词在词典中，0 表示不在\n",
    ">>> embedded_chars = tf.nn.embedding_lookup(w, x)\n",
    "\n",
    ">>> w\n",
    "array([[ 0.82283998,  0.21245265],              # 看到，w 确实是随机出来了，每一行表示词典中的对应词的 2 维 embedding 向量\n",
    "       [-0.737818  , -0.59785843],\n",
    "       [-0.24692678, -0.69566345],\n",
    "       [ 0.85945463, -0.21308041],\n",
    "       [ 0.28053808,  0.88169646]], dtype=float32)\n",
    ">>> x\n",
    "[[1, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 1, 1]]\n",
    ">>> sess.run(embedded_chars)\n",
    "array([[[-0.737818  , -0.59785843],             # 注意， embedding_chars 为 3 x 5 x 2 维，即样本个数 x 词典中词总个数 x embedding 维度\n",
    "        [-0.737818  , -0.59785843],\n",
    "        [ 0.82283998,  0.21245265],           # 我们以 embedding_chars[0::] 为例，也就是第一个样本得到的 lookup 结果\n",
    "        [ 0.82283998,  0.21245265],           # 第一个样本为 [1,1,0,0,0]，即该样本中有词典的前 2 个词，而没有后面 3 个词\n",
    "        [ 0.82283998,  0.21245265]],           # 我原以为 embedding_chars[0::] 会是前两个词的 embedding 向量 + 3 个 [0,0] \n",
    "                                    # 结果并不是这样，1 的部分填入 w[1]，而 0 的部分填入 w[0]\n",
    "       [[-0.737818  , -0.59785843],           # 这样有一个问题，那就是 w[2:] 也就是 w 矩阵从第三个开始往后的元素都没有用到啊！\n",
    "        [ 0.82283998,  0.21245265],           \n",
    "        [ 0.82283998,  0.21245265],           # 保留疑问，后面再说\n",
    "        [ 0.82283998,  0.21245265],\n",
    "        [ 0.82283998,  0.21245265]],\n",
    "\n",
    "       [[ 0.82283998,  0.21245265],\n",
    "        [ 0.82283998,  0.21245265],\n",
    "        [-0.737818  , -0.59785843],\n",
    "        [-0.737818  , -0.59785843],\n",
    "        [-0.737818  , -0.59785843]]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 更新认识  !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这个部分是之前做 Sentiment Analysis 时我的一些分析；而到现在我其实懂了，上面部分的 x 错了！\n",
    "\n",
    "上面分析中认为，x 为一个矢量，维度和词典中词的个数相同，x 的每个元素表示词典对应位置的词是否存在，存在则 1 否则 0\n",
    "\n",
    "实际上不是这样的，x 确实为一个矢量，只不过其长度和 x 这个句子的长度相同，每个元素则表示句子这个位置的词在词典中的索引，故此 x 的每个元素不限于 0 and 1，是句子中每个存在的词的索引\n",
    "\n",
    "最后，为了 embedding 的需要，要求 batch 样本中所有句子对应的 x 有相同的长度，故此需要做一个 padding\n",
    "\n",
    "这样：\n",
    "\n",
    "- 传给 embedding 层的数据为 batch_size X padded_sentence_length，即 batch_size 个已经被 padding 为相同长度的句子 \n",
    "- embedding 层的权重 W 为 vacabulary_size X embedding_size，表示词典中每个词对应的 embedding_size 长度的 vector\n",
    "- embedding 层的结果就是 batch_size X padded_sentence_length X embedding_size，即 batch_size 个句子，每个句子的每个词由其在词典中的位置索引变为该词对应的 embedding 矢量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neural_network(data, vocabulary_size, embedding_size = 128,  num_filters=128, dropout_keep_prob=0.5):\n",
    "    \"\"\"\n",
    "    data 即 batch of X，维度为 sample_count x padded_sentence_length，后者记为 input_size\n",
    "    \"\"\"\n",
    "    # embedding 层，把句子样本 batch 输入转为 vector\n",
    "    with tf.name_scope(\"embedding\"):\n",
    "        # W 在 -1 & 1 之间随机分布，维度为 vocabulary_size x embedding_size\n",
    "        W = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        # 见上一个 cell 中的研究，embedded_char 为 sample_count x input_size x embedding_size 维\n",
    "        embedded_chars = tf.nn.embedding_lookup(W, data)\n",
    "        # 在最后再添加一个维度，那么就是 sample_count x input_size x embedding_size x 1 维\n",
    "        # 我们知道 cnn 的输入为 sample_count x image_width x image_height x num_channel ，这里 num_channel 为 1\n",
    "        # 然后，input_size x embedding_size 就相当于图片中的 width x height\n",
    "        embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "\n",
    "    # 接下来是 CNN 层，注意和之前图片的串行 CNN 不同，这里采用的是并行 CNN，就是从 embedding 层出来的结果同时进入 3 个 CNN\n",
    "    # 3 层 CNN 对应 3 个 filters，每个 filters 都是一维，或者理解为 n x 1 的二维； 每层 filter 都是 num_filter\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-{}-{}\".format(i, filter_size)):\n",
    "            # 准备初始化权重，首先得到权重的 shape\n",
    "            # 输入的每个样本为 input_size x embedding_size ，那么我们把每层 filter 的维度设计为 filter_size x embedding_size\n",
    "            # 也就是说，filter 的一个维度和样本相当，故此 filter 只会在 input_size 这个维度上滑动\n",
    "            # 每层的 filter 都有 num_filters 组，而输入的 num_channel 都为 1\n",
    "            filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1))\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n",
    "            # 每次移动一步, conv 为 sample_size x input_size x 1 x num_filters\n",
    "            conv = tf.nn.conv2d(embedded_chars_expanded, W, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b))\n",
    "            # pooling，pooling 跨度很大，input_size - filter_size + 1, 也就是说 input_size 那么长的向量，pool 完之后只剩 1 长\n",
    "            # 故此 pooled 维度为 sample_size x 1 x 1 x num_filters\n",
    "            pooled = tf.nn.max_pool(h, ksize=[1, input_size - filter_size + 1, 1, 1], strides=[1,1,1,1], padding=\"VALID\")\n",
    "            pooled_outputs.append(pooled)\n",
    "    \n",
    "    # pooled_output 为一个数组，每个元素为 pooled，pooled 的维度为 sample_size x 1 x 1 x num_filters\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    # 在 idx=3 的维度上 concat，结果维度为 sample_size x 1 x 1 x num_filters_total\n",
    "    h_pool = tf.concat(3, pooled_outputs)     \n",
    "    # 保持最后一个维度不变，进行 flaten，结果是二维的；第一个维度为 sample_size * 1 * 1，第二个维度仍为 num_filters_total\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])    \n",
    "    \n",
    "    # dropout\n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        # 仍然是 sample_size x num_filters_total 维度\n",
    "        h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "        \n",
    "    # output full connection 层\n",
    "    with tf.name_scope(\"output\"):\n",
    "        # W = tf.get_variable(\"W\", shape=[num_filters_total, num_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.5))\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "        # sample_size x  num_classes 维度\n",
    "        output = tf.nn.xw_plus_b(h_drop, W, b)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part III. Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, input_size])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy:', 0.28)\n",
      "('accuracy:', 0.51999998)\n",
      "('accuracy:', 0.51999998)\n",
      "('accuracy:', 0.30000001)\n",
      "('accuracy:', 0.30000001)\n",
      "('accuracy:', 0.30000001)\n",
      "('accuracy:', 0.31999999)\n"
     ]
    }
   ],
   "source": [
    "output = neural_network(X, dropout_keep_prob)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output, Y))\n",
    "# optimizer = optimizer.minimize(loss)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars)   \n",
    "    \n",
    "y_pred = tf.argmax(tf.nn.softmax(output), 1)\n",
    "correct_predictions = tf.equal(y_pred, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    i = 0\n",
    "    pre_accuracy = 0\n",
    "    no_improvement = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        batch_x, batch_y = senti.get_train_batch(n=batch_size)\n",
    "        sess.run(optimizer, feed_dict={X:batch_x, Y:batch_y, dropout_keep_prob:0.5})\n",
    "            \n",
    "        if i % 10 == 0:\n",
    "            accur = sess.run(accuracy, feed_dict={X:test_x[0:50], Y:test_y[0:50], dropout_keep_prob:1.0})\n",
    "            print('accuracy:', accur)\n",
    "            if accur > pre_accuracy:\n",
    "                no_improvement = 0\n",
    "                pre_accuracy = accur\n",
    "                saver.save(sess, 'data/sentiment140/model.ckpt')\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                if no_improvement == 5:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看如果是简单的双层 full connection layers 加一个 softmax 层如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input_layer = len(senti.lex)\n",
    "n_layer_1 = 2000\n",
    "n_layer_2 = 2000\n",
    "n_output_layer = 3  # 结果 3 维\n",
    "\n",
    "def neural_network2(data):\n",
    "    layer_1_w_b = {'w_':tf.Variable(tf.random_normal([n_input_layer, n_layer_1])), 'b_':tf.Variable(tf.random_normal([n_layer_1]))}\n",
    "    layer_2_w_b = {'w_':tf.Variable(tf.random_normal([n_layer_1, n_layer_2])), 'b_':tf.Variable(tf.random_normal([n_layer_2]))}\n",
    "    layer_output_w_b = {'w_':tf.Variable(tf.random_normal([n_layer_2, n_output_layer])), 'b_':tf.Variable(tf.random_normal([n_output_layer]))}\n",
    " \n",
    "    layer_1 = tf.add(tf.matmul(tf.cast(data, tf.float32), layer_1_w_b['w_']), layer_1_w_b['b_'])\n",
    "    # layer_1 = tf.nn.xw_plus_b(tf.cast(data, tf.float32), layer_1_w_b['w_'], layer_1_w_b['b_'])   # 同上面\n",
    "    layer_1 = tf.nn.relu(layer_1)  # 激活函数\n",
    "    # layer_2 = tf.add(tf.matmul(layer_1, layer_2_w_b['w_']), layer_2_w_b['b_']) # 同下面\n",
    "    layer_2 = tf.nn.xw_plus_b(layer_1, layer_2_w_b['w_'], layer_2_w_b['b_'])\n",
    "    layer_2 = tf.nn.relu(layer_2 ) # 激活函数\n",
    "    layer_output = tf.add(tf.matmul(layer_2, layer_output_w_b['w_']), layer_output_w_b['b_'])\n",
    " \n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('training accuracy', 0.53333336)\n",
      "('testing accuracy:', 0.47999999)\n",
      "('training accuracy', 0.5777778)\n",
      "('testing accuracy:', 0.44)\n",
      "('training accuracy', 0.51111114)\n",
      "('testing accuracy:', 0.44)\n",
      "('training accuracy', 0.63333333)\n",
      "('testing accuracy:', 0.47999999)\n",
      "('training accuracy', 0.63333333)\n",
      "('testing accuracy:', 0.46000001)\n",
      "('training accuracy', 0.58888888)\n",
      "('testing accuracy:', 0.5)\n",
      "('training accuracy', 0.60000002)\n",
      "('testing accuracy:', 0.54000002)\n",
      "('training accuracy', 0.56666666)\n",
      "('testing accuracy:', 0.54000002)\n",
      "('training accuracy', 0.60000002)\n",
      "('testing accuracy:', 0.47999999)\n",
      "('training accuracy', 0.60000002)\n",
      "('testing accuracy:', 0.54000002)\n",
      "('training accuracy', 0.51111114)\n",
      "('testing accuracy:', 0.54000002)\n",
      "('training accuracy', 0.65555555)\n",
      "('testing accuracy:', 0.51999998)\n",
      "('training accuracy', 0.55555558)\n",
      "('testing accuracy:', 0.51999998)\n",
      "('training accuracy', 0.63333333)\n",
      "('testing accuracy:', 0.54000002)\n",
      "('training accuracy', 0.64444447)\n",
      "('testing accuracy:', 0.51999998)\n",
      "('training accuracy', 0.64444447)\n",
      "('testing accuracy:', 0.51999998)\n",
      "('training accuracy', 0.6111111)\n",
      "('testing accuracy:', 0.56)\n",
      "('training accuracy', 0.71111113)\n",
      "('testing accuracy:', 0.56)\n",
      "('training accuracy', 0.67777777)\n",
      "('testing accuracy:', 0.54000002)\n",
      "('training accuracy', 0.54444444)\n",
      "('testing accuracy:', 0.51999998)\n",
      "('training accuracy', 0.63333333)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.63333333)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.55555558)\n",
      "('testing accuracy:', 0.54000002)\n",
      "('training accuracy', 0.68888891)\n",
      "('testing accuracy:', 0.54000002)\n",
      "('training accuracy', 0.66666669)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.65555555)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.67777777)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.65555555)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.72222221)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.64444447)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.62222224)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.62222224)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.60000002)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.64444447)\n",
      "('testing accuracy:', 0.56)\n",
      "('training accuracy', 0.72222221)\n",
      "('testing accuracy:', 0.56)\n",
      "('training accuracy', 0.69999999)\n",
      "('testing accuracy:', 0.51999998)\n",
      "('training accuracy', 0.63333333)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.62222224)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.60000002)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.66666669)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.65555555)\n",
      "('testing accuracy:', 0.56)\n",
      "('training accuracy', 0.72222221)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.56666666)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.6111111)\n",
      "('testing accuracy:', 0.62)\n",
      "('training accuracy', 0.65555555)\n",
      "('testing accuracy:', 0.62)\n",
      "('training accuracy', 0.6111111)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.67777777)\n",
      "('testing accuracy:', 0.62)\n",
      "('training accuracy', 0.60000002)\n",
      "('testing accuracy:', 0.62)\n",
      "('training accuracy', 0.69999999)\n",
      "('testing accuracy:', 0.62)\n",
      "('training accuracy', 0.68888891)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.73333335)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.67777777)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.69999999)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.64444447)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.69999999)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.73333335)\n",
      "('testing accuracy:', 0.60000002)\n",
      "('training accuracy', 0.69999999)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.66666669)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.68888891)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.69999999)\n",
      "('testing accuracy:', 0.56)\n",
      "('training accuracy', 0.58888888)\n",
      "('testing accuracy:', 0.57999998)\n",
      "('training accuracy', 0.69999999)\n",
      "('testing accuracy:', 0.60000002)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a6b234125f5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msenti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainaccur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predict = neural_network2(X)\n",
    "cost_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predict, Y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost_func)\n",
    "correct = tf.equal(tf.argmax(predict,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.initialize_all_variables())\n",
    " \n",
    "    saver = tf.train.Saver()\n",
    "    i = 0\n",
    "    no_improvement = 0\n",
    "    pre_accuracy = 0\n",
    "    while True:   # 一直训练\n",
    "        i += 1\n",
    "        batch_x, batch_y = senti.get_train_batch(n=batch_size)\n",
    "        _, trainaccur = session.run([optimizer, accuracy], feed_dict={X:batch_x,Y:batch_y})\n",
    " \n",
    "        if i % 10 == 0:\n",
    "            print ('training accuracy', trainaccur)\n",
    "            accur = session.run(accuracy, feed_dict={X:test_x[50:100], Y:test_y[50:100]})\n",
    "            print('testing accuracy:', accur)\n",
    "            if accur > pre_accuracy:\n",
    "                no_improvement = 0\n",
    "                pre_accuracy = accur\n",
    "                # saver.save(session, 'data/sentiment140/model.ckpt')\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                if no_improvement == 20:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看到，无论是简单的神经网络还是 CNN 结果都并不理想"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
