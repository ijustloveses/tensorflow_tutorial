{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Checker by Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节参考 [斗大的熊猫](http://blog.topspeedsnail.com/archives/10833) ，根据姓名判断性别\n",
    "\n",
    "代码参考 [kaggle baby name competetion](https://github.com/tensorflow/models/tree/master/namignizer)\n",
    "\n",
    "以及 Sentiment_With_CNN.ipynb 一章的代码\n",
    "\n",
    "使用的数据集：[名字](https://pan.baidu.com/s/1hsHTEU4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集文件类似：\n",
    "```\n",
    "安镶怡,女\n",
    "饶黎明,男\n",
    "段焙曦,男\n",
    "苗芯萌,男\n",
    "覃慧藐,女\n",
    "芦玥微,女\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import codecs\n",
    "\n",
    "from sentiment140 import Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part I. 探索数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training set: 351789\n",
      "number of male: 206543\n",
      "number of female: 145246\n"
     ]
    }
   ],
   "source": [
    "datafile = 'data/name.csv'\n",
    "\n",
    "def parse_file(fname):\n",
    "    x, y = [], []\n",
    "    with codecs.open(fname, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) == 2:\n",
    "                x.append(parts[0])\n",
    "                y.append([0, 1] if parts[1] == u'男' else [1, 0])\n",
    "    return x, y\n",
    "\n",
    "train_x, train_y = parse_file(datafile)\n",
    "print \"number of training set: {}\".format(len(train_x))\n",
    "print \"number of male: {}\".format(len([y for y in train_y if y == [0, 1]]))\n",
    "print \"number of female: {}\".format(len([y for y in train_y if y == [1, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\u6700\\u957f\\u540d\\u5b57\\u7684\\u5b57\\u7b26\\u6570: ', 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 看一下最长名字的长度，因为要把全部名字都 padding 到这个长度上 \"\"\"\n",
    "max_name_length = max([len(name) for name in train_x])\n",
    "print(u\"最长名字的字符数: \", max_name_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary length: 6018\n",
      "[141, 3614, 2203]\n",
      "[85, 526, 485]\n",
      "[72, 86, 510]\n",
      "[130, 758, 69]\n",
      "[171, 296, 545]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 下面创建此表，给每个字都分配一个索引 \"\"\"\n",
    "from collections import defaultdict as dd\n",
    "counter = len(train_x)\n",
    "vocab = dd(int)\n",
    "for name in train_x:\n",
    "    for w in name:\n",
    "        vocab[w] += 1\n",
    "\n",
    "# 按词频由高到低排，前面加上空格。空格用于 padding\n",
    "vocab_list = [' '] + sorted(vocab, key=vocab.get, reverse=True)\n",
    "print(\"vocabulary length: {}\".format(len(vocab_list)))\n",
    "\n",
    "vocab_dict = dict([(x, y) for (y, x) in enumerate(vocab_list)])\n",
    "\"\"\" 转化名字为 vector，至于为什么这么转化，下面有详解 \"\"\"\n",
    "train_x_vec = []\n",
    "for name in train_x:\n",
    "    vec = []\n",
    "    for w in name:\n",
    "        vec.append(vocab_dict.get(w))\n",
    "    # padding\n",
    "    vec = vec + [0] * (max_name_length - len(vec))\n",
    "    train_x_vec.append(vec)\n",
    "\n",
    "for i in range(5):\n",
    "    print train_x_vec[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "input_size = max_name_length    # 下面有详解，这里 input_size 是 padding 后的长度，不是词典长度，vec 不是 one-hot 格式\n",
    "num_classes = 2\n",
    "print input_size\n",
    "batch_size = 64    # 测试集 batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Part II. Layer definations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面要做 embedding，故此这里先熟悉一下 tf.nn.embedding_lookup 函数\n",
    "```\n",
    ">>> sess = tf.InteractiveSession()\n",
    ">>> w = tf.random_uniform([5, 2], -1.0, 1.0)       # 这里假设词典中词个数为 5 维，embedding 到 2 维，初始化为 -1~1 之间的随机数\n",
    ">>> w = sess.run(w)\n",
    ">>> x = [[1,1,0,0,0], [1,0,0,0,0], [0,0,1,1,1]]     # x 为 3 x 5 维， 3 代表 3 个样本，5 为词典中词个数，1 表示该词在词典中，0 表示不在\n",
    ">>> embedded_chars = tf.nn.embedding_lookup(w, x)\n",
    "\n",
    ">>> w\n",
    "array([[ 0.82283998,  0.21245265],              # 看到，w 确实是随机出来了，每一行表示词典中的对应词的 2 维 embedding 向量\n",
    "       [-0.737818  , -0.59785843],\n",
    "       [-0.24692678, -0.69566345],\n",
    "       [ 0.85945463, -0.21308041],\n",
    "       [ 0.28053808,  0.88169646]], dtype=float32)\n",
    ">>> x\n",
    "[[1, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 1, 1]]\n",
    ">>> sess.run(embedded_chars)\n",
    "array([[[-0.737818  , -0.59785843],             # 注意， embedding_chars 为 3 x 5 x 2 维，即样本个数 x 词典中词总个数 x embedding 维度\n",
    "        [-0.737818  , -0.59785843],\n",
    "        [ 0.82283998,  0.21245265],           # 我们以 embedding_chars[0::] 为例，也就是第一个样本得到的 lookup 结果\n",
    "        [ 0.82283998,  0.21245265],           # 第一个样本为 [1,1,0,0,0]，即该样本中有词典的前 2 个词，而没有后面 3 个词\n",
    "        [ 0.82283998,  0.21245265]],           # 我原以为 embedding_chars[0::] 会是前两个词的 embedding 向量 + 3 个 [0,0] \n",
    "                                    # 结果并不是这样，1 的部分填入 w[1]，而 0 的部分填入 w[0]\n",
    "       [[-0.737818  , -0.59785843],           # 这样有一个问题，那就是 w[2:] 也就是 w 矩阵从第三个开始往后的元素都没有用到啊！\n",
    "        [ 0.82283998,  0.21245265],           \n",
    "        [ 0.82283998,  0.21245265],           # 保留疑问，后面再说\n",
    "        [ 0.82283998,  0.21245265],\n",
    "        [ 0.82283998,  0.21245265]],\n",
    "\n",
    "       [[ 0.82283998,  0.21245265],\n",
    "        [ 0.82283998,  0.21245265],\n",
    "        [-0.737818  , -0.59785843],\n",
    "        [-0.737818  , -0.59785843],\n",
    "        [-0.737818  , -0.59785843]]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 更新认识  !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这个部分是之前做 Sentiment Analysis 时我的一些分析；而到现在我其实懂了，上面部分的 x 错了！\n",
    "\n",
    "上面分析中认为，x 为一个矢量，维度和词典中词的个数相同，x 的每个元素表示词典对应位置的词是否存在，存在则 1 否则 0\n",
    "\n",
    "实际上不是这样的，x 确实为一个矢量，只不过其长度和 x 这个句子的长度相同，每个元素则表示句子这个位置的词在词典中的索引，故此 x 的每个元素不限于 0 and 1，是句子中每个存在的词的索引\n",
    "\n",
    "最后，为了 embedding 的需要，要求 batch 样本中所有句子对应的 x 有相同的长度，故此需要做一个 padding\n",
    "\n",
    "这样：\n",
    "\n",
    "- 传给 embedding 层的数据为 batch_size X padded_sentence_length，即 batch_size 个已经被 padding 为相同长度的句子 \n",
    "- embedding 层的权重 W 为 vacabulary_size X embedding_size，表示词典中每个词对应的 embedding_size 长度的 vector\n",
    "- embedding 层的结果就是 batch_size X padded_sentence_length X embedding_size，即 batch_size 个句子，每个句子的每个词由其在词典中的位置索引变为该词对应的 embedding 矢量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neural_network(data, vocabulary_size, embedding_size = 128,  num_filters=128, dropout_keep_prob=0.5):\n",
    "    \"\"\"\n",
    "    data 即 batch of X，维度为 sample_count x padded_sentence_length，后者记为 input_size\n",
    "    \"\"\"\n",
    "    # embedding 层，把句子样本 batch 输入转为 vector\n",
    "    with tf.name_scope(\"embedding\"):\n",
    "        # W 在 -1 & 1 之间随机分布，维度为 vocabulary_size x embedding_size\n",
    "        W = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        # 见上一个 cell 中的研究，embedded_char 为 sample_count x input_size x embedding_size 维\n",
    "        embedded_chars = tf.nn.embedding_lookup(W, data)\n",
    "        # 在最后再添加一个维度，那么就是 sample_count x input_size x embedding_size x 1 维\n",
    "        # 我们知道 cnn 的输入为 sample_count x image_width x image_height x num_channel ，这里 num_channel 为 1\n",
    "        # 然后，input_size x embedding_size 就相当于图片中的 width x height\n",
    "        embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "\n",
    "    # 接下来是 CNN 层，注意和之前图片的串行 CNN 不同，这里采用的是并行 CNN，就是从 embedding 层出来的结果同时进入 3 个 CNN\n",
    "    # 3 层 CNN 对应 3 个 filters，每个 filters 都是一维，或者理解为 n x 1 的二维； 每层 filter 都是 num_filter\n",
    "    filter_sizes = [1, 2, 3]\n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-{}-{}\".format(i, filter_size)):\n",
    "            # 准备初始化权重，首先得到权重的 shape\n",
    "            # 输入的每个样本为 input_size x embedding_size ，那么我们把每层 filter 的维度设计为 filter_size x embedding_size\n",
    "            # 也就是说，filter 的一个维度和样本相当，故此 filter 只会在 input_size 这个维度上滑动\n",
    "            # 每层的 filter 都有 num_filters 组，而输入的 num_channel 都为 1\n",
    "            filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1))\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n",
    "            # 每次移动一步, conv 为 sample_size x input_size x 1 x num_filters\n",
    "            conv = tf.nn.conv2d(embedded_chars_expanded, W, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b))\n",
    "            # pooling，pooling 跨度很大，input_size - filter_size + 1, 也就是说 input_size 那么长的向量，pool 完之后只剩 1 长\n",
    "            # 故此 pooled 维度为 sample_size x 1 x 1 x num_filters\n",
    "            pooled = tf.nn.max_pool(h, ksize=[1, input_size - filter_size + 1, 1, 1], strides=[1,1,1,1], padding=\"VALID\")\n",
    "            pooled_outputs.append(pooled)\n",
    "    \n",
    "    # pooled_output 为一个数组，每个元素为 pooled，pooled 的维度为 sample_size x 1 x 1 x num_filters\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    # 在 idx=3 的维度上 concat，结果维度为 sample_size x 1 x 1 x num_filters_total\n",
    "    h_pool = tf.concat(3, pooled_outputs)     \n",
    "    # 保持最后一个维度不变，进行 flaten，结果是二维的；第一个维度为 sample_size * 1 * 1，第二个维度仍为 num_filters_total\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])    \n",
    "    \n",
    "    # dropout\n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        # 仍然是 sample_size x num_filters_total 维度\n",
    "        h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "        \n",
    "    # output full connection 层\n",
    "    with tf.name_scope(\"output\"):\n",
    "        # W = tf.get_variable(\"W\", shape=[num_filters_total, num_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.5))\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "        # sample_size x  num_classes 维度\n",
    "        output = tf.nn.xw_plus_b(h_drop, W, b)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part III. Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, input_size])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 - batch 0 - loss: 4.71256637573\n",
      "epoch 0 - batch 100 - loss: 3.49745106697\n",
      "epoch 0 - batch 200 - loss: 3.74005746841\n",
      "epoch 0 - batch 300 - loss: 2.46446561813\n",
      "epoch 0 - batch 400 - loss: 2.21196365356\n",
      "epoch 0 - batch 500 - loss: 0.994374752045\n",
      "epoch 0 - batch 600 - loss: 1.73235440254\n",
      "epoch 0 - batch 700 - loss: 2.10601425171\n",
      "epoch 0 - batch 800 - loss: 1.15178155899\n",
      "epoch 0 - batch 900 - loss: 1.29807972908\n",
      "epoch 0 - batch 1000 - loss: 0.872881770134\n",
      "epoch 0 - batch 1100 - loss: 1.08145427704\n",
      "epoch 0 - batch 1200 - loss: 1.27475547791\n",
      "epoch 0 - batch 1300 - loss: 1.11164855957\n",
      "epoch 0 - batch 1400 - loss: 0.794882953167\n",
      "epoch 0 - batch 1500 - loss: 0.654997527599\n",
      "epoch 0 - batch 1600 - loss: 0.783994793892\n",
      "epoch 0 - batch 1700 - loss: 0.78123819828\n",
      "epoch 0 - batch 1800 - loss: 0.477663934231\n",
      "epoch 0 - batch 1900 - loss: 0.454263865948\n",
      "epoch 0 - batch 2000 - loss: 0.395449876785\n",
      "epoch 0 - batch 2100 - loss: 0.461769223213\n",
      "epoch 0 - batch 2200 - loss: 0.387417137623\n",
      "epoch 0 - batch 2300 - loss: 0.560413241386\n",
      "epoch 0 - batch 2400 - loss: 0.51694470644\n",
      "epoch 0 - batch 2500 - loss: 0.486692786217\n",
      "epoch 0 - batch 2600 - loss: 0.601698756218\n",
      "epoch 0 - batch 2700 - loss: 0.452123701572\n",
      "epoch 0 - batch 2800 - loss: 0.358398228884\n",
      "epoch 0 - batch 2900 - loss: 0.502308607101\n",
      "epoch 0 - batch 3000 - loss: 0.497456431389\n",
      "epoch 0 - batch 3100 - loss: 0.510496199131\n",
      "epoch 0 - batch 3200 - loss: 0.372653663158\n",
      "epoch 0 - batch 3300 - loss: 0.495478868484\n",
      "epoch 0 - batch 3400 - loss: 0.507781028748\n",
      "epoch 0 - batch 3500 - loss: 0.398335009813\n",
      "epoch 0 - batch 3600 - loss: 0.417326390743\n",
      "epoch 0 - batch 3700 - loss: 0.525165557861\n",
      "epoch 0 - batch 3800 - loss: 0.297610878944\n",
      "epoch 0 - batch 3900 - loss: 0.396846622229\n",
      "epoch 0 - batch 4000 - loss: 0.31063452363\n",
      "epoch 0 - batch 4100 - loss: 0.304177761078\n",
      "epoch 0 - batch 4200 - loss: 0.390259504318\n",
      "epoch 0 - batch 4300 - loss: 0.386574983597\n",
      "epoch 0 - batch 4400 - loss: 0.587668478489\n",
      "epoch 0 - batch 4500 - loss: 0.493942081928\n",
      "epoch 0 - batch 4600 - loss: 0.445814490318\n",
      "epoch 0 - batch 4700 - loss: 0.402843981981\n",
      "epoch 0 - batch 4800 - loss: 0.466262668371\n",
      "epoch 0 - batch 4900 - loss: 0.334336519241\n",
      "epoch 0 - batch 5000 - loss: 0.366539180279\n",
      "epoch 0 - batch 5100 - loss: 0.477034986019\n",
      "epoch 0 - batch 5200 - loss: 0.553504049778\n",
      "epoch 0 - batch 5300 - loss: 0.465033382177\n",
      "epoch 0 - batch 5400 - loss: 0.449937611818\n",
      "epoch 1 - batch 0 - loss: 0.369644403458\n",
      "epoch 1 - batch 100 - loss: 0.452287167311\n",
      "epoch 1 - batch 200 - loss: 0.570201635361\n",
      "epoch 1 - batch 300 - loss: 0.537708699703\n",
      "epoch 1 - batch 400 - loss: 0.430202901363\n",
      "epoch 1 - batch 500 - loss: 0.340475827456\n",
      "epoch 1 - batch 600 - loss: 0.416910648346\n",
      "epoch 1 - batch 700 - loss: 0.456292957067\n",
      "epoch 1 - batch 800 - loss: 0.480915307999\n",
      "epoch 1 - batch 900 - loss: 0.525520265102\n",
      "epoch 1 - batch 1000 - loss: 0.326161146164\n",
      "epoch 1 - batch 1100 - loss: 0.396686434746\n",
      "epoch 1 - batch 1200 - loss: 0.556067228317\n",
      "epoch 1 - batch 1300 - loss: 0.337701261044\n",
      "epoch 1 - batch 1400 - loss: 0.465586185455\n",
      "epoch 1 - batch 1500 - loss: 0.2971175313\n",
      "epoch 1 - batch 1600 - loss: 0.491900295019\n",
      "epoch 1 - batch 1700 - loss: 0.542630851269\n",
      "epoch 1 - batch 1800 - loss: 0.36268183589\n",
      "epoch 1 - batch 1900 - loss: 0.303912431002\n",
      "epoch 1 - batch 2000 - loss: 0.361219227314\n",
      "epoch 1 - batch 2100 - loss: 0.272746264935\n",
      "epoch 1 - batch 2200 - loss: 0.444296956062\n",
      "epoch 1 - batch 2300 - loss: 0.404642343521\n",
      "epoch 1 - batch 2400 - loss: 0.434309095144\n",
      "epoch 1 - batch 2500 - loss: 0.387346416712\n",
      "epoch 1 - batch 2600 - loss: 0.564598798752\n",
      "epoch 1 - batch 2700 - loss: 0.314648449421\n",
      "epoch 1 - batch 2800 - loss: 0.298944950104\n",
      "epoch 1 - batch 2900 - loss: 0.415612727404\n",
      "epoch 1 - batch 3000 - loss: 0.442488908768\n",
      "epoch 1 - batch 3100 - loss: 0.488509088755\n",
      "epoch 1 - batch 3200 - loss: 0.391574442387\n",
      "epoch 1 - batch 3300 - loss: 0.408502578735\n",
      "epoch 1 - batch 3400 - loss: 0.359507203102\n",
      "epoch 1 - batch 3500 - loss: 0.34966686368\n",
      "epoch 1 - batch 3600 - loss: 0.365145504475\n",
      "epoch 1 - batch 3700 - loss: 0.471222162247\n",
      "epoch 1 - batch 3800 - loss: 0.278223335743\n",
      "epoch 1 - batch 3900 - loss: 0.347994714975\n",
      "epoch 1 - batch 4000 - loss: 0.281771540642\n",
      "epoch 1 - batch 4100 - loss: 0.330829203129\n",
      "epoch 1 - batch 4200 - loss: 0.346230119467\n",
      "epoch 1 - batch 4300 - loss: 0.326647847891\n",
      "epoch 1 - batch 4400 - loss: 0.481469660997\n",
      "epoch 1 - batch 4500 - loss: 0.512512803078\n",
      "epoch 1 - batch 4600 - loss: 0.418000906706\n",
      "epoch 1 - batch 4700 - loss: 0.346804201603\n",
      "epoch 1 - batch 4800 - loss: 0.430584132671\n",
      "epoch 1 - batch 4900 - loss: 0.34626737237\n",
      "epoch 1 - batch 5000 - loss: 0.363371700048\n",
      "epoch 1 - batch 5100 - loss: 0.459349095821\n",
      "epoch 1 - batch 5200 - loss: 0.571684539318\n",
      "epoch 1 - batch 5300 - loss: 0.484384745359\n",
      "epoch 1 - batch 5400 - loss: 0.403480112553\n",
      "epoch 2 - batch 0 - loss: 0.356663286686\n",
      "epoch 2 - batch 100 - loss: 0.382230043411\n",
      "epoch 2 - batch 200 - loss: 0.521154463291\n",
      "epoch 2 - batch 300 - loss: 0.551728188992\n",
      "epoch 2 - batch 400 - loss: 0.44560366869\n",
      "epoch 2 - batch 500 - loss: 0.415161222219\n",
      "epoch 2 - batch 600 - loss: 0.381725549698\n",
      "epoch 2 - batch 700 - loss: 0.464100420475\n",
      "epoch 2 - batch 800 - loss: 0.435967594385\n",
      "epoch 2 - batch 900 - loss: 0.50647687912\n",
      "epoch 2 - batch 1000 - loss: 0.275795549154\n",
      "epoch 2 - batch 1100 - loss: 0.335819423199\n",
      "epoch 2 - batch 1200 - loss: 0.477063387632\n",
      "epoch 2 - batch 1300 - loss: 0.348893404007\n",
      "epoch 2 - batch 1400 - loss: 0.42088830471\n",
      "epoch 2 - batch 1500 - loss: 0.297356545925\n",
      "epoch 2 - batch 1600 - loss: 0.481967896223\n",
      "epoch 2 - batch 1700 - loss: 0.477452039719\n",
      "epoch 2 - batch 1800 - loss: 0.357296049595\n",
      "epoch 2 - batch 1900 - loss: 0.308018952608\n",
      "epoch 2 - batch 2000 - loss: 0.318222522736\n",
      "epoch 2 - batch 2100 - loss: 0.208462804556\n",
      "epoch 2 - batch 2200 - loss: 0.413336187601\n",
      "epoch 2 - batch 2300 - loss: 0.415518939495\n",
      "epoch 2 - batch 2400 - loss: 0.426372975111\n",
      "epoch 2 - batch 2500 - loss: 0.412657618523\n",
      "epoch 2 - batch 2600 - loss: 0.535138309002\n",
      "epoch 2 - batch 2700 - loss: 0.333435118198\n",
      "epoch 2 - batch 2800 - loss: 0.296045660973\n",
      "epoch 2 - batch 2900 - loss: 0.355565428734\n",
      "epoch 2 - batch 3000 - loss: 0.409474492073\n",
      "epoch 2 - batch 3100 - loss: 0.439748734236\n",
      "epoch 2 - batch 3200 - loss: 0.341706871986\n",
      "epoch 2 - batch 3300 - loss: 0.366537034512\n",
      "epoch 2 - batch 3400 - loss: 0.320703983307\n",
      "epoch 2 - batch 3500 - loss: 0.317065447569\n",
      "epoch 2 - batch 3600 - loss: 0.329370975494\n",
      "epoch 2 - batch 3700 - loss: 0.484695792198\n",
      "epoch 2 - batch 3800 - loss: 0.327379196882\n",
      "epoch 2 - batch 3900 - loss: 0.337413400412\n",
      "epoch 2 - batch 4000 - loss: 0.306765168905\n",
      "epoch 2 - batch 4100 - loss: 0.313590049744\n",
      "epoch 2 - batch 4200 - loss: 0.310653686523\n",
      "epoch 2 - batch 4300 - loss: 0.275273263454\n",
      "epoch 2 - batch 4400 - loss: 0.501829564571\n",
      "epoch 2 - batch 4500 - loss: 0.411427944899\n",
      "epoch 2 - batch 4600 - loss: 0.449206262827\n",
      "epoch 2 - batch 4700 - loss: 0.343548923731\n",
      "epoch 2 - batch 4800 - loss: 0.413321107626\n",
      "epoch 2 - batch 4900 - loss: 0.282830327749\n",
      "epoch 2 - batch 5000 - loss: 0.346175730228\n",
      "epoch 2 - batch 5100 - loss: 0.417522579432\n",
      "epoch 2 - batch 5200 - loss: 0.503137350082\n",
      "epoch 2 - batch 5300 - loss: 0.460047185421\n",
      "epoch 2 - batch 5400 - loss: 0.382147490978\n",
      "epoch 3 - batch 0 - loss: 0.314780294895\n",
      "epoch 3 - batch 100 - loss: 0.425855875015\n",
      "epoch 3 - batch 200 - loss: 0.543521642685\n",
      "epoch 3 - batch 300 - loss: 0.459853351116\n",
      "epoch 3 - batch 400 - loss: 0.443035364151\n",
      "epoch 3 - batch 500 - loss: 0.401082515717\n",
      "epoch 3 - batch 600 - loss: 0.403454124928\n",
      "epoch 3 - batch 700 - loss: 0.476893097162\n",
      "epoch 3 - batch 800 - loss: 0.353819549084\n",
      "epoch 3 - batch 900 - loss: 0.494245141745\n",
      "epoch 3 - batch 1000 - loss: 0.272287547588\n",
      "epoch 3 - batch 1100 - loss: 0.33501085639\n",
      "epoch 3 - batch 1200 - loss: 0.499419629574\n",
      "epoch 3 - batch 1300 - loss: 0.328222364187\n",
      "epoch 3 - batch 1400 - loss: 0.41006654501\n",
      "epoch 3 - batch 1500 - loss: 0.231913611293\n",
      "epoch 3 - batch 1600 - loss: 0.43343859911\n",
      "epoch 3 - batch 1700 - loss: 0.408948838711\n",
      "epoch 3 - batch 1800 - loss: 0.33664649725\n",
      "epoch 3 - batch 1900 - loss: 0.24972948432\n",
      "epoch 3 - batch 2000 - loss: 0.333768963814\n",
      "epoch 3 - batch 2100 - loss: 0.227676957846\n",
      "epoch 3 - batch 2200 - loss: 0.359216034412\n",
      "epoch 3 - batch 2300 - loss: 0.373289197683\n",
      "epoch 3 - batch 2400 - loss: 0.439240574837\n",
      "epoch 3 - batch 2500 - loss: 0.397933483124\n",
      "epoch 3 - batch 2600 - loss: 0.500658988953\n",
      "epoch 3 - batch 2700 - loss: 0.268679976463\n",
      "epoch 3 - batch 2800 - loss: 0.316993951797\n",
      "epoch 3 - batch 2900 - loss: 0.328857839108\n",
      "epoch 3 - batch 3000 - loss: 0.41533729434\n",
      "epoch 3 - batch 3100 - loss: 0.374238073826\n",
      "epoch 3 - batch 3200 - loss: 0.290066480637\n",
      "epoch 3 - batch 3300 - loss: 0.301377654076\n",
      "epoch 3 - batch 3400 - loss: 0.329914629459\n",
      "epoch 3 - batch 3500 - loss: 0.297039985657\n",
      "epoch 3 - batch 3600 - loss: 0.329328715801\n",
      "epoch 3 - batch 3700 - loss: 0.485247135162\n",
      "epoch 3 - batch 3800 - loss: 0.281242340803\n",
      "epoch 3 - batch 3900 - loss: 0.379017263651\n",
      "epoch 3 - batch 4000 - loss: 0.294860064983\n",
      "epoch 3 - batch 4100 - loss: 0.274649322033\n",
      "epoch 3 - batch 4200 - loss: 0.325209021568\n",
      "epoch 3 - batch 4300 - loss: 0.277848601341\n",
      "epoch 3 - batch 4400 - loss: 0.43527969718\n",
      "epoch 3 - batch 4500 - loss: 0.442489773035\n",
      "epoch 3 - batch 4600 - loss: 0.391716957092\n",
      "epoch 3 - batch 4700 - loss: 0.344350725412\n",
      "epoch 3 - batch 4800 - loss: 0.368183523417\n",
      "epoch 3 - batch 4900 - loss: 0.283968269825\n",
      "epoch 3 - batch 5000 - loss: 0.321874439716\n",
      "epoch 3 - batch 5100 - loss: 0.368290990591\n",
      "epoch 3 - batch 5200 - loss: 0.548603594303\n",
      "epoch 3 - batch 5300 - loss: 0.440149635077\n",
      "epoch 3 - batch 5400 - loss: 0.387651860714\n",
      "epoch 4 - batch 0 - loss: 0.282499909401\n",
      "epoch 4 - batch 100 - loss: 0.41009503603\n",
      "epoch 4 - batch 200 - loss: 0.511038839817\n",
      "epoch 4 - batch 300 - loss: 0.462444633245\n",
      "epoch 4 - batch 400 - loss: 0.486009091139\n",
      "epoch 4 - batch 500 - loss: 0.369425296783\n",
      "epoch 4 - batch 600 - loss: 0.343893110752\n",
      "epoch 4 - batch 700 - loss: 0.481334537268\n",
      "epoch 4 - batch 800 - loss: 0.411320596933\n",
      "epoch 4 - batch 900 - loss: 0.42863163352\n",
      "epoch 4 - batch 1000 - loss: 0.234699308872\n",
      "epoch 4 - batch 1100 - loss: 0.328175246716\n",
      "epoch 4 - batch 1200 - loss: 0.510914087296\n",
      "epoch 4 - batch 1300 - loss: 0.293634355068\n",
      "epoch 4 - batch 1400 - loss: 0.444543331861\n",
      "epoch 4 - batch 1500 - loss: 0.261171907187\n",
      "epoch 4 - batch 1600 - loss: 0.356314629316\n",
      "epoch 4 - batch 1700 - loss: 0.408731162548\n",
      "epoch 4 - batch 1800 - loss: 0.367766499519\n",
      "epoch 4 - batch 1900 - loss: 0.254774600267\n",
      "epoch 4 - batch 2000 - loss: 0.266825139523\n",
      "epoch 4 - batch 2100 - loss: 0.175798147917\n",
      "epoch 4 - batch 2200 - loss: 0.3316796422\n",
      "epoch 4 - batch 2300 - loss: 0.353046357632\n",
      "epoch 4 - batch 2400 - loss: 0.370196968317\n",
      "epoch 4 - batch 2500 - loss: 0.415114581585\n",
      "epoch 4 - batch 2600 - loss: 0.408148646355\n",
      "epoch 4 - batch 2700 - loss: 0.26119363308\n",
      "epoch 4 - batch 2800 - loss: 0.287044584751\n",
      "epoch 4 - batch 2900 - loss: 0.392784178257\n",
      "epoch 4 - batch 3000 - loss: 0.407355457544\n",
      "epoch 4 - batch 3100 - loss: 0.337913155556\n",
      "epoch 4 - batch 3200 - loss: 0.278957128525\n",
      "epoch 4 - batch 3300 - loss: 0.266451984644\n",
      "epoch 4 - batch 3400 - loss: 0.268783926964\n",
      "epoch 4 - batch 3500 - loss: 0.283494085073\n",
      "epoch 4 - batch 3600 - loss: 0.306506693363\n",
      "epoch 4 - batch 3700 - loss: 0.493659555912\n",
      "epoch 4 - batch 3800 - loss: 0.289620697498\n",
      "epoch 4 - batch 3900 - loss: 0.365919619799\n",
      "epoch 4 - batch 4000 - loss: 0.266183316708\n",
      "epoch 4 - batch 4100 - loss: 0.295525103807\n",
      "epoch 4 - batch 4200 - loss: 0.268022090197\n",
      "epoch 4 - batch 4300 - loss: 0.297807395458\n",
      "epoch 4 - batch 4400 - loss: 0.414203494787\n",
      "epoch 4 - batch 4500 - loss: 0.399124622345\n",
      "epoch 4 - batch 4600 - loss: 0.379348635674\n",
      "epoch 4 - batch 4700 - loss: 0.326768696308\n",
      "epoch 4 - batch 4800 - loss: 0.367972373962\n",
      "epoch 4 - batch 4900 - loss: 0.288112640381\n",
      "epoch 4 - batch 5000 - loss: 0.3101952672\n",
      "epoch 4 - batch 5100 - loss: 0.369404435158\n",
      "epoch 4 - batch 5200 - loss: 0.494953334332\n",
      "epoch 4 - batch 5300 - loss: 0.427543878555\n",
      "epoch 4 - batch 5400 - loss: 0.347551763058\n",
      "epoch 5 - batch 0 - loss: 0.286081105471\n",
      "epoch 5 - batch 100 - loss: 0.369048207998\n",
      "epoch 5 - batch 200 - loss: 0.452415913343\n",
      "epoch 5 - batch 300 - loss: 0.391252547503\n",
      "epoch 5 - batch 400 - loss: 0.474871426821\n",
      "epoch 5 - batch 500 - loss: 0.367754757404\n",
      "epoch 5 - batch 600 - loss: 0.314148128033\n",
      "epoch 5 - batch 700 - loss: 0.469891935587\n",
      "epoch 5 - batch 800 - loss: 0.319633245468\n",
      "epoch 5 - batch 900 - loss: 0.462155014277\n",
      "epoch 5 - batch 1000 - loss: 0.262580633163\n",
      "epoch 5 - batch 1100 - loss: 0.301358669996\n",
      "epoch 5 - batch 1200 - loss: 0.433343648911\n",
      "epoch 5 - batch 1300 - loss: 0.319237142801\n",
      "epoch 5 - batch 1400 - loss: 0.394755661488\n",
      "epoch 5 - batch 1500 - loss: 0.312581270933\n",
      "epoch 5 - batch 1600 - loss: 0.360107719898\n",
      "epoch 5 - batch 1700 - loss: 0.384340405464\n",
      "epoch 5 - batch 1800 - loss: 0.330045163631\n",
      "epoch 5 - batch 1900 - loss: 0.297443330288\n",
      "epoch 5 - batch 2000 - loss: 0.302103936672\n",
      "epoch 5 - batch 2100 - loss: 0.175227969885\n",
      "epoch 5 - batch 2200 - loss: 0.318064570427\n",
      "epoch 5 - batch 2300 - loss: 0.421752512455\n",
      "epoch 5 - batch 2400 - loss: 0.345276683569\n",
      "epoch 5 - batch 2500 - loss: 0.376018643379\n",
      "epoch 5 - batch 2600 - loss: 0.512222647667\n",
      "epoch 5 - batch 2700 - loss: 0.207779586315\n",
      "epoch 5 - batch 2800 - loss: 0.260920882225\n",
      "epoch 5 - batch 2900 - loss: 0.336757391691\n",
      "epoch 5 - batch 3000 - loss: 0.385470449924\n",
      "epoch 5 - batch 3100 - loss: 0.315950512886\n",
      "epoch 5 - batch 3200 - loss: 0.289015233517\n",
      "epoch 5 - batch 3300 - loss: 0.259794950485\n",
      "epoch 5 - batch 3400 - loss: 0.228248804808\n",
      "epoch 5 - batch 3500 - loss: 0.283196866512\n",
      "epoch 5 - batch 3600 - loss: 0.285047501326\n",
      "epoch 5 - batch 3700 - loss: 0.381146192551\n",
      "epoch 5 - batch 3800 - loss: 0.290576040745\n",
      "epoch 5 - batch 3900 - loss: 0.339499801397\n",
      "epoch 5 - batch 4000 - loss: 0.331971913576\n",
      "epoch 5 - batch 4100 - loss: 0.294654428959\n",
      "epoch 5 - batch 4200 - loss: 0.281819522381\n",
      "epoch 5 - batch 4300 - loss: 0.227943256497\n",
      "epoch 5 - batch 4400 - loss: 0.448164761066\n",
      "epoch 5 - batch 4500 - loss: 0.361605495214\n",
      "epoch 5 - batch 4600 - loss: 0.345306843519\n",
      "epoch 5 - batch 4700 - loss: 0.272544115782\n",
      "epoch 5 - batch 4800 - loss: 0.355232208967\n",
      "epoch 5 - batch 4900 - loss: 0.280337870121\n",
      "epoch 5 - batch 5000 - loss: 0.329152822495\n",
      "epoch 5 - batch 5100 - loss: 0.26889115572\n",
      "epoch 5 - batch 5200 - loss: 0.457637995481\n",
      "epoch 5 - batch 5300 - loss: 0.423284590244\n",
      "epoch 5 - batch 5400 - loss: 0.38002038002\n",
      "epoch 6 - batch 0 - loss: 0.362531125546\n",
      "epoch 6 - batch 100 - loss: 0.361529707909\n",
      "epoch 6 - batch 200 - loss: 0.464068561792\n",
      "epoch 6 - batch 300 - loss: 0.398045957088\n",
      "epoch 6 - batch 400 - loss: 0.452822715044\n",
      "epoch 6 - batch 500 - loss: 0.330442368984\n",
      "epoch 6 - batch 600 - loss: 0.335657536983\n",
      "epoch 6 - batch 700 - loss: 0.437335073948\n",
      "epoch 6 - batch 800 - loss: 0.391373425722\n",
      "epoch 6 - batch 900 - loss: 0.465804040432\n",
      "epoch 6 - batch 1000 - loss: 0.25372171402\n",
      "epoch 6 - batch 1100 - loss: 0.315596044064\n",
      "epoch 6 - batch 1200 - loss: 0.522702932358\n",
      "epoch 6 - batch 1300 - loss: 0.307590842247\n",
      "epoch 6 - batch 1400 - loss: 0.36725795269\n",
      "epoch 6 - batch 1500 - loss: 0.267889916897\n",
      "epoch 6 - batch 1600 - loss: 0.459909558296\n",
      "epoch 6 - batch 1700 - loss: 0.414215147495\n",
      "epoch 6 - batch 1800 - loss: 0.322014391422\n",
      "epoch 6 - batch 1900 - loss: 0.278663754463\n",
      "epoch 6 - batch 2000 - loss: 0.306325018406\n",
      "epoch 6 - batch 2100 - loss: 0.188165619969\n",
      "epoch 6 - batch 2200 - loss: 0.292562663555\n",
      "epoch 6 - batch 2300 - loss: 0.312901377678\n",
      "epoch 6 - batch 2400 - loss: 0.344165444374\n",
      "epoch 6 - batch 2500 - loss: 0.408708006144\n",
      "epoch 6 - batch 2600 - loss: 0.471483945847\n",
      "epoch 6 - batch 2700 - loss: 0.256792187691\n",
      "epoch 6 - batch 2800 - loss: 0.283497095108\n",
      "epoch 6 - batch 2900 - loss: 0.375873178244\n",
      "epoch 6 - batch 3000 - loss: 0.411271750927\n",
      "epoch 6 - batch 3100 - loss: 0.350371956825\n",
      "epoch 6 - batch 3200 - loss: 0.30367231369\n",
      "epoch 6 - batch 3300 - loss: 0.21478676796\n",
      "epoch 6 - batch 3400 - loss: 0.260006457567\n",
      "epoch 6 - batch 3500 - loss: 0.258693277836\n",
      "epoch 6 - batch 3600 - loss: 0.274528205395\n",
      "epoch 6 - batch 3700 - loss: 0.425742805004\n",
      "epoch 6 - batch 3800 - loss: 0.206933915615\n",
      "epoch 6 - batch 3900 - loss: 0.315724730492\n",
      "epoch 6 - batch 4000 - loss: 0.312278807163\n",
      "epoch 6 - batch 4100 - loss: 0.240724772215\n",
      "epoch 6 - batch 4200 - loss: 0.270605027676\n",
      "epoch 6 - batch 4300 - loss: 0.26260727644\n",
      "epoch 6 - batch 4400 - loss: 0.420652091503\n",
      "epoch 6 - batch 4500 - loss: 0.366040110588\n",
      "epoch 6 - batch 4600 - loss: 0.375886410475\n",
      "epoch 6 - batch 4700 - loss: 0.289534151554\n",
      "epoch 6 - batch 4800 - loss: 0.389000356197\n",
      "epoch 6 - batch 4900 - loss: 0.275406092405\n",
      "epoch 6 - batch 5000 - loss: 0.299187481403\n",
      "epoch 6 - batch 5100 - loss: 0.298014461994\n",
      "epoch 6 - batch 5200 - loss: 0.387445449829\n",
      "epoch 6 - batch 5300 - loss: 0.364006340504\n",
      "epoch 6 - batch 5400 - loss: 0.31666585803\n",
      "epoch 7 - batch 0 - loss: 0.308596074581\n",
      "epoch 7 - batch 100 - loss: 0.295049309731\n",
      "epoch 7 - batch 200 - loss: 0.467988610268\n",
      "epoch 7 - batch 300 - loss: 0.34970420599\n",
      "epoch 7 - batch 400 - loss: 0.392451941967\n",
      "epoch 7 - batch 500 - loss: 0.323276758194\n",
      "epoch 7 - batch 600 - loss: 0.280852109194\n",
      "epoch 7 - batch 700 - loss: 0.443159520626\n",
      "epoch 7 - batch 800 - loss: 0.353798419237\n",
      "epoch 7 - batch 900 - loss: 0.340366154909\n",
      "epoch 7 - batch 1000 - loss: 0.240164071321\n",
      "epoch 7 - batch 1100 - loss: 0.277970224619\n",
      "epoch 7 - batch 1200 - loss: 0.421801537275\n",
      "epoch 7 - batch 1300 - loss: 0.241725116968\n",
      "epoch 7 - batch 1400 - loss: 0.332373023033\n",
      "epoch 7 - batch 1500 - loss: 0.275091201067\n",
      "epoch 7 - batch 1600 - loss: 0.290687143803\n",
      "epoch 7 - batch 1700 - loss: 0.392649441957\n",
      "epoch 7 - batch 1800 - loss: 0.326874107122\n",
      "epoch 7 - batch 1900 - loss: 0.220894724131\n",
      "epoch 7 - batch 2000 - loss: 0.259724080563\n",
      "epoch 7 - batch 2100 - loss: 0.18010532856\n",
      "epoch 7 - batch 2200 - loss: 0.342567890882\n",
      "epoch 7 - batch 2300 - loss: 0.345045149326\n",
      "epoch 7 - batch 2400 - loss: 0.321854233742\n",
      "epoch 7 - batch 2500 - loss: 0.355099797249\n",
      "epoch 7 - batch 2600 - loss: 0.422668755054\n",
      "epoch 7 - batch 2700 - loss: 0.245134830475\n",
      "epoch 7 - batch 2800 - loss: 0.317980289459\n",
      "epoch 7 - batch 2900 - loss: 0.377418428659\n",
      "epoch 7 - batch 3000 - loss: 0.406527608633\n",
      "epoch 7 - batch 3100 - loss: 0.317029356956\n",
      "epoch 7 - batch 3200 - loss: 0.315565764904\n",
      "epoch 7 - batch 3300 - loss: 0.262624919415\n",
      "epoch 7 - batch 3400 - loss: 0.255455136299\n",
      "epoch 7 - batch 3500 - loss: 0.202252984047\n",
      "epoch 7 - batch 3600 - loss: 0.291444271803\n",
      "epoch 7 - batch 3700 - loss: 0.425057768822\n",
      "epoch 7 - batch 3800 - loss: 0.244221329689\n",
      "epoch 7 - batch 3900 - loss: 0.320832401514\n",
      "epoch 7 - batch 4000 - loss: 0.308788448572\n",
      "epoch 7 - batch 4100 - loss: 0.267205566168\n",
      "epoch 7 - batch 4200 - loss: 0.224272832274\n",
      "epoch 7 - batch 4300 - loss: 0.240826278925\n",
      "epoch 7 - batch 4400 - loss: 0.416260898113\n",
      "epoch 7 - batch 4500 - loss: 0.423316299915\n",
      "epoch 7 - batch 4600 - loss: 0.412597537041\n",
      "epoch 7 - batch 4700 - loss: 0.295794248581\n",
      "epoch 7 - batch 4800 - loss: 0.337845504284\n",
      "epoch 7 - batch 4900 - loss: 0.250289022923\n",
      "epoch 7 - batch 5000 - loss: 0.241963848472\n",
      "epoch 7 - batch 5100 - loss: 0.333075463772\n",
      "epoch 7 - batch 5200 - loss: 0.438647478819\n",
      "epoch 7 - batch 5300 - loss: 0.476107001305\n",
      "epoch 7 - batch 5400 - loss: 0.340153336525\n",
      "epoch 8 - batch 0 - loss: 0.292498767376\n",
      "epoch 8 - batch 100 - loss: 0.30800807476\n",
      "epoch 8 - batch 200 - loss: 0.462791055441\n",
      "epoch 8 - batch 300 - loss: 0.381728947163\n",
      "epoch 8 - batch 400 - loss: 0.404744565487\n",
      "epoch 8 - batch 500 - loss: 0.314924687147\n",
      "epoch 8 - batch 600 - loss: 0.269212961197\n",
      "epoch 8 - batch 700 - loss: 0.389013171196\n",
      "epoch 8 - batch 800 - loss: 0.301574736834\n",
      "epoch 8 - batch 900 - loss: 0.460377991199\n",
      "epoch 8 - batch 1000 - loss: 0.210338741541\n",
      "epoch 8 - batch 1100 - loss: 0.264207869768\n",
      "epoch 8 - batch 1200 - loss: 0.453378677368\n",
      "epoch 8 - batch 1300 - loss: 0.259879589081\n",
      "epoch 8 - batch 1400 - loss: 0.40199816227\n",
      "epoch 8 - batch 1500 - loss: 0.275628954172\n",
      "epoch 8 - batch 1600 - loss: 0.336148321629\n",
      "epoch 8 - batch 1700 - loss: 0.399514317513\n",
      "epoch 8 - batch 1800 - loss: 0.250876307487\n",
      "epoch 8 - batch 1900 - loss: 0.248501509428\n",
      "epoch 8 - batch 2000 - loss: 0.280405730009\n",
      "epoch 8 - batch 2100 - loss: 0.195066705346\n",
      "epoch 8 - batch 2200 - loss: 0.297738015652\n",
      "epoch 8 - batch 2300 - loss: 0.288786560297\n",
      "epoch 8 - batch 2400 - loss: 0.350277394056\n",
      "epoch 8 - batch 2500 - loss: 0.401008784771\n",
      "epoch 8 - batch 2600 - loss: 0.399501740932\n",
      "epoch 8 - batch 2700 - loss: 0.245540454984\n",
      "epoch 8 - batch 2800 - loss: 0.260256022215\n",
      "epoch 8 - batch 2900 - loss: 0.3144325912\n",
      "epoch 8 - batch 3000 - loss: 0.39157679677\n",
      "epoch 8 - batch 3100 - loss: 0.266476213932\n",
      "epoch 8 - batch 3200 - loss: 0.325916439295\n",
      "epoch 8 - batch 3300 - loss: 0.262622952461\n",
      "epoch 8 - batch 3400 - loss: 0.235070556402\n",
      "epoch 8 - batch 3500 - loss: 0.244356423616\n",
      "epoch 8 - batch 3600 - loss: 0.234415948391\n",
      "epoch 8 - batch 3700 - loss: 0.342735826969\n",
      "epoch 8 - batch 3800 - loss: 0.201380461454\n",
      "epoch 8 - batch 3900 - loss: 0.389452040195\n",
      "epoch 8 - batch 4000 - loss: 0.231936663389\n",
      "epoch 8 - batch 4100 - loss: 0.224024921656\n",
      "epoch 8 - batch 4200 - loss: 0.252691805363\n",
      "epoch 8 - batch 4300 - loss: 0.229330003262\n",
      "epoch 8 - batch 4400 - loss: 0.371649831533\n",
      "epoch 8 - batch 4500 - loss: 0.361081719398\n",
      "epoch 8 - batch 4600 - loss: 0.366897135973\n",
      "epoch 8 - batch 4700 - loss: 0.278930008411\n",
      "epoch 8 - batch 4800 - loss: 0.340695381165\n",
      "epoch 8 - batch 4900 - loss: 0.252868592739\n",
      "epoch 8 - batch 5000 - loss: 0.269948303699\n",
      "epoch 8 - batch 5100 - loss: 0.26457297802\n",
      "epoch 8 - batch 5200 - loss: 0.39174747467\n",
      "epoch 8 - batch 5300 - loss: 0.420612752438\n",
      "epoch 8 - batch 5400 - loss: 0.319687306881\n",
      "epoch 9 - batch 0 - loss: 0.273385882378\n",
      "epoch 9 - batch 100 - loss: 0.275999546051\n",
      "epoch 9 - batch 200 - loss: 0.490461230278\n",
      "epoch 9 - batch 300 - loss: 0.313242793083\n",
      "epoch 9 - batch 400 - loss: 0.424867928028\n",
      "epoch 9 - batch 500 - loss: 0.352649152279\n",
      "epoch 9 - batch 600 - loss: 0.275908768177\n",
      "epoch 9 - batch 700 - loss: 0.375600457191\n",
      "epoch 9 - batch 800 - loss: 0.268155515194\n",
      "epoch 9 - batch 900 - loss: 0.386386990547\n",
      "epoch 9 - batch 1000 - loss: 0.241063117981\n",
      "epoch 9 - batch 1100 - loss: 0.232120215893\n",
      "epoch 9 - batch 1200 - loss: 0.439122319221\n",
      "epoch 9 - batch 1300 - loss: 0.222924277186\n",
      "epoch 9 - batch 1400 - loss: 0.307486176491\n",
      "epoch 9 - batch 1500 - loss: 0.223216339946\n",
      "epoch 9 - batch 1600 - loss: 0.290312021971\n",
      "epoch 9 - batch 1700 - loss: 0.382515192032\n",
      "epoch 9 - batch 1800 - loss: 0.225560948253\n",
      "epoch 9 - batch 1900 - loss: 0.221138909459\n",
      "epoch 9 - batch 2000 - loss: 0.283092975616\n",
      "epoch 9 - batch 2100 - loss: 0.162292838097\n",
      "epoch 9 - batch 2200 - loss: 0.293055057526\n",
      "epoch 9 - batch 2300 - loss: 0.301577568054\n",
      "epoch 9 - batch 2400 - loss: 0.315721571445\n",
      "epoch 9 - batch 2500 - loss: 0.347700595856\n",
      "epoch 9 - batch 2600 - loss: 0.422649145126\n",
      "epoch 9 - batch 2700 - loss: 0.229647159576\n",
      "epoch 9 - batch 2800 - loss: 0.268279075623\n",
      "epoch 9 - batch 2900 - loss: 0.374611586332\n",
      "epoch 9 - batch 3000 - loss: 0.36163932085\n",
      "epoch 9 - batch 3100 - loss: 0.280576348305\n",
      "epoch 9 - batch 3200 - loss: 0.250542938709\n",
      "epoch 9 - batch 3300 - loss: 0.187187254429\n",
      "epoch 9 - batch 3400 - loss: 0.260210037231\n",
      "epoch 9 - batch 3500 - loss: 0.218641221523\n",
      "epoch 9 - batch 3600 - loss: 0.272356688976\n",
      "epoch 9 - batch 3700 - loss: 0.389303207397\n",
      "epoch 9 - batch 3800 - loss: 0.275552153587\n",
      "epoch 9 - batch 3900 - loss: 0.296355545521\n",
      "epoch 9 - batch 4000 - loss: 0.295440167189\n",
      "epoch 9 - batch 4100 - loss: 0.215986639261\n",
      "epoch 9 - batch 4200 - loss: 0.236247941852\n",
      "epoch 9 - batch 4300 - loss: 0.246079459786\n",
      "epoch 9 - batch 4400 - loss: 0.375045180321\n",
      "epoch 9 - batch 4500 - loss: 0.309267908335\n",
      "epoch 9 - batch 4600 - loss: 0.344895452261\n",
      "epoch 9 - batch 4700 - loss: 0.317521005869\n",
      "epoch 9 - batch 4800 - loss: 0.329786479473\n",
      "epoch 9 - batch 4900 - loss: 0.233832269907\n",
      "epoch 9 - batch 5000 - loss: 0.268789917231\n",
      "epoch 9 - batch 5100 - loss: 0.402188897133\n",
      "epoch 9 - batch 5200 - loss: 0.380562454462\n",
      "epoch 9 - batch 5300 - loss: 0.375217556953\n",
      "epoch 9 - batch 5400 - loss: 0.331148117781\n",
      "epoch 10 - batch 0 - loss: 0.301277339458\n",
      "epoch 10 - batch 100 - loss: 0.285975277424\n",
      "epoch 10 - batch 200 - loss: 0.412162810564\n",
      "epoch 10 - batch 300 - loss: 0.342227339745\n",
      "epoch 10 - batch 400 - loss: 0.418501138687\n",
      "epoch 10 - batch 500 - loss: 0.303901433945\n",
      "epoch 10 - batch 600 - loss: 0.265468001366\n",
      "epoch 10 - batch 700 - loss: 0.448636740446\n",
      "epoch 10 - batch 800 - loss: 0.316636264324\n",
      "epoch 10 - batch 900 - loss: 0.396217077971\n",
      "epoch 10 - batch 1000 - loss: 0.179827526212\n",
      "epoch 10 - batch 1100 - loss: 0.321166276932\n",
      "epoch 10 - batch 1200 - loss: 0.445995301008\n",
      "epoch 10 - batch 1300 - loss: 0.255248606205\n",
      "epoch 10 - batch 1400 - loss: 0.269632846117\n",
      "epoch 10 - batch 1500 - loss: 0.240489810705\n",
      "epoch 10 - batch 1600 - loss: 0.349850386381\n",
      "epoch 10 - batch 1700 - loss: 0.330919981003\n",
      "epoch 10 - batch 1800 - loss: 0.290610402822\n",
      "epoch 10 - batch 1900 - loss: 0.219433486462\n",
      "epoch 10 - batch 2000 - loss: 0.212762340903\n",
      "epoch 10 - batch 2100 - loss: 0.154330596328\n",
      "epoch 10 - batch 2200 - loss: 0.352970033884\n",
      "epoch 10 - batch 2300 - loss: 0.251057505608\n",
      "epoch 10 - batch 2400 - loss: 0.361786305904\n",
      "epoch 10 - batch 2500 - loss: 0.376123547554\n",
      "epoch 10 - batch 2600 - loss: 0.410847961903\n",
      "epoch 10 - batch 2700 - loss: 0.245958447456\n",
      "epoch 10 - batch 2800 - loss: 0.272665381432\n",
      "epoch 10 - batch 2900 - loss: 0.299287736416\n",
      "epoch 10 - batch 3000 - loss: 0.413407355547\n",
      "epoch 10 - batch 3100 - loss: 0.341360986233\n",
      "epoch 10 - batch 3200 - loss: 0.266548901796\n",
      "epoch 10 - batch 3300 - loss: 0.299892961979\n",
      "epoch 10 - batch 3400 - loss: 0.284888625145\n",
      "epoch 10 - batch 3500 - loss: 0.171534270048\n",
      "epoch 10 - batch 3600 - loss: 0.206567838788\n",
      "epoch 10 - batch 3700 - loss: 0.419111847878\n",
      "epoch 10 - batch 3800 - loss: 0.222423002124\n",
      "epoch 10 - batch 3900 - loss: 0.304288417101\n",
      "epoch 10 - batch 4000 - loss: 0.26923713088\n",
      "epoch 10 - batch 4100 - loss: 0.194860517979\n",
      "epoch 10 - batch 4200 - loss: 0.210209578276\n",
      "epoch 10 - batch 4300 - loss: 0.182782828808\n",
      "epoch 10 - batch 4400 - loss: 0.358859121799\n",
      "epoch 10 - batch 4500 - loss: 0.332343071699\n",
      "epoch 10 - batch 4600 - loss: 0.318549335003\n",
      "epoch 10 - batch 4700 - loss: 0.264448940754\n",
      "epoch 10 - batch 4800 - loss: 0.299447834492\n",
      "epoch 10 - batch 4900 - loss: 0.226954102516\n",
      "epoch 10 - batch 5000 - loss: 0.241351187229\n",
      "epoch 10 - batch 5100 - loss: 0.243391722441\n",
      "epoch 10 - batch 5200 - loss: 0.435248941183\n",
      "epoch 10 - batch 5300 - loss: 0.339906454086\n",
      "epoch 10 - batch 5400 - loss: 0.355463474989\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 11\n",
    "save_interval = 5\n",
    "\n",
    "output = neural_network(X, len(vocab_list), dropout_keep_prob=dropout_keep_prob)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output, Y))\n",
    "# optimizer = optimizer.minimize(loss)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars)   \n",
    "    \n",
    "num_batch = len(train_x_vec) // batch_size\n",
    "    \n",
    "# saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for e in range(total_epochs):\n",
    "    for i in range(num_batch):\n",
    "        batch_x = train_x_vec[i * batch_size: (i + 1) * batch_size]\n",
    "        batch_y = train_y[i * batch_size: (i + 1) * batch_size]\n",
    "        _, loss_ = sess.run([train_op, loss], feed_dict={X:batch_x, Y:batch_y, dropout_keep_prob:0.5})\n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch {} - batch {} - loss: {}\".format(e, i, loss_))\n",
    "            \n",
    "    if e % save_interval == 0:\n",
    "        saver.save(sess, 'name2sex.model', global_step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_name_vec(name):\n",
    "    vec = []\n",
    "    for w in name:\n",
    "        vec.append(vocab_dict.get(w, 0))\n",
    "    vec = vec + [0] * (max_name_length - len(vec))\n",
    "    x = [vec]\n",
    "    return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\u9648\\u5eb7\\u4f73', 'female')\n"
     ]
    }
   ],
   "source": [
    "x = create_name_vec(u'白富美')\n",
    "pred = tf.argmax(output, 1)\n",
    "res = sess.run(pred, {X:x, dropout_keep_prob:1.0})\n",
    "print(name, 'female' if res[0] == 0 else 'male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\u9648\\u5eb7\\u4f73', 'male')\n"
     ]
    }
   ],
   "source": [
    "x = create_name_vec(u'杨博宇')\n",
    "pred = tf.argmax(output, 1)\n",
    "res = sess.run(pred, {X:x, dropout_keep_prob:1.0})\n",
    "print(name, 'female' if res[0] == 0 else 'male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\u9648\\u5eb7\\u4f73', 'female')\n"
     ]
    }
   ],
   "source": [
    "x = create_name_vec(u'秦香莲')\n",
    "pred = tf.argmax(output, 1)\n",
    "res = sess.run(pred, {X:x, dropout_keep_prob:1.0})\n",
    "print(name, 'female' if res[0] == 0 else 'male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_gender(name):\n",
    "    x = create_name_vec(name)\n",
    "    \n",
    "    output = neural_network(x, len(vocab_list), dropout_keep_prob=1.0)\n",
    "    # saver = tf.train.Saver(tf.trainable_variables())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    session = tf.Session()\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state('.')\n",
    "    if ckpt != None:\n",
    "        print(ckpt.model_checkpoint_path)\n",
    "        saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print \"Failed to find module\"\n",
    "    \n",
    "    pred = tf.argmax(output, 1)\n",
    "    res = session.run(pred, {X:x, dropout_keep_prob:1.0})\n",
    "    print(name, 'female' if res[0] == 0 else 'male')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
